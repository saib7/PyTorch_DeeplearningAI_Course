{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c11bac2c-d2fd-4ca1-97ae-b9c7389c964e",
      "metadata": {},
      "source": [
        "# Programming Assignment: Building a Translation System\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Welcome to the final notebook in our transformer architecture series! After exploring attention mechanisms, encoder models, and decoder models separately, we now bring everything together to build a complete **Encoder-Decoder architecture** for neural machine translation.\n",
        "\n",
        "The encoder-decoder architecture is the foundation of many sequence-to-sequence (seq2seq) tasks, particularly machine translation. This architecture allows us to transform an input sequence (like an English sentence) into an output sequence (like a French translation) by learning the complex mappings between languages.\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "In this notebook, we will:\n",
        "- Understand the complete encoder-decoder architecture and how it combines our previous components\n",
        "- Implement a full transformer model for translation\n",
        "- Train our model on English-to-French translation\n",
        "- Evaluate and use our model to translate new sentences\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "This notebook assumes you've completed the previous notebooks on:\n",
        "1. Attention mechanisms\n",
        "2. Encoder architecture\n",
        "3. Decoder architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Table of Contents\n",
        "- [ 1 - Understanding the Encoder-Decoder Architecture](#1)\n",
        "  - [ 1.1 How Encoder-Decoder Models Work](#1-1)\n",
        "  - [ 1.2 Key Components and Information Flow](#1-2)\n",
        "- [ 2 - Understanding the Encoder-Decoder Architecture](#2)\n",
        "  - [ 2.1 Loading the Translation Dataset](#2-1)\n",
        "  - [ 2.2 Data Preprocessing and Tokenization](#2-2)\n",
        "- [ 3 - Building Vocabulary and Creating Data Loaders](#3)\n",
        "  - [ 3.1 Building Vocabularies](#3-1)\n",
        "  - [ 3.2 Preparing Translation Pairs](#3-2)\n",
        "  - [ 3.3 Creating PyTorch Dataset and DataLoaders](#3-3)\n",
        "- [ 4 - Building the Encoder-Decoder Architecture](#4)\n",
        "  - [ 4.1 Helper Functions for Masking](#4-1)\n",
        "  - [ 4.2 Positional Encoding](#4-2)\n",
        "  - [ 4.3 Complete Translator Model](#4-3)\n",
        "    - [ Exercise 1](#ex01)\n",
        "    - [ Exercise 2](#ex02)\n",
        "    - [ Exercise 3](#ex03)\n",
        "  - [ 4.4 Instantiating the Model](#4-4)\n",
        "  - [ 4.5 Testing the Model Forward Pass](#4-5)\n",
        "- [ 5 - Training the Translator](#5)\n",
        "  - [ 5.1 Initializing Training Components](#5-1)\n",
        "  - [ 5.2 Train the Model](#5-2)\n",
        "  - [ 5.3 Visualize Training Progress](#5-3)\n",
        "  - [ 5.4 Test Translation on Training Examples](#5-4)\n",
        "- [ 6 - Conclusion](#6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7279da32",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<h4 style=\"color:green; font-weight:bold;\">TIPS FOR SUCCESSFUL GRADING OF YOUR ASSIGNMENT:</h4>\n",
        "\n",
        "- All cells are frozen except for the ones where you need to submit your solutions or when explicitly mentioned you can interact with it.\n",
        "\n",
        "- You can add new cells to experiment but these will be omitted by the grader, so don't rely on newly created cells to host your solution code, use the provided places for this.\n",
        "\n",
        "- Avoid using global variables unless you absolutely have to. The grader tests your code in an isolated environment without running all cells from the top. As a result, global variables may be unavailable when scoring your submission. Global variables that are meant to be used will be defined in UPPERCASE.\n",
        "\n",
        "- - To submit your notebook for grading, first save it by clicking the ðŸ’¾ icon on the top left of the page and then click on the <span style=\"background-color: blue; color: white; padding: 3px 5px; font-size: 16px; border-radius: 5px;\">Submit assignment</span> button on the top right of the page.\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58058a73-b7b7-473b-96ba-1cc03a3fd581",
      "metadata": {},
      "source": [
        "<a id='1'></a>\n",
        "## 1 - Understanding the Encoder-Decoder Architecture\n",
        "\n",
        "The encoder-decoder architecture is a powerful framework for sequence-to-sequence learning tasks. It consists of two main components working in tandem:\n",
        "\n",
        "1. **The Encoder**: Processes the entire input sequence and creates a rich representation (context) of it\n",
        "2. **The Decoder**: Takes this context and generates the output sequence step by step\n",
        "\n",
        "Think of it as a two-stage translation process:\n",
        "- First, the encoder \"understands\" the source sentence completely\n",
        "- Then, the decoder \"expresses\" this understanding in the target language\n",
        "\n",
        "<a id='1-1'></a>\n",
        "### 1.1 How Encoder-Decoder Models Work\n",
        "\n",
        "The encoder-decoder architecture follows these key principles:\n",
        "\n",
        "1. **Encoding Phase**: The encoder processes the entire input sequence (e.g., an English sentence) and produces a sequence of hidden states that capture the meaning and context of each word in relation to the entire sentence.\n",
        "\n",
        "2. **Context Passing**: The encoder's output (hidden states) is passed to the decoder as context. In transformer models, this happens through cross-attention mechanisms.\n",
        "\n",
        "3. **Decoding Phase**: The decoder generates the output sequence one token at a time, using:\n",
        "   - The encoder's context (through cross-attention)\n",
        "   - Previously generated tokens (through self-attention)\n",
        "   - Learned patterns from training data\n",
        "\n",
        "4. **Autoregressive Generation**: During inference, the decoder generates tokens sequentially, where each new token depends on all previously generated tokens.\n",
        "\n",
        "<a id='1-2'></a>\n",
        "### 1.2 Key Components and Information Flow\n",
        "\n",
        "The information flow in an encoder-decoder transformer can be visualized as:\n",
        "\n",
        "```\n",
        "Input Sequence â†’ Encoder â†’ Context Vectors â†’ Decoder â†’ Output Sequence\n",
        "     (English)              (Hidden States)              (Desired language)\n",
        "```\n",
        "\n",
        "Each component plays a crucial role:\n",
        "- **Encoder Self-Attention**: Helps each word understand its context within the source sentence\n",
        "- **Decoder Self-Attention**: Ensures coherence in the generated target sequence\n",
        "- **Cross-Attention**: Connects source and target, allowing the decoder to \"look at\" relevant parts of the input when generating each output word"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "855a66a1-8ee9-4c87-8485-f4a1cd5dfbea",
      "metadata": {},
      "source": [
        "<a id='2'></a>\n",
        "## 2 - Understanding the Encoder-Decoder Architecture\n",
        "\n",
        "The encoder-decoder architecture transforms input sequences into output sequences through two key components:\n",
        "\n",
        "1. **Encoder**: Processes the entire input sequence and creates rich contextual representations\n",
        "2. **Decoder**: Uses the encoder's context to generate the output sequence step-by-step\n",
        "\n",
        "![Encoder-Decoder Architecture](images/encoder_decoder_architecture.svg)\n",
        "\n",
        "### Key Information Flow\n",
        "\n",
        "The architecture follows this pattern:\n",
        "- **Encoder Self-Attention**: Each input word understands its context within the source sentence\n",
        "- **Cross-Attention**: Decoder \"looks at\" relevant input parts when generating each output word  \n",
        "- **Decoder Self-Attention**: Ensures coherence in the generated sequence\n",
        "- **Autoregressive Generation**: Each new output token depends on previously generated tokens\n",
        "\n",
        "This design enables effective sequence-to-sequence learning for tasks like machine translation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "b7e95a64-45bc-4d7a-ac24-6a106e5e2b19",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# For data handling\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "5aaf057a-0b9c-4225-9a9c-01ffc170ec00",
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "943b367e-4c65-4ea4-b88e-64baaa72efb3",
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "import helper_utils\n",
        "import unittests"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bfad967-3ff1-424f-ab83-1d2a7f597939",
      "metadata": {},
      "source": [
        "<a id='2-1'></a>\n",
        "### 2.1 Loading the Translation Dataset\n",
        "\n",
        "In this assignment, you will work on a framework modular enough to be trained on datasets for different languages (but not a multi-language translator). Run the cell below to choose the language you want to work on for this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "351b6bc8-a4c9-4865-a2dd-a53a5e9b3c9d",
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "translation_pairs, target_language = helper_utils.load_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4090f6a-d69e-4a3a-a7e8-d995fea50831",
      "metadata": {},
      "source": [
        "<a id='2-2'></a>\n",
        "### 2.2 Data Preprocessing and Tokenization\n",
        "\n",
        "Before training a translation model, the text data needs to be properly preprocessed. This involves two key steps:\n",
        "\n",
        "1. **Text Normalization**: Converting text to lowercase, handling special characters, and preserving language-specific features (like accents in French or umlauts in German)\n",
        "2. **Tokenization**: Splitting sentences into individual tokens (words and punctuation) while preserving contractions like \"he's\" or \"can't\"\n",
        "\n",
        "For this assignment, we've provided the preprocessing functions in the `helper_utils` module. This includes:\n",
        "- `prepare_data()`: A function that normalizes your translation pairs and creates a language-appropriate tokenizer\n",
        "- `MultilingualTokenizer`: A tokenizer class that handles multiple languages correctly\n",
        "- `normalize_string()`: A function that cleans text while preserving important language features\n",
        "\n",
        "**Note**: The preprocessing code is provided so you can focus on the core learning objective - building the encoder-decoder architecture. In practice, data preprocessing is crucial for model performance, but here we want you to concentrate on understanding the translation model itself.\n",
        "\n",
        "To load and preprocess your data, run the cell below.\n",
        "\n",
        "\n",
        "The function returns:\n",
        "- `normalized_pairs`: A list of cleaned (English, Target Language) translation pairs\n",
        "- `tokenizer`: A tokenizer object you can use to convert text to tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "67a98e20-6c48-43df-a66e-32797a413af0",
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "normalized_pairs, tokenizer = helper_utils.prepare_data(\n",
        "    translation_pairs, \n",
        "    target_language,\n",
        "    max_pairs=150000,  # Process first 150,000 pairs for faster training\n",
        "    max_length=40      # Keep sentences with <= 40 words\n",
        ")\n",
        "\n",
        "# Cell 4: Check some normalized pairs\n",
        "import random\n",
        "\n",
        "print(f\"\\nRandom normalized {target_language} pairs:\")\n",
        "random_samples = random.sample(normalized_pairs, min(3, len(normalized_pairs)))\n",
        "for eng, target in random_samples:\n",
        "    print(f\"EN: {eng}\")\n",
        "    print(f\"{target_language}: {target}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# Cell 5: Use the tokenizer on custom text\n",
        "custom_text = \"I love programming!\"\n",
        "tokens = tokenizer(custom_text)\n",
        "print(f\"\\nCustom text: {custom_text}\")\n",
        "print(f\"Tokens: {tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ead68f56-610d-4814-81a5-8076bba34201",
      "metadata": {},
      "source": [
        "<a id='3'></a>\n",
        "## 3 - Building Vocabulary and Creating Data Loaders\n",
        "\n",
        "<a id='3-1'></a>\n",
        "### 3.1 Building Vocabularies\n",
        "\n",
        "Now you'll build vocabularies for both source (English) and target languages using the same approach as in previous notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "c932fe72-4ac1-4140-bc8a-a9f62fc54d28",
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "def build_vocab(sentences, tokenizer, min_freq=1):\n",
        "    \"\"\"\n",
        "    Build vocabulary from sentences\n",
        "    \"\"\"\n",
        "    counter = Counter()  # Counter to count word frequencies in all sentences\n",
        "    for sent in sentences:\n",
        "        counter.update(tokenizer(sent))  # Tokenize sentence and add token counts\n",
        "    \n",
        "    # Start vocab with special tokens for translation\n",
        "    # <pad>: padding token, <unk>: unknown token, <sos>: start of sequence, <eos>: end of sequence\n",
        "    vocab = ['<pad>', '<unk>', '<sos>', '<eos>'] + [w for w, c in counter.items() if c >= min_freq]\n",
        "    \n",
        "    # Create a mapping from word to unique index\n",
        "    word2idx = {w: i for i, w in enumerate(vocab)}\n",
        "    # Create a mapping from index back to word (inverse of word2idx)\n",
        "    idx2word = {i: w for i, w in enumerate(vocab)}\n",
        "    \n",
        "    # Return the vocab list and the two dictionaries\n",
        "    return vocab, word2idx, idx2word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "4b178065-fc27-478b-9481-eeb403f8926f",
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "# Extract English and target language sentences separately\n",
        "eng_sentences = [eng for eng, tgt in normalized_pairs]\n",
        "tgt_sentences = [tgt for eng, tgt in normalized_pairs]\n",
        "\n",
        "# Build vocabularies for both languages\n",
        "print(\"Building English vocabulary...\")\n",
        "eng_vocab, eng_word2idx, eng_idx2word = build_vocab(eng_sentences, tokenizer, min_freq=2)\n",
        "print(f\"English vocab size: {len(eng_vocab)}\")\n",
        "print(f\"First 20 English vocab words: {eng_vocab[:20]}\")\n",
        "\n",
        "print(f\"\\nBuilding {target_language} vocabulary...\")\n",
        "tgt_vocab, tgt_word2idx, tgt_idx2word = build_vocab(tgt_sentences, tokenizer, min_freq=2)\n",
        "print(f\"{target_language} vocab size: {len(tgt_vocab)}\")\n",
        "print(f\"First 20 {target_language} vocab words: {tgt_vocab[:20]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e500c964-919b-4d9f-aa67-09febf650a0f",
      "metadata": {},
      "source": [
        "<a id='3-2'></a>\n",
        "### 3.2 Preparing Translation Pairs\n",
        "\n",
        "For the encoder-decoder model, you need to prepare pairs of sequences where each source sentence is paired with its target translation. You'll add special tokens to mark the beginning and end of sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "94d03fbb-871c-4544-ab37-bb6ede5c22c2",
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "def prepare_sequence(sentence, tokenizer, word2idx, max_length=20, add_special_tokens=True):\n",
        "    \"\"\"\n",
        "    Convert a sentence to a list of indices with special tokens\n",
        "    \"\"\"\n",
        "    tokens = tokenizer(sentence)\n",
        "    \n",
        "    if add_special_tokens:\n",
        "        # Add <sos> at the beginning and <eos> at the end\n",
        "        tokens = ['<sos>'] + tokens + ['<eos>']\n",
        "    \n",
        "    # Convert tokens to indices\n",
        "    indices = [word2idx.get(token, word2idx['<unk>']) for token in tokens]\n",
        "    \n",
        "    # Pad or truncate to max_length\n",
        "    if len(indices) < max_length:\n",
        "        # Pad with <pad> tokens\n",
        "        indices = indices + [word2idx['<pad>']] * (max_length - len(indices))\n",
        "    else:\n",
        "        # Truncate if too long\n",
        "        indices = indices[:max_length]\n",
        "    \n",
        "    return indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "ecf0ad38-f833-4ad1-aeaa-6edd0cf0ce73",
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "# Prepare all translation pairs\n",
        "MAX_LENGTH = 40\n",
        "prepared_pairs = []\n",
        "\n",
        "for eng, tgt in normalized_pairs:\n",
        "    # Prepare source (English) - no special tokens for encoder input\n",
        "    eng_tokens = tokenizer(eng)\n",
        "    eng_indices = [eng_word2idx.get(token, eng_word2idx['<unk>']) for token in eng_tokens]\n",
        "    \n",
        "    # Pad or truncate\n",
        "    if len(eng_indices) < MAX_LENGTH:\n",
        "        eng_indices = eng_indices + [eng_word2idx['<pad>']] * (MAX_LENGTH - len(eng_indices))\n",
        "    else:\n",
        "        eng_indices = eng_indices[:MAX_LENGTH]\n",
        "    \n",
        "    # Prepare target - with special tokens for decoder\n",
        "    tgt_indices = prepare_sequence(tgt, tokenizer, tgt_word2idx, MAX_LENGTH, add_special_tokens=True)\n",
        "    \n",
        "    prepared_pairs.append((eng_indices, tgt_indices))\n",
        "\n",
        "print(f\"Number of prepared pairs: {len(prepared_pairs)}\")\n",
        "\n",
        "# Show an example pair\n",
        "example_idx = 0\n",
        "eng_indices, tgt_indices = prepared_pairs[example_idx]\n",
        "\n",
        "print(\"\\nExample prepared pair:\")\n",
        "print(f\"Original English: {normalized_pairs[example_idx][0]}\")\n",
        "print(f\"English tokens: {[eng_idx2word[i] for i in eng_indices if i != eng_word2idx['<pad>']]}\")\n",
        "print(f\"English indices: {eng_indices}\")\n",
        "\n",
        "print(f\"\\nOriginal {target_language}: {normalized_pairs[example_idx][1]}\")\n",
        "print(f\"{target_language} tokens: {[tgt_idx2word[i] for i in tgt_indices if i != tgt_word2idx['<pad>']]}\")\n",
        "print(f\"{target_language} indices: {tgt_indices}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d3ea9fe-0634-4ec4-9f98-c06114c57d40",
      "metadata": {},
      "source": [
        "<a id='3-3'></a>\n",
        "### 3.3 Creating PyTorch Dataset and DataLoaders\n",
        "\n",
        "You'll create a custom Dataset class to handle the data efficiently during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "96bdc1d7-777a-480b-be5c-4dcb3c63d506",
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for translation pairs\n",
        "    \"\"\"\n",
        "    def __init__(self, pairs):\n",
        "        self.pairs = pairs\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        src_indices, tgt_indices = self.pairs[idx]\n",
        "        \n",
        "        # Convert to tensors\n",
        "        src_tensor = torch.tensor(src_indices, dtype=torch.long)\n",
        "        tgt_tensor = torch.tensor(tgt_indices, dtype=torch.long)\n",
        "        \n",
        "        return src_tensor, tgt_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "503114c1-6bb5-47a1-aa73-3b62fa805242",
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "# Create the full dataset\n",
        "full_dataset = TranslationDataset(prepared_pairs)\n",
        "\n",
        "# Create random indices for splitting\n",
        "total_size = len(full_dataset)\n",
        "indices = list(range(total_size))\n",
        "\n",
        "# Shuffle indices with seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "indices = torch.randperm(total_size).tolist()\n",
        "\n",
        "# Calculate split point\n",
        "split_point = int(0.9 * total_size)\n",
        "\n",
        "# Create train and validation indices\n",
        "train_indices = indices[:split_point]\n",
        "val_indices = indices[split_point:]\n",
        "\n",
        "# Create subset datasets\n",
        "train_dataset = Subset(full_dataset, train_indices)\n",
        "val_dataset = Subset(full_dataset, val_indices)\n",
        "\n",
        "print(f\"Training pairs: {len(train_dataset)}\")\n",
        "print(f\"Validation pairs: {len(val_dataset)}\")\n",
        "\n",
        "# Create data loaders\n",
        "BATCH_SIZE = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Test the data loader\n",
        "for src_batch, tgt_batch in train_loader:\n",
        "    print(f\"Source batch shape: {src_batch.shape}\")\n",
        "    print(f\"Target batch shape: {tgt_batch.shape}\")\n",
        "    # Show first example from batch\n",
        "    print(f\"\\nFirst example in batch:\")\n",
        "    src_tokens = [eng_idx2word[idx.item()] for idx in src_batch[0] if idx.item() != eng_word2idx['<pad>']]\n",
        "    tgt_tokens = [tgt_idx2word[idx.item()] for idx in tgt_batch[0] if idx.item() != tgt_word2idx['<pad>']]\n",
        "    print(f\"Source (English): {' '.join(src_tokens)}\")\n",
        "    print(f\"Target ({target_language}): {' '.join(tgt_tokens)}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fe8f986-9e1d-466f-846b-6200044955c1",
      "metadata": {},
      "source": [
        "<a id='4'></a>\n",
        "## 4 - Building the Encoder-Decoder Architecture\n",
        "\n",
        "<a id='4-1'></a>\n",
        "### 4.1 Helper Functions for Masking\n",
        "\n",
        "Before building the model, you need helper functions to create masks. Masks are crucial in transformer-based models to control what information the model can \"see\" during processing. They prevent the model from:\n",
        "1. Paying attention to padding tokens (which are just placeholders)\n",
        "2. Cheating by looking at future tokens during training\n",
        "\n",
        "#### Understanding Padding Masks\n",
        "\n",
        "When we batch sequences together, they often have different lengths. We pad shorter sequences with zeros to make all sequences the same length:\n",
        "\n",
        "```\n",
        "Original sentences:\n",
        "\"I am\" â†’ ['I', 'am'] â†’ [34, 67]\n",
        "\"She loves cats\" â†’ ['She', 'loves', 'cats'] â†’ [12, 89, 45]\n",
        "\n",
        "After padding (assuming max_length=5):\n",
        "[34, 67, 0, 0, 0]  # \"I am\" + padding\n",
        "[12, 89, 45, 0, 0]  # \"She loves cats\" + padding\n",
        "```\n",
        "\n",
        "The padding mask tells the model to ignore these padding positions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "9af12d00-3662-4f32-87d2-74049418e06e",
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "def create_padding_mask(seq, pad_idx=0):\n",
        "    \"\"\"\n",
        "    Create a mask to hide padding tokens\n",
        "    Args:\n",
        "        seq: Input sequence tensor [batch_size, seq_length]\n",
        "        pad_idx: Index used for padding (usually 0)\n",
        "    Returns:\n",
        "        Boolean mask where True = ignore this position\n",
        "    \"\"\"\n",
        "    return (seq == pad_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "be1c2b04-de27-43bf-ad3c-a4774cb2eba4",
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "padded_seq = create_padding_mask(np.array([34, 67, 0, 0, 0]))\n",
        "print(padded_seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21e1ce45-c38a-44f9-ad86-a12ba9186685",
      "metadata": {},
      "source": [
        "**Example:**\n",
        "```\n",
        "Input sequence: [34, 67, 0, 0, 0]\n",
        "Padding mask:   [False, False, True, True, True]\n",
        "                   â†‘      â†‘      â†‘     â†‘     â†‘\n",
        "                 Keep   Keep  Ignore Ignore Ignore\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7fa2315-5916-422c-9c36-0aa4329da7df",
      "metadata": {},
      "source": [
        "#### Understanding Causal Masks (Look-Ahead Masks)\n",
        "\n",
        "During training, the decoder generates tokens one at a time. To prevent it from \"cheating\" by looking at future tokens it hasn't generated yet, we use a causal mask:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "db9f9bd6-74aa-492a-a6ae-ae9a3b1fcaa0",
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "def make_causal_mask(size):\n",
        "    \"\"\"\n",
        "    Create a mask to hide future tokens (for decoder self-attention)\n",
        "    Args:\n",
        "        size: Sequence length\n",
        "    Returns:\n",
        "        Upper triangular matrix where True = ignore this position\n",
        "    \"\"\"\n",
        "    mask = torch.triu(torch.ones(size, size), diagonal=1).bool()\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "94f96984-4dd6-4db0-9961-89de0ddbd042",
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "# Example: Decoder input with padding\n",
        "decoder_input = ['<sos>', 'Hello', 'world', '<pad>', '<pad>']\n",
        "indices = [2, 34, 67, 0, 0]\n",
        "\n",
        "# Create both masks\n",
        "padding_mask = create_padding_mask(indices)  # [F, F, F, T, T]\n",
        "subsequent_mask = make_causal_mask(5)   # Upper triangular matrix\n",
        "print(subsequent_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d206970a-3644-40ac-a872-1a3fc2fd8682",
      "metadata": {},
      "source": [
        "Combined effect:\n",
        "- Position 0: Can see position 0 only (not padding)\n",
        "- Position 1: Can see positions 0-1 (not padding)\n",
        "- Position 2: Can see positions 0-2 (not padding)\n",
        "- Positions 3-4: Ignored (they're padding)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2f6b7a0-1d74-4c14-b115-9bdc6f92a2b7",
      "metadata": {},
      "source": [
        "<a id='4-2'></a>\n",
        "### 4.2 Positional Encoding\n",
        "\n",
        "Transformers, unlike RNNs or LSTMs, process all tokens in a sequence simultaneously through attention mechanisms. This parallel processing is powerful but comes with a limitation: the model has no inherent understanding of word order. Without position information, \"The cat chased the dog\" would be indistinguishable from \"The dog chased the cat\" to the model.\n",
        "\n",
        "Positional encoding solves this by adding position-dependent signals to the word embeddings. These signals use sinusoidal functions with different frequencies - think of it like giving each position in the sequence a unique \"signature\" that the model can learn to interpret. The clever use of sine and cosine functions at different frequencies allows the model to learn relative positions (how far apart two words are) and absolute positions (where in the sentence a word appears). This positional information is simply added to the word embeddings, allowing the model to distinguish between the same word appearing in different positions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "c991dad9-65dc-488d-9cbc-e9de82d17c60",
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Adds positional information to token embeddings using sinusoidal patterns.\n",
        "    \n",
        "    Since transformers don't have inherent notion of sequence order (unlike RNNs),\n",
        "    we add positional encodings to give the model information about where each\n",
        "    token appears in the sequence.\n",
        "    \"\"\"\n",
        "    def __init__(self, max_len, d_model):\n",
        "        \"\"\"\n",
        "        Initialize positional encoding matrix.\n",
        "        \n",
        "        Args:\n",
        "            max_len (int): Maximum sequence length the model will handle\n",
        "                          (e.g., 100 for sentences up to 100 tokens)\n",
        "            d_model (int): Dimension of the model's embeddings \n",
        "                          (e.g., 256 or 512 - must match embedding size)\n",
        "        \n",
        "        Creates a fixed sinusoidal pattern matrix of shape [max_len, d_model]\n",
        "        where each row represents the positional encoding for that position.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        # Create positional encoding matrix\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        \n",
        "        # Create div_term for the sinusoidal pattern\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
        "                           -(torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        \n",
        "        # Apply sin to even indices\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        # Apply cos to odd indices  \n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        \n",
        "        # Register as buffer (not trained, but saved with model)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Add positional encoding to input embeddings.\n",
        "        \n",
        "        Args:\n",
        "            x (Tensor): Token embeddings of shape [batch_size, seq_len, d_model]\n",
        "                       where seq_len <= max_len from initialization\n",
        "        \n",
        "        Returns:\n",
        "            Tensor: Positional encodings of shape [batch_size, seq_len, d_model]\n",
        "                   (same shape as input, ready to be added to embeddings)\n",
        "        \n",
        "        Example:\n",
        "            If x represents embeddings for \"I love cats\" (3 tokens):\n",
        "            - Input x shape: [batch_size, 3, 256]\n",
        "            - Output shape: [batch_size, 3, 256]\n",
        "            - Returns positions 0, 1, 2 encoded as 256-dim vectors\n",
        "        \"\"\"\n",
        "        seq_len = x.size(1)\n",
        "        return self.pe[:, :seq_len, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f044f6f7-abe3-43f0-af82-3747128f7e2f",
      "metadata": {},
      "source": [
        "<a id='4-3'></a>\n",
        "### 4.3 Complete Translator Model\n",
        "\n",
        "Now you'll build the complete Translator model that combines encoder and decoder using PyTorch's built-in transformer layers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8fb96e5-5a0f-407c-a662-8c90157e1e97",
      "metadata": {},
      "source": [
        "<a id='ex01'></a>\n",
        "### Exercise 1\n",
        "\n",
        "In this exercise, you'll build the **Encoder** component of our translation model. The encoder is responsible for reading and understanding the source language (English in our case) and creating a rich contextual representation that captures the meaning of the entire sentence.\n",
        "\n",
        "#### What You'll Build:\n",
        "\n",
        "The encoder processes source sentences through the following steps:\n",
        "\n",
        "1. **Token Embedding**: Convert word indices to dense vectors\n",
        "2. **Positional Encoding**: Add position information (since transformers don't inherently understand word order)\n",
        "3. **Transformer Encoding**: Apply self-attention to understand relationships between all words in the source sentence\n",
        "4. **Output Memory**: Generate contextualized representations that the decoder will use for translation\n",
        "\n",
        "#### Your Task:\n",
        "\n",
        "Complete the `Encoder` class by filling in the missing code where you see `None`. Follow the inline comments that guide you through each step:\n",
        "\n",
        "- Initialize the embedding layer with the correct vocabulary size and dimensions\n",
        "- Add positional encoding to give the model sequence order information\n",
        "- Create transformer encoder layers with the specified architecture\n",
        "- Implement the forward pass to process input sentences\n",
        "\n",
        "#### Key Concepts to Remember:\n",
        "\n",
        "- **Padding Index**: We use index 0 for padding tokens (to handle variable-length sentences)\n",
        "- **Batch First**: We set `batch_first=True` to keep dimensions intuitive `[batch_size, seq_len, d_model]`\n",
        "- **Memory**: The encoder output is called \"memory\" because the decoder will repeatedly reference it during translation\n",
        "\n",
        "#### Expected Output:\n",
        "\n",
        "After completing this exercise, your encoder should:\n",
        "- Accept batches of tokenized source sentences\n",
        "- Return memory representations and padding masks\n",
        "- Handle variable-length inputs through padding and masking\n",
        "\n",
        "<details>\n",
        "<summary><h4 style=\"color:green; cursor:pointer;\">ðŸ’¡ Click here for detailed hints</h4></summary>\n",
        "\n",
        "### Hint 1: Token Embedding Layer\n",
        "\n",
        "**What you need**: Create an embedding layer that converts token indices to dense vectors.\n",
        "\n",
        "**Key points**:\n",
        "- PyTorch provides `nn.Embedding(num_embeddings, embedding_dim, padding_idx)` for this\n",
        "- `num_embeddings` should be the size of your source vocabulary (passed as `vocab_size` parameter)\n",
        "- `embedding_dim` should match your model dimension (the `d_model` parameter)\n",
        "- Set `padding_idx=0` so the model learns to ignore padding tokens\n",
        "\n",
        "**Example structure**:\n",
        "```python\n",
        "self.token_emb = nn.Embedding(\n",
        "    num_embeddings=?,  # vocab_size\n",
        "    embedding_dim=?,   # d_model\n",
        "    padding_idx=0\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Hint 2: Positional Encoding\n",
        "\n",
        "**What you need**: Add positional information to help the model understand word order.\n",
        "\n",
        "**Key points**:\n",
        "- The `PositionalEncoding` class is already implemented for you above\n",
        "- You need to instantiate it with the correct parameters\n",
        "- It requires `max_len` (maximum sequence length) and `d_model` (embedding dimension)\n",
        "- Both parameters are passed to the `__init__` method\n",
        "\n",
        "**Example structure**:\n",
        "```python\n",
        "self.pos_enc = PositionalEncoding(\n",
        "    max_len=?,   # Parameter from __init__\n",
        "    d_model=?    # Parameter from __init__\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Hint 3: Transformer Encoder Layer\n",
        "\n",
        "**What you need**: Create a single transformer encoder layer with multi-head attention.\n",
        "\n",
        "**Key points**:\n",
        "- Use `nn.TransformerEncoderLayer` from PyTorch\n",
        "- Required parameters:\n",
        "  - `d_model`: Model dimension (hidden size)\n",
        "  - `nhead`: Number of attention heads\n",
        "  - `dim_feedforward`: Dimension of the feedforward network\n",
        "  - `dropout`: Dropout rate for regularization\n",
        "  - `batch_first=True`: Keep batch dimension first for intuitive tensor shapes\n",
        "- All these parameters are passed to your `__init__` method\n",
        "\n",
        "**Example structure**:\n",
        "```python\n",
        "encoder_layer = nn.TransformerEncoderLayer(\n",
        "    d_model=?,           # From parameter\n",
        "    nhead=?,             # From parameter\n",
        "    dim_feedforward=?,   # From parameter\n",
        "    dropout=?,           # From parameter\n",
        "    batch_first=True     # Important!\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Hint 4: Stacking Multiple Encoder Layers\n",
        "\n",
        "**What you need**: Stack multiple encoder layers to create a deep encoder.\n",
        "\n",
        "**Key points**:\n",
        "- Use `nn.TransformerEncoder` to stack multiple layers\n",
        "- It takes the encoder layer you just created and the number of layers\n",
        "- `num_layers` parameter determines the depth\n",
        "\n",
        "**Example structure**:\n",
        "```python\n",
        "self.transformer_encoder = nn.TransformerEncoder(\n",
        "    encoder_layer=?,   # The encoder_layer you created above\n",
        "    num_layers=?       # From parameter\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Hint 5: Forward Pass - Embedding and Positional Encoding\n",
        "\n",
        "**What you need**: Convert tokens to embeddings and add positional information.\n",
        "\n",
        "**Key points**:\n",
        "- First, embed the tokens using `self.token_emb(x)`\n",
        "- Then, get positional encodings using `self.pos_enc(x)`\n",
        "- Add them together: `embeddings + positional_encodings`\n",
        "- The result should be a tensor of shape `[batch_size, seq_len, d_model]`\n",
        "\n",
        "**Example structure**:\n",
        "```python\n",
        "x = self.token_emb(?) + self.pos_enc(?)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Hint 6: Forward Pass - Applying Dropout\n",
        "\n",
        "**What you need**: Apply dropout for regularization.\n",
        "\n",
        "**Key points**:\n",
        "- Use the `self.dropout` layer that's already defined\n",
        "- Pass the embedded + positional encoded tensor through it\n",
        "- This randomly zeros some elements during training to prevent overfitting\n",
        "\n",
        "**Example structure**:\n",
        "```python\n",
        "x = self.dropout(?)  # Apply to the embedded tensor\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Hint 7: Forward Pass - Transformer Encoder\n",
        "\n",
        "**What you need**: Pass the embeddings through the transformer encoder layers.\n",
        "\n",
        "**Key points**:\n",
        "- Use `self.transformer_encoder` that you created in `__init__`\n",
        "- It needs two inputs:\n",
        "  1. The embedded tensor (with positional encoding and dropout applied)\n",
        "  2. The padding mask (via `src_key_padding_mask` parameter)\n",
        "- The padding mask tells the encoder which positions to ignore (where padding tokens are)\n",
        "\n",
        "**Example structure**:\n",
        "```python\n",
        "memory = self.transformer_encoder(\n",
        "    ?,                           # The embedded tensor\n",
        "    src_key_padding_mask=?       # The padding_mask created earlier\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Summary of the Forward Pass Flow:\n",
        "\n",
        "```\n",
        "Input indices [batch, seq_len]\n",
        "    â†“\n",
        "Token Embedding [batch, seq_len, d_model]\n",
        "    â†“\n",
        "+ Positional Encoding [batch, seq_len, d_model]\n",
        "    â†“\n",
        "Dropout\n",
        "    â†“\n",
        "Transformer Encoder (with padding mask)\n",
        "    â†“\n",
        "Memory (contextualized representations) [batch, seq_len, d_model]\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "Let's start building the foundation of our translation model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "9e385e4a-1522-4fe0-b6bf-da2249bad969",
      "metadata": {
        "deletable": false,
        "editable": true,
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# GRADED CELL  \n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder: Processes the source language (English) and creates a context representation\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model=256, nhead=8, num_layers=3, \n",
        "                 dim_feedforward=512, max_len=100, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "        \n",
        "        # Token embedding: Converts word indices to vectors with input dimension vocab_size, output dimension d_model and the padding_idx being the index 0\n",
        "        self.token_emb = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
        "        \n",
        "        # Generate the positional encoding, by calling the previous defined layer PositionalEncoding, with max length given by max_len and d_model being the embedding dimension\n",
        "        self.pos_enc = PositionalEncoding(max_len, d_model)\n",
        "        \n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(dropout) \n",
        "         \n",
        "        # Define the encoder, by calling nn.TransformerEncoderLayer with the appropriate parameters. Do not forget th pass the dropout value! Include the parameter batch_first = None\n",
        "        enc_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, \n",
        "                                             dim_feedforward=dim_feedforward, dropout=dropout,\n",
        "                                             batch_first=True)\n",
        "\n",
        "        # Set the transformer encoder layer by calling the nn.TransformerEncoder with the encoder_layer and the number of layers (given by the parameter)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: Source language token indices [batch_size, seq_len]\n",
        "        Returns:\n",
        "            memory: Encoded representation [batch_size, seq_len, d_model]\n",
        "        \"\"\"\n",
        "        ### START CODE HERE ###\n",
        "        \n",
        "        # Create padding mask\n",
        "        padding_mask = create_padding_mask(src, pad_idx=0)  # [batch_size, seq_len]\n",
        "        \n",
        "        # Embed tokens and add positional encoding\n",
        "        src = self.token_emb(src) * math.sqrt(self.token_emb.embedding_dim)\n",
        "        \n",
        "        # Perform dropout using the dropout layer defined above.\n",
        "        src = self.dropout(src + self.pos_enc(src))\n",
        "        \n",
        "        # Pass through transformer encoder, by generating the contextual representation and the padding mask, passed in the argument called src_key_padding_mask\n",
        "        memory = self.transformer_encoder(src, src_key_padding_mask=padding_mask)\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        return memory, padding_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "dfe177aa-649e-4a87-b653-8adda51a7a31",
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "unittests.exercise_1(Encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "a3a19046-5871-46d0-adb3-2815cada39e3",
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "# Usage\n",
        "encoder = Encoder(\n",
        "                vocab_size=5000,\n",
        "                d_model=256,\n",
        "                nhead=8,\n",
        "                num_layers=3,\n",
        "                dim_feedforward=512,\n",
        "                max_len=100,\n",
        "                dropout=0.1\n",
        ")\n",
        "\n",
        "helper_utils.show_model_layers(encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f369859-92db-4ada-a740-eb31432ae311",
      "metadata": {},
      "source": [
        "**Expected Output**\n",
        "\n",
        "```\n",
        "======================================================================\n",
        " TranslationEncoder - Main Layers\n",
        "======================================================================\n",
        "\n",
        "Layer                          Type                           Parameters\n",
        "----------------------------------------------------------------------\n",
        "token_emb                      Embedding                       1,280,000\n",
        "pos_enc                        PositionalEncoding                      0\n",
        "dropout                        Dropout                                 0\n",
        "transformer_encoder            TransformerEncoder              1,581,312\n",
        "----------------------------------------------------------------------\n",
        "TOTAL                                                          2,861,312\n",
        "======================================================================\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c1ae3d3-d669-4542-bcff-90c1e717fd0f",
      "metadata": {},
      "source": [
        "<a id='ex02'></a>\n",
        "### Exercise 2\n",
        "\n",
        "In this exercise, you'll build the **Decoder** component of our translation model. Unlike the generation-only decoder from Lab 3, this decoder is specifically designed for **translation** and includes cross-attention to encoder outputs.\n",
        "\n",
        "#### How This Decoder Differs from Lab 3\n",
        "\n",
        "| Aspect | Lab 3 Decoder (Generation) | Assignment Decoder (Translation) |\n",
        "|--------|---------------------------|----------------------------------|\n",
        "| **Primary Task** | Text generation (Shakespeare) | Translation (English â†’ French) |\n",
        "| **Input Source** | Only previous tokens | Previous tokens + Encoder memory |\n",
        "| **Attention Types** | Self-attention only | Self-attention + Cross-attention |\n",
        "| **Key Component** | `TransformerEncoder` with masking | `TransformerDecoder` with memory |\n",
        "| **Memory Input** | âŒ None | âœ… Encoder outputs |\n",
        "\n",
        "#### Translation Decoder Architecture\n",
        "\n",
        "```\n",
        "Target Tokens â†’ Embedding â†’ Positional Encoding\n",
        "                â†“\n",
        "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "         â”‚  Decoder Block  â”‚\n",
        "         â”‚                 â”‚\n",
        "         â”‚ Self-Attention  â”‚ â† Causal mask (no future peeking)\n",
        "         â”‚       â†“         â”‚\n",
        "         â”‚ Cross-Attention â”‚ â† Attends to encoder memory  \n",
        "         â”‚       â†“         â”‚\n",
        "         â”‚ Feed Forward    â”‚\n",
        "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                â†“\n",
        "         Output Projection â†’ Vocabulary Logits\n",
        "```\n",
        "\n",
        "#### ðŸ”‘ Critical Difference: Understanding Memory Masks\n",
        "\n",
        "This is a key concept that's often confusing. Let's clarify when and why we use different masks:\n",
        "\n",
        "**In Lab 3 (Generation-Only Decoder):**\n",
        "```python\n",
        "# Lab 3: Only decoder, no encoder - \"memory\" is the same target sequence\n",
        "decoded = self.transformer_decoder(\n",
        "    x,                        # Target sequence\n",
        "    x,                        # Same target sequence used as \"memory\"\n",
        "    tgt_mask=causal_mask,     # Causal mask for target self-attention  \n",
        "    memory_mask=causal_mask,  # SAME causal mask for fake \"cross-attention\"\n",
        "    ...\n",
        ")\n",
        "```\n",
        "- **Why `memory_mask=causal_mask`?** Because there's no real encoder - the decoder attends to itself\n",
        "- Both self-attention and \"cross-attention\" need causal masking to prevent future peeking\n",
        "\n",
        "**In This Assignment (Translation Decoder):**\n",
        "```python\n",
        "# Assignment: Real encoder-decoder translation  \n",
        "decoded = self.transformer_decoder(\n",
        "    x,                        # Target sequence\n",
        "    memory,                   # ENCODER OUTPUT (source language)\n",
        "    tgt_mask=causal_mask,     # Causal mask for target self-attention\n",
        "    memory_mask=None,         # NO mask for cross-attention to source\n",
        "    ...\n",
        ")\n",
        "```\n",
        "- **Why `memory_mask=None`?** Because we want to see **ALL source words** when generating each target word\n",
        "- The source sentence is complete and static - no causal constraint needed\n",
        "- Cross-attention should freely attend to any source position\n",
        "\n",
        "#### ðŸŽ¯ The Key Insight:\n",
        "\n",
        "| Decoder Type | Memory Content | Memory Mask | Reason |\n",
        "|--------------|----------------|-------------|---------|\n",
        "| **Lab 3 (Generation)** | Target sequence (itself) | `causal_mask` | Prevent future peeking in autoregressive generation |\n",
        "| **Assignment (Translation)** | Encoder output (source) | `None` | Allow full attention to complete source sentence |\n",
        "\n",
        "**Translation Example:**\n",
        "```\n",
        "Source:  [\"I\", \"love\", \"cats\"]           â† ENCODER OUTPUT\n",
        "Target:  [<sos>, \"Eu\", \"amo\", ...]       â† DECODER INPUT  \n",
        "Cross-attention: Can see ALL source words â† NO MASK NEEDED\n",
        "```\n",
        "\n",
        "#### Key Components You'll Implement:\n",
        "\n",
        "1. **Target Embedding & Positional Encoding**: Convert target language tokens to vectors with position information\n",
        "2. **Causal Masking**: Prevent decoder from seeing future tokens (maintains autoregressive property)  \n",
        "3. **Cross-Attention Decoder**: Use `TransformerDecoderLayer` that can attend to encoder memory\n",
        "4. **Output Projection**: Convert hidden states to target vocabulary probabilities\n",
        "\n",
        "<details>\n",
        "<summary><h4 style=\"color:green; cursor:pointer;\">ðŸ’¡ Click here for detailed hints</h4></summary>\n",
        "\n",
        "### Hint 1: Token Embedding for Target Language\n",
        "\n",
        "**What you need**: Create an embedding layer for target language tokens.\n",
        "\n",
        "**Key points**:\n",
        "- Very similar to the encoder's embedding layer\n",
        "- Use `nn.Embedding(vocab_size, d_model, padding_idx=0)`\n",
        "- The `vocab_size` here is for the **target** language vocabulary\n",
        "- Must have the same `d_model` as the encoder for compatibility\n",
        "\n",
        "**Example structure**:\n",
        "```python\n",
        "self.token_emb = nn.Embedding(\n",
        "    num_embeddings=?,  # vocab_size parameter\n",
        "    embedding_dim=?,   # d_model parameter\n",
        "    padding_idx=0\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Hint 2: Positional Encoding for Target Sequence\n",
        "\n",
        "**What you need**: Add positional information to target token embeddings.\n",
        "\n",
        "**Key points**:\n",
        "- Same as encoder - use the `PositionalEncoding` class\n",
        "- Instantiate with `max_len` and `d_model` parameters\n",
        "- This helps the decoder understand the order of generated words\n",
        "\n",
        "**Example structure**:\n",
        "```python\n",
        "self.pos_enc = PositionalEncoding(?, ?)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Hint 3: Transformer Decoder Layer\n",
        "\n",
        "**What you need**: Create a decoder layer (NOT an encoder layer!).\n",
        "\n",
        "**Key points**:\n",
        "- Use `nn.TransformerDecoderLayer` (this is different from Exercise 1!)\n",
        "- This layer has both self-attention AND cross-attention\n",
        "- Parameters are the same: `d_model`, `nhead`, `dim_feedforward`, `dropout`, `batch_first=True`\n",
        "\n",
        "**Example structure**:\n",
        "```python\n",
        "decoder_layer = nn.TransformerDecoderLayer(\n",
        "    d_model=?,\n",
        "    nhead=?,\n",
        "    dim_feedforward=?,\n",
        "    dropout=?,\n",
        "    batch_first=True\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Hint 4: Stacking Decoder Layers\n",
        "\n",
        "**What you need**: Stack multiple decoder layers together.\n",
        "\n",
        "**Key points**:\n",
        "- Use `nn.TransformerDecoder` (note: Decoder, not Encoder!)\n",
        "- Pass the decoder layer and the number of layers\n",
        "- Creates a deep decoder with multiple layers of self-attention and cross-attention\n",
        "\n",
        "**Example structure**:\n",
        "```python\n",
        "self.transformer_decoder = nn.TransformerDecoder(?, ?)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Hint 5: Output Projection Layer\n",
        "\n",
        "**What you need**: Convert decoder hidden states to vocabulary logits.\n",
        "\n",
        "**Key points**:\n",
        "- Use `nn.Linear` to project from `d_model` to `vocab_size`\n",
        "- Input dimension: `d_model` (hidden state size)\n",
        "- Output dimension: `vocab_size` (target vocabulary size)\n",
        "- This produces a score for each word in the target vocabulary\n",
        "\n",
        "**Example structure**:\n",
        "```python\n",
        "self.output_projection = nn.Linear(?, ?)  # in_features, out_features\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Hint 6: Forward Pass - Creating the Causal Mask\n",
        "\n",
        "**What you need**: Create a mask to prevent the decoder from seeing future tokens.\n",
        "\n",
        "**Key points**:\n",
        "- Use the `make_causal_mask()` function that's already provided\n",
        "- Pass the sequence length: `tgt.size(1)` gives you the length of the target sequence\n",
        "- Move the mask to the same device as your input: `.to(tgt.device)`\n",
        "\n",
        "**Example structure**:\n",
        "```python\n",
        "x_seq_len = tgt.size(1)\n",
        "x_subsequent_mask = make_causal_mask(?).to(?)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Hint 7: Forward Pass - Embedding with Scaling\n",
        "\n",
        "**What you need**: Embed tokens and scale by sqrt(d_model).\n",
        "\n",
        "**Key points**:\n",
        "- First embed: `self.token_emb(tgt)`\n",
        "- Then multiply by `math.sqrt(self.d_model)`\n",
        "- This scaling is a common practice in transformer models to prevent vanishing gradients\n",
        "- You'll need to import `math` module\n",
        "\n",
        "**Example structure**:\n",
        "```python\n",
        "x = self.token_emb(tgt) * math.sqrt(self.d_model)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Hint 8: Forward Pass - Adding Positional Encoding\n",
        "\n",
        "**What you need**: Add positional information to the scaled embeddings.\n",
        "\n",
        "**Key points**:\n",
        "- Get positional encodings: `self.pos_enc(tgt)`\n",
        "- Add to the embedded tensor: `tgt + positional_encoding`\n",
        "- Simple addition works because both have shape `[batch, seq_len, d_model]`\n",
        "\n",
        "**Example structure**:\n",
        "```python\n",
        "tgt = tgt + self.pos_enc(?)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Hint 9: Forward Pass - Applying Dropout\n",
        "\n",
        "**What you need**: Apply dropout regularization.\n",
        "\n",
        "**Key points**:\n",
        "- Use `self.dropout` layer\n",
        "- Apply to the embedded + positional encoded tensor\n",
        "\n",
        "**Example structure**:\n",
        "```python\n",
        "x = self.dropout(?)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Hint 10: Forward Pass - The Decoder (Most Important!)\n",
        "\n",
        "**What you need**: Pass through the transformer decoder with cross-attention to encoder memory.\n",
        "\n",
        "**Key points**:\n",
        "- Use `self.transformer_decoder`\n",
        "- **First positional argument**: Target sequence (the embedded tensor `x`)\n",
        "- **Second positional argument**: Encoder memory (passed as `memory` parameter)\n",
        "- **`tgt_mask`**: The causal mask you created (`x_subsequent_mask`)\n",
        "- **`memory_mask`**: Set to `None` (we want to see ALL source words!)\n",
        "- **`tgt_key_padding_mask`**: Target padding mask (`x_padding_mask`)\n",
        "- **`memory_key_padding_mask`**: Source padding mask (passed as `memory_padding_mask` parameter)\n",
        "\n",
        "**Example structure**:\n",
        "```python\n",
        "decoded = self.transformer_decoder(\n",
        "    ?,                              # Target sequence (x)\n",
        "    ?,                              # Encoder memory\n",
        "    tgt_mask=?,                     # Causal mask\n",
        "    memory_mask=None,               # No mask for source!\n",
        "    tgt_key_padding_mask=?,         # Target padding\n",
        "    memory_key_padding_mask=?       # Source padding\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Hint 11: Forward Pass - Final Layer Norm\n",
        "\n",
        "**What you need**: Apply layer normalization for training stability.\n",
        "\n",
        "**Key points**:\n",
        "- Use `self.ln_final` that's already defined\n",
        "- Apply to the decoded tensor\n",
        "\n",
        "**Example structure**:\n",
        "```python\n",
        "x = self.ln_final(?)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Hint 12: Forward Pass - Output Projection\n",
        "\n",
        "**What you need**: Project to vocabulary size to get logits.\n",
        "\n",
        "**Key points**:\n",
        "- Use `self.output_projection`\n",
        "- Apply to the normalized decoded tensor\n",
        "- Output shape: `[batch_size, seq_len, vocab_size]`\n",
        "\n",
        "**Example structure**:\n",
        "```python\n",
        "output = self.output_projection(?)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Summary of the Forward Pass Flow:\n",
        "\n",
        "```\n",
        "Target indices [batch, seq_len]\n",
        "    â†“\n",
        "Token Embedding (scaled) [batch, seq_len, d_model]\n",
        "    â†“\n",
        "+ Positional Encoding\n",
        "    â†“\n",
        "Dropout\n",
        "    â†“\n",
        "Transformer Decoder\n",
        "  - Self-Attention (with causal mask)\n",
        "  - Cross-Attention (to encoder memory, no mask!)\n",
        "  - Feed Forward\n",
        "    â†“\n",
        "Layer Normalization\n",
        "    â†“\n",
        "Output Projection\n",
        "    â†“\n",
        "Logits [batch, seq_len, vocab_size]\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "#### Expected Behavior:\n",
        "\n",
        "Your decoder should:\n",
        "- âœ… Accept target sequences AND encoder memory (unlike Lab 3)\n",
        "- âœ… Generate predictions for the next token at each position  \n",
        "- âœ… Use cross-attention to \"look at\" relevant source words\n",
        "- âœ… Properly mask future tokens during training\n",
        "- âœ… Output logits over the entire target vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "d3634534-345c-450c-adf5-b12297e611ca",
      "metadata": {
        "deletable": false,
        "editable": true,
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# GRADED CELL  \n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder component for translation (works with encoder output)\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model=256, nhead=8, num_layers=3,\n",
        "                 dim_feedforward=512, max_len=100, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        ### START CODE HERE ###\n",
        "        \n",
        "        # Token embedding: Converts target word indices to vectors with input dimension vocab_size, output dimension d_model and the padding_idx being the index 0\n",
        "        self.token_emb = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
        "        \n",
        "        # Generate the positional encoding for target sequence\n",
        "        self.pos_enc = PositionalEncoding(max_len, d_model)\n",
        "        \n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(dropout) \n",
        "        \n",
        "        # Define the decoder layer with the desired parameters\n",
        "        dec_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, \n",
        "                                             dim_feedforward=dim_feedforward, dropout=dropout,\n",
        "                                             batch_first=True)\n",
        "        \n",
        "        # Set the transformer decoder by passing the decoder layer and the number of layers\n",
        "        self.transformer_decoder = nn.TransformerDecoder(dec_layer, num_layers=num_layers)\n",
        "        \n",
        "        # Output projection layer: Projects decoder output to target vocabulary size, it must input the d_model and output vocab_size\n",
        "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
        "        \n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "    def forward(self, tgt, memory, memory_padding_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tgt: Target language token indices [batch_size, tgt_seq_len]\n",
        "            memory: Encoder output [batch_size, src_seq_len, d_model]\n",
        "            memory_padding_mask: Mask for encoder padding [batch_size, src_seq_len]\n",
        "        Returns:\n",
        "            output: Predicted token logits [batch_size, tgt_seq_len, vocab_size]\n",
        "        \"\"\"\n",
        "        ### START CODE HERE ###\n",
        "        \n",
        "        # Create padding mask for target sequence\n",
        "        tgt_padding_mask = create_padding_mask(tgt, pad_idx=0)  # [batch_size, tgt_seq_len]\n",
        "        \n",
        "        # Create subsequent mask to prevent decoder from looking at future tokens\n",
        "        tgt_seq_len = tgt.size(1)\n",
        "        tgt_subsequent_mask = make_causal_mask(tgt_seq_len).to(tgt.device)\n",
        "\n",
        "        # Convert into token embeddings\n",
        "        tgt = self.token_emb(tgt) * math.sqrt(self.token_emb.embedding_dim)\n",
        "        \n",
        "        # Add positional encoding so model knows word positions\n",
        "        tgt = self.dropout(tgt + self.pos_enc(tgt))\n",
        "        \n",
        "        # # Apply dropout to embedded target\n",
        "        # tgt = self.dropout(tgt)\n",
        "        \n",
        "        # Pass through transformer decoder with cross-attention to encoder memory\n",
        "        decoded = self.transformer_decoder(\n",
        "            tgt,\n",
        "            memory,\n",
        "            tgt_mask=tgt_subsequent_mask,\n",
        "            tgt_key_padding_mask=tgt_padding_mask,\n",
        "            memory_key_padding_mask=memory_padding_mask\n",
        "        )\n",
        "\n",
        "        # Project decoder output to vocabulary size\n",
        "        output = self.output_projection(decoded)\n",
        "        \n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "116f90f3-3c2c-4974-88a0-f68a238f275c",
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "decoder = Decoder(vocab_size=5000, d_model=256, nhead=8, num_layers=3)\n",
        "helper_utils.show_decoder_layers(decoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6cf8aeb-d425-4442-9184-29676ec310f5",
      "metadata": {},
      "source": [
        "**Expected Output**\n",
        "```\n",
        "======================================================================\n",
        " Decoder - Main Layers\n",
        "======================================================================\n",
        "\n",
        "Layer                          Type                           Parameters\n",
        "----------------------------------------------------------------------\n",
        "token_emb                      Embedding                       1,280,000\n",
        "pos_enc                        PositionalEncoding                      0\n",
        "dropout                        Dropout                                 0\n",
        "transformer_decoder            TransformerDecoder              2,372,352\n",
        "output_projection              Linear                          1,285,000\n",
        "----------------------------------------------------------------------\n",
        "TOTAL                                                          4,937,352\n",
        "======================================================================\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "94230b59",
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "unittests.exercise_2(Decoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c23e262-7a4d-4d6e-a227-3118bcc3fb40",
      "metadata": {},
      "source": [
        "<a id='ex03'></a>\n",
        "### Exercise 3\n",
        "\n",
        "Congratulations on building the encoder and decoder! Now it's time to bring them together into a complete translation system. In this final exercise, you'll create the **EncoderDecoder** model that orchestrates the full translation pipeline.\n",
        "\n",
        "#### What You'll Build:\n",
        "\n",
        "The complete translation model that:\n",
        "\n",
        "1. **Reads** the source language (via the Encoder)\n",
        "2. **Understands** the meaning and context\n",
        "3. **Generates** the target language translation (via the Decoder)\n",
        "\n",
        "This is where your previous work comes together into a functioning translator!\n",
        "\n",
        "#### Your Task:\n",
        "\n",
        "Complete the `EncoderDecoder` class by:\n",
        "\n",
        "- Instantiating the Encoder with source language vocabulary\n",
        "- Instantiating the Decoder with target language vocabulary  \n",
        "- Connecting them in the forward pass to perform translation\n",
        "\n",
        "The implementation is straightforward but powerful - you're wiring together the components you've already built.\n",
        "\n",
        "#### Key Design Decisions:\n",
        "\n",
        "- **Separate Vocabularies**: Source and target languages have different vocabulary sizes (e.g., English: 5000 words, Target Language: 6000 words)\n",
        "- **Separate Layer Counts**: You can have different depths for encoder and decoder (e.g., 3 encoder layers, 4 decoder layers)\n",
        "- **Shared Dimensions**: Both components must use the same `d_model` to ensure compatibility\n",
        "\n",
        "#### The Translation Flow:\n",
        "**Example in Portuguese**\n",
        "\n",
        "```\n",
        "English \"I love cats\" â†’ [ENCODER] â†’ Memory (understanding)\n",
        "                                          â†“\n",
        "Portuguese \"<sos> Eu amo\" â†’ [DECODER + Memory] â†’ \"gatos\" (next word)\n",
        "```\n",
        "\n",
        "#### Expected Output:\n",
        "\n",
        "After completing this exercise, you'll have a model that:\n",
        "- Accepts source sentences in one language\n",
        "- Accepts partial target sentences during training\n",
        "- Outputs probability distributions over target vocabulary\n",
        "- Can be trained end-to-end for translation\n",
        "\n",
        "<details>\n",
        "<summary><h4 style=\"color:green; cursor:pointer;\">ðŸ’¡ Click here for detailed hints</h4></summary>\n",
        "\n",
        "### Hint 1: Instantiating the Encoder\n",
        "\n",
        "**What you need**: Create an encoder instance for the source language.\n",
        "\n",
        "**Key points**:\n",
        "- Use the `Encoder` class you implemented in Exercise 1\n",
        "- The encoder needs to know the source vocabulary size\n",
        "- Pass `src_vocab_size` for the `vocab_size` parameter\n",
        "- Pass all the other model parameters: `d_model`, `nhead`, `num_enc_layers` (for `num_layers`), etc.\n",
        "- Make sure to use `num_enc_layers` (not `num_dec_layers`) for the encoder depth\n",
        "\n",
        "**Example structure**:\n",
        "```python\n",
        "self.encoder = Encoder(\n",
        "    vocab_size=?,          # Source vocabulary size\n",
        "    d_model=?,             # Model dimension\n",
        "    nhead=?,               # Number of attention heads\n",
        "    num_layers=?,          # Number of ENCODER layers\n",
        "    dim_feedforward=?,     # Feedforward dimension\n",
        "    max_len=?,             # Maximum sequence length\n",
        "    dropout=?              # Dropout rate\n",
        ")\n",
        "```\n",
        "\n",
        "**What parameters to use?**\n",
        "- `vocab_size` â†’ `src_vocab_size` (from `__init__` parameters)\n",
        "- `num_layers` â†’ `num_enc_layers` (from `__init__` parameters)\n",
        "- All others â†’ pass directly from `__init__` parameters\n",
        "\n",
        "---\n",
        "\n",
        "### Hint 2: Instantiating the Decoder\n",
        "\n",
        "**What you need**: Create a decoder instance for the target language.\n",
        "\n",
        "**Key points**:\n",
        "- Use the `Decoder` class you implemented in Exercise 2\n",
        "- The decoder needs to know the target vocabulary size\n",
        "- Pass `tgt_vocab_size` for the `vocab_size` parameter\n",
        "- Pass all the other model parameters\n",
        "- Use `num_dec_layers` (not `num_enc_layers`) for the decoder depth\n",
        "\n",
        "**Example structure**:\n",
        "```python\n",
        "self.decoder = Decoder(\n",
        "    vocab_size=?,          # Target vocabulary size\n",
        "    d_model=?,             # Model dimension (must match encoder!)\n",
        "    nhead=?,               # Number of attention heads\n",
        "    num_layers=?,          # Number of DECODER layers\n",
        "    dim_feedforward=?,     # Feedforward dimension\n",
        "    max_len=?,             # Maximum sequence length\n",
        "    dropout=?              # Dropout rate\n",
        ")\n",
        "```\n",
        "\n",
        "**What parameters to use?**\n",
        "- `vocab_size` â†’ `tgt_vocab_size` (from `__init__` parameters)\n",
        "- `num_layers` â†’ `num_dec_layers` (from `__init__` parameters)\n",
        "- All others â†’ pass directly from `__init__` parameters\n",
        "\n",
        "**Critical**: The `d_model` must be the same for both encoder and decoder!\n",
        "\n",
        "---\n",
        "\n",
        "### Hint 3: Forward Pass - Encoding the Source\n",
        "\n",
        "**What you need**: Pass the source sequence through the encoder.\n",
        "\n",
        "**Key points**:\n",
        "- Call `self.encoder` with the source sequence (`x`)\n",
        "- The encoder returns TWO things:\n",
        "  1. `memory`: The contextualized representations of the source\n",
        "  2. `src_padding_mask`: Mask indicating which positions are padding\n",
        "- Store both because you'll need them for the decoder\n",
        "\n",
        "**Example structure**:\n",
        "```python\n",
        "memory, src_padding_mask = self.encoder(?)\n",
        "```\n",
        "\n",
        "**What to pass?**\n",
        "- The source sequence tensor `x` (shape: `[batch_size, src_seq_len]`)\n",
        "\n",
        "---\n",
        "\n",
        "### Hint 4: Forward Pass - Decoding to Target\n",
        "\n",
        "**What you need**: Pass the target sequence and encoder memory through the decoder.\n",
        "\n",
        "**Key points**:\n",
        "- Call `self.decoder` with THREE arguments:\n",
        "  1. Target sequence (`tgt`)\n",
        "  2. Encoder memory (`memory`)\n",
        "  3. Source padding mask (`src_padding_mask` or `memory_padding_mask` parameter name in decoder)\n",
        "- The decoder returns logits over the target vocabulary\n",
        "\n",
        "**Example structure**:\n",
        "```python\n",
        "output = self.decoder(\n",
        "    ?,      # Target sequence\n",
        "    ?,      # Encoder memory\n",
        "    ?       # Source padding mask\n",
        ")\n",
        "```\n",
        "\n",
        "**What to pass?**\n",
        "- First argument: `tgt` (target sequence tensor)\n",
        "- Second argument: `memory` (from encoder)\n",
        "- Third argument: `src_padding_mask` (from encoder)\n",
        "\n",
        "---\n",
        "\n",
        "### Complete Flow Visualization:\n",
        "\n",
        "```\n",
        "Source (English)                Target (French)\n",
        "     â†“                               â†“\n",
        "  [x indices]                    [tgt indices]\n",
        "     â†“                               â†“\n",
        "  ENCODER                        (wait for memory)\n",
        "     â†“                               â†“\n",
        "  memory + mask  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  DECODER\n",
        "                                     â†“\n",
        "                                  [logits]\n",
        "```\n",
        "\n",
        "**Key Understanding**:\n",
        "1. Encoder processes the source **independently** first\n",
        "2. Encoder produces memory (understanding of source)\n",
        "3. Decoder uses both target tokens AND encoder memory\n",
        "4. Cross-attention allows decoder to \"look back\" at source while generating target\n",
        "\n",
        "---\n",
        "\n",
        "### Common Mistakes to Avoid:\n",
        "\n",
        "âŒ **Mistake 1**: Passing `num_enc_layers` to decoder or `num_dec_layers` to encoder\n",
        "âœ… **Correct**: Use `num_enc_layers` for encoder, `num_dec_layers` for decoder\n",
        "\n",
        "âŒ **Mistake 2**: Passing `src_vocab_size` to decoder or `tgt_vocab_size` to encoder\n",
        "âœ… **Correct**: Use `src_vocab_size` for encoder, `tgt_vocab_size` for decoder\n",
        "\n",
        "âŒ **Mistake 3**: Only passing `tgt` to decoder\n",
        "âœ… **Correct**: Decoder needs `tgt`, `memory`, AND `memory_padding_mask`\n",
        "\n",
        "âŒ **Mistake 4**: Forgetting to unpack encoder outputs\n",
        "âœ… **Correct**: `memory, src_padding_mask = self.encoder(x)`\n",
        "\n",
        "---\n",
        "\n",
        "### Testing Your Implementation:\n",
        "\n",
        "After implementing, you should be able to:\n",
        "```python\n",
        "model = EncoderDecoder(src_vocab_size=1000, tgt_vocab_size=1500)\n",
        "src = torch.randint(0, 1000, (32, 20))  # Batch of 32, length 20\n",
        "tgt = torch.randint(0, 1500, (32, 15))  # Batch of 32, length 15\n",
        "output = model(src, tgt)\n",
        "# output shape: [32, 15, 1500]\n",
        "```\n",
        "\n",
        "The output shape should be `[batch_size, tgt_seq_len, tgt_vocab_size]`\n",
        "\n",
        "</details>\n",
        "\n",
        "You're implementing the same architecture used in production translation systems!\n",
        "\n",
        "Let's complete your translation model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "f963caba-90af-4431-ae24-74be8ae9e0fe",
      "metadata": {
        "deletable": false,
        "editable": true,
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete Encoder-Decoder translation model combining encoder and decoder modules\n",
        "    \"\"\"\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=256, nhead=8,\n",
        "                 num_enc_layers=3, num_dec_layers=3, dim_feedforward=512,\n",
        "                 max_len=100, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        ### START CODE HERE ###\n",
        "        \n",
        "        # Initialize encoder for source language with source vocabulary size\n",
        "        self.encoder = Encoder(\n",
        "            vocab_size=src_vocab_size,\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_layers=num_enc_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            max_len=max_len,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        \n",
        "        # Initialize translation decoder for target language with target vocabulary size  \n",
        "        self.decoder = Decoder(\n",
        "            vocab_size=tgt_vocab_size,\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_layers=num_dec_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            max_len=max_len,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        \n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "    def forward(self, src, tgt):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Source language token indices [batch_size, src_seq_len]\n",
        "            tgt: Target language token indices [batch_size, tgt_seq_len]\n",
        "        Returns:\n",
        "            output: Predicted token logits [batch_size, tgt_seq_len, tgt_vocab_size]\n",
        "        \"\"\"\n",
        "        ### START CODE HERE ###\n",
        "        \n",
        "        # Encode the source sequence to get memory and source padding mask\n",
        "        memory, src_padding_mask = self.encoder(src)\n",
        "        \n",
        "        # Decode using encoder memory to generate target sequence predictions\n",
        "        output = self.decoder(tgt, memory, src_padding_mask)\n",
        "        \n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "478633fa-e686-41c0-96f7-4d65a4000efa",
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create EncoderDecoder model\n",
        "model = EncoderDecoder(\n",
        "    src_vocab_size=5000, \n",
        "    tgt_vocab_size=5000, \n",
        "    d_model=256, \n",
        "    nhead=8, \n",
        "    num_enc_layers=3,\n",
        "    num_dec_layers=3\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Show the summary\n",
        "helper_utils.show_encoderdecoder_layers(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2635bcef-f9c1-447d-a984-3a55aa0085d3",
      "metadata": {},
      "source": [
        "**Expected Output**\n",
        "```\n",
        "\n",
        "======================================================================\n",
        " EncoderDecoder - Main Components\n",
        "======================================================================\n",
        "\n",
        "Component                      Type                           Parameters\n",
        "----------------------------------------------------------------------\n",
        "encoder                        Encoder                         2,861,312\n",
        "decoder                        Decoder                         4,937,352\n",
        "----------------------------------------------------------------------\n",
        "TOTAL                                                          7,798,664\n",
        "======================================================================\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "bccabb30-400a-49b7-a5ca-1d95e442889b",
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "unittests.exercise_3(EncoderDecoder, Encoder, Decoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dca4afb-aca3-420a-b269-2fd1619ea87f",
      "metadata": {},
      "source": [
        "<a id='4-4'></a>\n",
        "### 4.4 Instantiating the Model\n",
        "\n",
        "Now you'll create an instance of the Translator model with the vocabularies you built earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "4fe49fff-9bc6-47aa-b342-ed55113df03d",
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "# Model hyperparameters (in section 4.4)\n",
        "D_MODEL = 256\n",
        "NHEAD = 8\n",
        "NUM_ENC_LAYERS = 3\n",
        "NUM_DEC_LAYERS = 3\n",
        "DIM_FEEDFORWARD = 512\n",
        "DROPOUT = 0.1\n",
        "\n",
        "# Create the model with dynamic vocabulary sizes\n",
        "model = EncoderDecoder(\n",
        "    src_vocab_size=len(eng_vocab),\n",
        "    tgt_vocab_size=len(tgt_vocab),  # Uses target language vocab\n",
        "    d_model=D_MODEL,\n",
        "    nhead=NHEAD,\n",
        "    num_enc_layers=NUM_ENC_LAYERS,\n",
        "    num_dec_layers=NUM_DEC_LAYERS,\n",
        "    dim_feedforward=DIM_FEEDFORWARD,\n",
        "    max_len=MAX_LENGTH,\n",
        "    dropout=DROPOUT\n",
        ").to(device)\n",
        "\n",
        "helper_utils.show_encoderdecoder_layers(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e62899e8-be0a-41f9-9e86-d4b69889b3d0",
      "metadata": {},
      "source": [
        "<a id='4-5'></a>\n",
        "### 4.5 Testing the Model Forward Pass\n",
        "\n",
        "Let's verify that the model works correctly by passing a sample batch through it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "62f7ba79-27b8-4eb8-b9fc-da5e60980a1a",
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "# Test the model with a sample batch\n",
        "for src_batch, tgt_batch in train_loader:\n",
        "    src_batch = src_batch.to(device)\n",
        "    tgt_batch = tgt_batch.to(device)\n",
        "    \n",
        "    # Use all but last token as input to decoder\n",
        "    tgt_input = tgt_batch[:, :-1]\n",
        "    \n",
        "    # Forward pass\n",
        "    output = model(src_batch, tgt_input)\n",
        "    \n",
        "    print(f\"Source shape: {src_batch.shape}\")\n",
        "    print(f\"Target input shape: {tgt_input.shape}\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(f\"Output dimension matches target vocabulary: {output.shape[-1] == len(tgt_vocab)}\")\n",
        "    \n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "737efa71-e031-405c-b8f9-40be2336559c",
      "metadata": {},
      "source": [
        "**Expected Output**\n",
        "```\n",
        "Source shape: torch.Size([64, 40])\n",
        "Target input shape: torch.Size([64, 39])\n",
        "Output shape: torch.Size([64, 39, 11603])\n",
        "Output dimension matches target vocabulary: True\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c079da2-ff25-43f6-9013-803b15f845b7",
      "metadata": {},
      "source": [
        "<a id='5'></a>\n",
        "## 5 - Training the Translator\n",
        "\n",
        "<a id='5-1'></a>\n",
        "### 5.1 Initializing Training Components\n",
        "\n",
        "You will now initialize the optimizer and the loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "d20400ad-5499-4879-a9a7-01dec4ba301a",
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "# Initialize optimizer and criterion\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding tokens\n",
        "\n",
        "print(f\"Training setup:\")\n",
        "print(f\"  Optimizer: Adam (lr=0.001)\")\n",
        "print(f\"  Loss function: CrossEntropyLoss\")\n",
        "print(f\"  Training batches: {len(train_loader)}\")\n",
        "print(f\"  Validation batches: {len(val_loader)}\")\n",
        "print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b73fd28b-01b0-4f22-a55c-e85bcbe16895",
      "metadata": {},
      "source": [
        "<a id='5-2'></a>\n",
        "### 5.2 Train the Model\n",
        "\n",
        "Now you'll train the translator model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "5de1a815-eb8c-4e48-be37-d8ffd18ac889",
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "# Train the model (increase number of epochs to get better results but longer training time)\n",
        "NUM_EPOCHS = 2\n",
        "\n",
        "print(\"Starting training...\")\n",
        "history = helper_utils.train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    num_epochs=NUM_EPOCHS\n",
        ")\n",
        "\n",
        "print(\"Training completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee871cab-6aa4-4f9f-b8a9-6c3418a63131",
      "metadata": {},
      "source": [
        "<a id='5-3'></a>\n",
        "### 5.3 Visualize Training Progress\n",
        "\n",
        "Plot the training and validation losses to see how the model learned over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "1dfaad3d-106a-47c8-abab-8f50c40165d8",
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "helper_utils.plot_training_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba72b389-55f0-4f2e-bc97-a2ff4ab682fc",
      "metadata": {},
      "source": [
        "## 5.5 Generating Translations with the Trained Model\n",
        "\n",
        "Now that you've trained your encoder-decoder model, it's time to see it in action! In this section, you'll implement a translation function that takes an English sentence and generates its translation in your target language.\n",
        "\n",
        "### How Translation Generation Works\n",
        "\n",
        "Unlike training (where you have the complete target sequence), during inference you need to generate the translation token by token. This process is called **autoregressive generation**:\n",
        "\n",
        "1. **Start with `<sos>`**: Begin the target sequence with the start-of-sequence token\n",
        "2. **Generate one token at a time**: Feed the current partial translation to the decoder\n",
        "3. **Select the next word**: Choose the most likely next token from the model's predictions\n",
        "4. **Repeat**: Add the predicted token to the sequence and continue\n",
        "5. **Stop at `<eos>`**: End generation when the model predicts the end-of-sequence token\n",
        "\n",
        "### Greedy Decoding\n",
        "\n",
        "The implementation below uses **greedy decoding** - at each step, you simply choose the token with the highest probability. While this doesn't always produce the best translation (beam search would be better), it's simple and effective for demonstration purposes.\n",
        "\n",
        "### The Translation Function\n",
        "\n",
        "The following function handles the complete translation pipeline:\n",
        "- Preprocessing the input sentence (tokenization, conversion to indices)\n",
        "- Managing padding and tensor formatting\n",
        "- Performing the autoregressive generation\n",
        "- Converting the output back to readable text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "275020a6-315e-4e1e-ae8b-4e027eeeda34",
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "def translate_sentence(model, sentence, src_word2idx, tgt_idx2word, tokenizer, max_length=20, temperature=1.0, debug=False):\n",
        "    \"\"\"\n",
        "    Translate a single sentence using greedy decoding with optional temperature sampling\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Create reverse mapping for target vocabulary\n",
        "    tgt_word2idx = {word: idx for idx, word in tgt_idx2word.items()}\n",
        "    \n",
        "    # Tokenize and convert to indices\n",
        "    tokens = tokenizer(sentence.lower())\n",
        "    src_indices = [src_word2idx.get(token, src_word2idx['<unk>']) for token in tokens]\n",
        "    \n",
        "    if debug:\n",
        "        print(f\"Input tokens: {tokens}\")\n",
        "        print(f\"Input indices: {src_indices}\")\n",
        "    \n",
        "    # Pad source to max_length\n",
        "    if len(src_indices) < max_length:\n",
        "        src_indices = src_indices + [src_word2idx['<pad>']] * (max_length - len(src_indices))\n",
        "    else:\n",
        "        src_indices = src_indices[:max_length]\n",
        "    \n",
        "    # Convert to tensor and add batch dimension\n",
        "    src_tensor = torch.tensor(src_indices).unsqueeze(0).to(device)\n",
        "    \n",
        "    # Generate translation using autoregressive decoding\n",
        "    with torch.no_grad():\n",
        "        # Get encoder output and source padding mask\n",
        "        encoder_memory, src_padding_mask = model.encoder(src_tensor)\n",
        "        \n",
        "        if debug:\n",
        "            print(f\"Encoder memory shape: {encoder_memory.shape}\")\n",
        "            print(f\"Source padding mask shape: {src_padding_mask.shape}\")\n",
        "        \n",
        "        # Start with <sos> token\n",
        "        tgt_indices = [tgt_word2idx['<sos>']]\n",
        "        \n",
        "        for step in range(max_length - 1):\n",
        "            # Create target tensor with only tokens generated so far\n",
        "            tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0).to(device)\n",
        "            \n",
        "            # Pass to decoder with encoder memory\n",
        "            decoder_output = model.decoder(\n",
        "                tgt_tensor, \n",
        "                memory=encoder_memory,\n",
        "                memory_padding_mask=src_padding_mask\n",
        "            )\n",
        "            \n",
        "            # Get prediction for the NEXT token (from last position)\n",
        "            next_token_logits = decoder_output[0, -1, :]  # Last position predicts next token\n",
        "            \n",
        "            if debug and step < 3:\n",
        "                print(f\"Step {step}: Logits shape: {next_token_logits.shape}\")\n",
        "                print(f\"Step {step}: Top 5 logits: {torch.topk(next_token_logits, 5)}\")\n",
        "            \n",
        "            # Apply temperature for more diverse sampling\n",
        "            if temperature != 1.0:\n",
        "                next_token_logits = next_token_logits / temperature\n",
        "            \n",
        "            # Use sampling instead of pure greedy for better diversity\n",
        "            if temperature > 1.0:\n",
        "                probs = torch.softmax(next_token_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, 1).item()\n",
        "            else:\n",
        "                next_token = torch.argmax(next_token_logits).item()\n",
        "            \n",
        "            if debug and step < 3:\n",
        "                print(f\"Step {step}: Selected token: {next_token} ({tgt_idx2word.get(next_token, 'UNK')})\")\n",
        "            \n",
        "            # Add predicted token to sequence\n",
        "            tgt_indices.append(next_token)\n",
        "            \n",
        "            # Stop if <eos> token is generated\n",
        "            if next_token == tgt_word2idx['<eos>']:\n",
        "                break\n",
        "    \n",
        "    # Convert indices to words (exclude special tokens)\n",
        "    translated_tokens = [tgt_idx2word[idx] for idx in tgt_indices \n",
        "                        if idx not in [tgt_word2idx['<pad>'], tgt_word2idx['<sos>'], tgt_word2idx['<eos>']]]\n",
        "    \n",
        "    if debug:\n",
        "        print(f\"Final target indices: {tgt_indices}\")\n",
        "        print(f\"Translated tokens: {translated_tokens}\")\n",
        "    \n",
        "    return ' '.join(translated_tokens)\n",
        "\n",
        "# Quick debug version\n",
        "def debug_translate(model, sentence, src_word2idx, tgt_idx2word, tokenizer, max_length=20):\n",
        "    \"\"\"Debug version with temperature and verbose output\"\"\"\n",
        "    print(f\"\\nðŸ” DEBUG: Translating '{sentence}'\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Try with different temperatures\n",
        "    print(\"\\n1. Greedy (temperature=1.0):\")\n",
        "    result1 = translate_sentence(model, sentence, src_word2idx, tgt_idx2word, tokenizer, \n",
        "                               max_length, temperature=1.0, debug=True)\n",
        "    print(f\"Result: '{result1}'\")\n",
        "    \n",
        "    print(\"\\n2. With temperature=1.5:\")\n",
        "    result2 = translate_sentence(model, sentence, src_word2idx, tgt_idx2word, tokenizer, \n",
        "                               max_length, temperature=1.5, debug=False)\n",
        "    print(f\"Result: '{result2}'\")\n",
        "    \n",
        "    print(\"\\n3. With temperature=2.0:\")\n",
        "    result3 = translate_sentence(model, sentence, src_word2idx, tgt_idx2word, tokenizer, \n",
        "                               max_length, temperature=2.0, debug=False)\n",
        "    print(f\"Result: '{result3}'\")\n",
        "    \n",
        "    return result1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c361e27e-d9a2-48df-b6c8-091e399ea38f",
      "metadata": {},
      "source": [
        "<a id='5-4'></a>\n",
        "### 5.4 Test Translation on Training Examples\n",
        "\n",
        "First, you'll test the translation on some examples from the training set to see if the model learned the patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "21208bd0-9c2a-4382-9e6f-0698b95770c8",
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "# Test on some training examples\n",
        "print(\"Testing on training examples:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Select a few examples from the training set\n",
        "test_indices = [0, 100, 200, 300, 400]\n",
        "\n",
        "for idx in test_indices:\n",
        "    eng_sentence, fra_reference = normalized_pairs[idx]\n",
        "    \n",
        "    # Translate\n",
        "    translation = translate_sentence(model, eng_sentence, eng_word2idx, tgt_idx2word, tokenizer)\n",
        "    \n",
        "    print(f\"English:    {eng_sentence}\")\n",
        "    print(f\"Reference:  {fra_reference}\")\n",
        "    print(f\"Translated: {translation}\")\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76ef9213-120d-4eb2-bf18-a2b42aec9139",
      "metadata": {},
      "source": [
        "<a id='6'></a>\n",
        "## 6 - Conclusion\n",
        "\n",
        "Great job completing this assignment! You've successfully built a complete encoder-decoder transformer architecture for machine translation from scratch.\n",
        "\n",
        "### What You Built\n",
        "\n",
        "Throughout this assignment, you implemented:\n",
        "\n",
        "- An encoder that processes and understands source language sentences\n",
        "- A decoder that generates translations in the target language  \n",
        "- The complete translation system that combines both components\n",
        "- A training pipeline that teaches the model to translate between languages\n",
        "- An inference system that generates translations for new sentences\n",
        "\n",
        "### Key Concepts You've Mastered\n",
        "\n",
        "By completing this implementation, you now understand:\n",
        "\n",
        "1. How encoders create contextualized representations of input text\n",
        "2. How decoders generate output sequences using cross-attention to the encoder\n",
        "3. The role of attention masks in handling variable-length sequences and preventing information leakage\n",
        "4. The difference between training (with teacher forcing) and inference (autoregressive generation)\n",
        "5. How positional encodings give transformers awareness of word order\n",
        "\n",
        "### Why This Matters\n",
        "\n",
        "The architecture you built forms the foundation of modern machine translation systems and many other NLP applications. This encoder-decoder pattern appears in tasks ranging from text summarization to question answering. The hands-on experience you've gained implementing these components from scratch gives you deep insight into how state-of-the-art NLP models actually work."
      ]
    }
  ],
  "metadata": {
    "grader_version": "1",
    "kernelspec": {
      "display_name": ".venv (3.12.6)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
