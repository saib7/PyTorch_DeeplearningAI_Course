{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7db819c6-738b-401a-8d55-0ba897b33bdc",
   "metadata": {},
   "source": [
    "# Self-Attention: Building the Foundation of Transformers\n",
    "\n",
    "## Understanding Attention Through Implementation\n",
    "\n",
    "In this notebook, you'll build self-attention from scratch to understand the mechanism that revolutionized natural language processing. Rather than treating attention as a black box, you'll see exactly how it computes relationships between words, why it's so powerful, and how it learns to focus on relevant context.\n",
    "\n",
    "Self-attention is the core innovation that enables models like BERT, GPT, and other transformers to understand language. By the end of this notebook, you'll have implemented the same attention mechanism used in these state-of-the-art models, working through simple examples to build deep intuition about how and why it works.\n",
    "\n",
    "Let's demystify attention by coding each component step by step, visualizing the computations, and seeing how different patterns emerge from the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c239046a-ba84-4619-a9d2-9a9c838bafeb",
   "metadata": {},
   "source": [
    "## 1 - Data Preparation\n",
    "\n",
    "### 1.1 Introduction\n",
    "\n",
    "Before you can train any language model, you need to convert raw text into a numerical format that the model can understand. This fundamental step transforms human-readable text into the mathematical representations that neural networks require.\n",
    "\n",
    "You'll start by preparing a **toy dataset**: just a few simple sentences. This small scale lets you trace through every computation and truly understand what's happening inside the attention mechanism.\n",
    "\n",
    "As part of this process, you'll build a vocabulary that includes:\n",
    "- A **padding token** (`<pad>`) for making all sequences the same length in a batch\n",
    "- An **unknown token** (`<unk>`) to handle any rare or out-of-vocabulary words the model might encounter\n",
    "\n",
    "These special tokens are essential for handling real-world text where sequences vary in length and new words may appear during inference.\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f59ab8-e7d6-4e94-8185-13b0ca56975e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import re\n",
    "import urllib.request\n",
    "from collections import Counter\n",
    "from typing import Callable, Dict, List, Optional, Sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757ed030-3dbf-46b9-8215-a40462c24eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Tiny toy dataset\n",
    "sentences = \"\"\"\n",
    "the dog chased the cat\n",
    "the cat chased the mouse\n",
    "the dog ran fast\n",
    "the mouse ran fast\n",
    "the cat lay down\n",
    "\"\"\"\n",
    "\n",
    "# Build vocab\n",
    "tokens = sentences.split()\n",
    "vocab = ['<pad>', '<unk>'] + sorted(set(tokens))\n",
    "word2idx = {w:i for i,w in enumerate(vocab)}\n",
    "idx2word = {i:w for w,i in word2idx.items()}\n",
    "print(\"Vocab:\", vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0a6174-f358-4c4f-8616-c1078de96bc5",
   "metadata": {},
   "source": [
    "### 1.2 The Tokenizer\n",
    "\n",
    "A fundamental step in any NLP pipeline is **tokenization**. A tokenizer splits raw text into meaningful chunks (tokens)—typically words—that become the basic units your model processes. Since neural networks work with numbers, not text, the tokenizer also handles the crucial task of mapping these tokens to unique numerical IDs.\n",
    "\n",
    "Below, you'll define a simple Python tokenizer as a class. This version uses regular expressions to extract whole words while normalizing the text by:\n",
    "- Converting everything to lowercase for consistency\n",
    "- Removing punctuation to focus on word meanings\n",
    "- Splitting on whitespace and word boundaries\n",
    "\n",
    "This clean, simplified approach is perfect for understanding attention mechanics without getting distracted by complex linguistic edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094556f3-46c7-42a1-9316-c62dd2bdec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    \"\"\"Splits on whitespace and lowercases, with optional regex for real word tokens.\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, text):\n",
    "        # Option 1: Basic split (uncomment to just split on spaces)\n",
    "        # return text.lower().split()\n",
    "        # Option 2: More robust - returns only word tokens (ignores punctuation)\n",
    "        return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "# Usage example:\n",
    "tokenizer = SimpleTokenizer()\n",
    "tokens = tokenizer(\"The Dog chased the Cat.\")\n",
    "print(tokens)  # Output: ['the', 'dog', 'chased', 'the', 'cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b30cc-caae-446c-b68f-00b77a6e7b47",
   "metadata": {},
   "source": [
    "### 1.3 Building the Vocabulary\n",
    "\n",
    "Once you have a tokenizer that splits your sentences into individual words, the next step is to build a **vocabulary**—a list of all unique tokens that appear in your dataset.\n",
    "\n",
    "This vocabulary is crucial because it links each word to a unique ID, which your model will use throughout training and inference. You'll also want to include two special tokens in your vocabulary: one for padding (`<pad>`) and one for unknown or out-of-vocabulary tokens (`<unk>`).\n",
    "\n",
    "Below you'll find a function to create:\n",
    "- the vocabulary list,\n",
    "- a dictionary that maps each token to its unique index (**word2idx**),\n",
    "- and a reverse mapping from indices to words (**idx2word**).\n",
    "\n",
    "You can adjust the `min_freq` parameter to exclude rare words and keep your vocabulary more manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06825ece-1175-4461-b23e-c8b54b54f4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, tokenizer, min_freq=1):\n",
    "    counter = Counter()  # Counter to count word frequencies in all sentences\n",
    "    for sent in sentences:\n",
    "        counter.update(tokenizer(sent))  # Tokenize sentence and add token counts\n",
    "\n",
    "    # Start vocab with special tokens, then add words meeting min_freq threshold\n",
    "    vocab = ['<pad>', '<unk>'] + [w for w, c in counter.items() if c >= min_freq]\n",
    "\n",
    "    # Create a mapping from word to unique index\n",
    "    word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "\n",
    "    # Create a mapping from index back to word (inverse of word2idx)\n",
    "    idx2word = {i: w for i, w in enumerate(vocab)}\n",
    "\n",
    "    # Return the vocab list and the two dictionaries\n",
    "    return vocab, word2idx, idx2word\n",
    "\n",
    "# Using our sample sentences and tokenizer\n",
    "sentences = [\n",
    "    \"the dog chased the cat\",\n",
    "    \"the cat chased the mouse\",\n",
    "    \"the dog ran fast\",\n",
    "    \"the mouse ran fast\",\n",
    "    \"the cat lay down\"\n",
    "]\n",
    "\n",
    "tokenizer = SimpleTokenizer()                 # Define the tokenizer (splits into lowercase words)\n",
    "vocab, word2idx, idx2word = build_vocab(sentences, tokenizer)  # Build vocab & mappings\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f6466b-fc5a-4f83-919b-096e3276f769",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'dog'\n",
    "id_word = word2idx[word]\n",
    "print(f\"ID for word = {word}: {id_word}\")\n",
    "print(f\"Word for ID = {id_word}: {idx2word[id_word]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910341bd-053e-43f7-9d1a-a24399add1a5",
   "metadata": {},
   "source": [
    "### 1.4 Sliding Windows\n",
    "\n",
    "Now that you have a list of tokenized and numericalized sentences, you need to organize your data into **(input, target)** pairs suitable for training a language model.\n",
    "\n",
    "The most common approach for this is to use a **sliding window**: you select a fixed-size window of words as input, and train the model to predict the next word that follows the window. By moving this window across every sentence in your dataset, you generate many (input, target) examples for training.\n",
    "\n",
    "This method helps your model learn the sequential structure of language, so it can anticipate what comes next given a context. For example:\n",
    "\n",
    "| Step | Input                              | Target  |\n",
    "|------|------------------------------------|---------|\n",
    "|  1   | the, dog, chased, the              | cat     |\n",
    "|  2   | dog, chased, the, cat              | quickly |\n",
    "\n",
    "Now you'll implement this sliding window approach in code. This will prepare your input and target lists, ready for training your attention-based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c942be-f76a-425c-8276-20a6d9ee3d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizer()\n",
    "vocab, word2idx, idx2word = build_vocab(sentences, tokenizer)\n",
    "\n",
    "# 2. Parameters\n",
    "SEQ_LEN = 4   # Length of input sequence for each example\n",
    "\n",
    "# 3. Convert sentences to token ID lists\n",
    "encoded_sentences = []  # Will be a list of lists of token IDs\n",
    "for sent in sentences:\n",
    "    tokens = tokenizer(sent)  # Split sentence into tokens\n",
    "    ids = [word2idx.get(tok, word2idx['<unk>']) for tok in tokens]  # Map tokens to IDs\n",
    "    encoded_sentences.append(ids)\n",
    "\n",
    "# 4. Create sliding window dataset (inputs, targets)\n",
    "inputs = []\n",
    "targets = []\n",
    "for ids in encoded_sentences:\n",
    "    # For each possible window in the sentence\n",
    "    for i in range(len(ids) - SEQ_LEN):\n",
    "        window = ids[i:i+SEQ_LEN]        # Input: SEQ_LEN-token window\n",
    "        target = ids[i+SEQ_LEN]          # Target: next token after the window\n",
    "        inputs.append(window)\n",
    "        targets.append(target)\n",
    "\n",
    "# 5. Let's show the dataset as text for illustration\n",
    "for inp, tgt in zip(inputs, targets):\n",
    "    inp_words = [idx2word[i] for i in inp]\n",
    "    tgt_word = idx2word[tgt]\n",
    "    print(f\"Input: {inp_words}  →  Target: {tgt_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34962401-3d82-43ce-9605-90e0b10ea5ef",
   "metadata": {},
   "source": [
    "### 1.5 Turning the Data Into a PyTorch Dataset\n",
    "\n",
    "Now that you’ve prepared your `inputs` and `targets` lists, it’s time to convert them into a format that PyTorch can use for training: a `Dataset`.\n",
    "\n",
    "A PyTorch `Dataset` is a simple class structure that knows how to return an input–target pair given an index. This makes it easy to batch your data and feed it into a model, especially for larger sets of examples.\n",
    "\n",
    "Below, you’ll see how to wrap your lists in a `Dataset`, and then use a `DataLoader` to shuffle and batch your data automatically during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7bed41-ae76-428e-905c-eb899ad0348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        # Convert your list of input windows to a PyTorch tensor (for fast indexing)\n",
    "        self.inputs = torch.tensor(inputs, dtype=torch.long)\n",
    "        # Convert your target word indices to a PyTorch tensor\n",
    "        self.targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of samples in the dataset\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return a tuple (input_window, target_word) for the given index\n",
    "        return self.inputs[idx], self.targets[idx]\n",
    "\n",
    "# Create an instance of your dataset\n",
    "dataset = TinyDataset(inputs, targets)\n",
    "\n",
    "# Create a DataLoader that will feed batches of data to your model during training.\n",
    "# batch_size=4: each batch will contain 4 (input, target) pairs\n",
    "# shuffle=True: randomize order each epoch to improve training\n",
    "# num_workers=0: no extra processes for loading data (good for small datasets)\n",
    "loader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb89f05-f18f-4cb7-b280-5a699116e50f",
   "metadata": {},
   "source": [
    "## 2 - Manual Self-Attention\n",
    "\n",
    "### 2.1 Introduction\n",
    "\n",
    "Now you'll dig into the heart of what makes Transformers powerful: **self-attention**.\n",
    "\n",
    "#### What is Self-Attention?\n",
    "\n",
    "Self-attention is a mechanism that allows each word in a sequence to \"look at\" and \"gather information from\" every other word, including itself. Unlike traditional sequential models that process words one by one, self-attention computes relationships between all words simultaneously.\n",
    "\n",
    "Think of it this way: when you read the sentence \"The cat sat on the mat because it was tired,\" your brain automatically knows that \"it\" refers to \"cat.\" Self-attention gives neural networks this same capability to understand relationships and dependencies between words, no matter how far apart they are.\n",
    "\n",
    "#### The Self-Attention Algorithm\n",
    "\n",
    "Here's how self-attention transforms a sequence:\n",
    "\n",
    "```\n",
    "Input: \"The cat sat\"\n",
    "         ↓\n",
    "   [Embeddings]\n",
    "         ↓\n",
    "   Q, K, V Projections\n",
    "         ↓\n",
    "   Attention Scores (Q × K^T)\n",
    "         ↓\n",
    "   Attention Weights (Softmax)\n",
    "         ↓\n",
    "   Weighted Sum (Weights × V)\n",
    "         ↓\n",
    "Output: Context-aware representations\n",
    "```\n",
    "\n",
    "#### The Core Components\n",
    "\n",
    "Self-attention operates through three learned transformations of your input:\n",
    "\n",
    "1. **Queries (Q)**: \"What information am I looking for?\"\n",
    "   - Each word generates a query vector representing what it wants to know\n",
    "\n",
    "2. **Keys (K)**: \"What information do I contain?\"\n",
    "   - Each word generates a key vector advertising what information it has\n",
    "\n",
    "3. **Values (V)**: \"What information will I actually provide?\"\n",
    "   - Each word generates a value vector containing its actual contribution\n",
    "\n",
    "#### The Attention Computation Flow\n",
    "\n",
    "```\n",
    "Step 1: Linear Projections\n",
    "Input → W_Q → Queries\n",
    "Input → W_K → Keys  \n",
    "Input → W_V → Values\n",
    "\n",
    "Step 2: Similarity Scores\n",
    "Scores = Q × K^T / √d_k\n",
    "(How relevant is each word to every other word?)\n",
    "\n",
    "Step 3: Attention Weights  \n",
    "Weights = Softmax(Scores)\n",
    "(Convert scores to probabilities)\n",
    "\n",
    "Step 4: Weighted Aggregation\n",
    "Output = Weights × V\n",
    "(Combine values based on attention weights)\n",
    "```\n",
    "\n",
    "#### Visual Example\n",
    "\n",
    "Consider the sentence \"The cat sat\":\n",
    "\n",
    "```\n",
    "Attention Weight Matrix:\n",
    "        The   cat   sat\n",
    "The   [0.6   0.3   0.1]  ← \"The\" pays most attention to itself\n",
    "cat   [0.2   0.7   0.1]  ← \"cat\" focuses mainly on itself  \n",
    "sat   [0.1   0.3   0.6]  ← \"sat\" attends to \"cat\" and itself\n",
    "\n",
    "Each row shows where that word \"looks\" for information\n",
    "```\n",
    "\n",
    "After training, these attention patterns become more sophisticated:\n",
    "- \"sat\" might learn to strongly attend to \"cat\" (the subject)\n",
    "- Articles like \"the\" might distribute attention more evenly\n",
    "- Pronouns would learn to attend to their antecedents\n",
    "\n",
    "#### Why Self-Attention is Powerful\n",
    "\n",
    "1. **Parallel Processing**: Unlike RNNs, all positions are processed simultaneously\n",
    "2. **Long-Range Dependencies**: Can directly connect distant words without sequential steps\n",
    "3. **Interpretability**: Attention weights show what the model is \"looking at\"\n",
    "4. **Flexibility**: Learns task-specific attention patterns automatically\n",
    "\n",
    "Step by step, you'll implement each part of the self-attention formula—projections, dot products, scaling, softmax, and the final weighted sum—all by hand. This careful, explicit walkthrough will help you truly understand how self-attention works under the hood. Start slow here; later on, you'll see how these same ideas power much more complex, real-world NLP models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267700f4-0bd3-4d4b-8b06-ab5155ad66a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualSelfAttention(nn.Module):\n",
    "    \"\"\"A custom PyTorch module that performs self-attention.\"\"\"\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        # Create three learnable linear projections: for queries, keys, and values\n",
    "        self.to_q = nn.Linear(d, d, bias=False)  # Projects input to \"query\" vectors\n",
    "        self.to_k = nn.Linear(d, d, bias=False)  # Projects input to \"key\" vectors\n",
    "        self.to_v = nn.Linear(d, d, bias=False)  # Projects input to \"value\" vectors\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq, d] (batch of token embeddings)\n",
    "        Q = self.to_q(x)   # [batch, seq, d] - Project input to queries\n",
    "        K = self.to_k(x)   # [batch, seq, d] - Project input to keys\n",
    "        V = self.to_v(x)   # [batch, seq, d] - Project input to values\n",
    "\n",
    "        # Compute scaled dot-product attention: Q @ K^T, scaled by sqrt(dim)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(Q.size(-1))  # [batch, seq, seq]\n",
    "\n",
    "        # Convert raw scores into probabilities with softmax (row-wise)\n",
    "        attn = F.softmax(scores, dim=-1)  # Each row sums to 1\n",
    "\n",
    "        # Compute the weighted sum of value vectors for each position\n",
    "        out = torch.matmul(attn, V)  # [batch, seq, d]\n",
    "\n",
    "        # Return the new context-aware representations and attention weights\n",
    "        return out, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a07c9a7-1d39-4905-99a9-4efa0ce2fd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"the dog chased the cat\"\n",
    "tokens = tokenizer(sentence)\n",
    "print(\"Tokens:\", tokens)  # ['dog', 'chased', 'cat']\n",
    "\n",
    "token_ids = [word2idx.get(tok, word2idx['<unk>']) for tok in tokens]\n",
    "print(\"Token IDs:\", token_ids)  # e.g. [2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ae6ab0-a83e-4773-9352-515eaf50f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 2\n",
    "# We'll make trainable embeddings for realism:\n",
    "embed = nn.Embedding(len(vocab), embedding_dim)\n",
    "torch.manual_seed(42)  # For reproducibility\n",
    "x = embed(torch.tensor(token_ids).unsqueeze(0))  # shape: (1, 3, 2)\n",
    "print(\"Input embeddings:\\n\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213fdd59-02d9-468f-b08b-20c7d42de300",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_layer = ManualSelfAttention(embedding_dim)\n",
    "\n",
    "# Put the input through self-attention\n",
    "out, attn = attn_layer(x)\n",
    "\n",
    "print(\"Attention weights:\\n\", attn[0].detach().numpy())\n",
    "print(\"Output representations:\\n\", out[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100cce7b-31ae-4c5d-a4a3-69842ae1bb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "print(\"idx2word:\", idx2word)\n",
    "\n",
    "print(\"\\nAttention Weights Matrix (rows: query token, columns: attended token):\")\n",
    "for i, w in enumerate(tokens):\n",
    "    row = [\"{:.2f}\".format(a) for a in attn[0, i].detach().cpu().numpy()]\n",
    "    print(f\"{w:>8} attends to -> {row}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e380ce-881c-4279-86e5-84e63c70fc11",
   "metadata": {},
   "source": [
    "### 2.2 Self-Attention with Position Embeddings\n",
    "\n",
    "#### Attention is Position-Blind\n",
    "\n",
    "Self-attention lets each word \"look at\" every other word in the sequence—but on its own, it treats the input like an unordered set of words. The attention mechanism itself has no inherent notion of sequence or position. Consider these two sentences:\n",
    "\n",
    "- \"The cat chased the dog\"\n",
    "- \"The dog chased the cat\"\n",
    "\n",
    "Without position information, self-attention would produce identical representations for both sentences since they contain the same words! This is clearly wrong—word order fundamentally changes meaning.\n",
    "\n",
    "#### The Solution: Adding Position Information\n",
    "\n",
    "To help the model distinguish word order, you give it a **positional embedding**: a unique vector for each position in the input sequence. These position vectors act like GPS coordinates for words, telling the model not just *what* each word is, but *where* it appears in the sequence:\n",
    "\n",
    "```\n",
    "Word Embedding:     [cat] → [0.2, 0.5, -0.1, ...]  (what the word means)\n",
    "Position Embedding: [pos 2] → [0.1, -0.3, 0.4, ...]  (where it appears)\n",
    "                           ↓\n",
    "Combined Input:     [0.3, 0.2, 0.3, ...]  (meaning + position)\n",
    "```\n",
    "\n",
    "By adding position embeddings to word embeddings, you create position-aware representations that allow self-attention to learn different patterns based on word order.\n",
    "\n",
    "#### Learned vs. Sinusoidal Positional Encodings\n",
    "\n",
    "**Note**: In this notebook, you're using **learned positional embeddings** (`nn.Embedding`) - this is the approach shown in the course lectures. The model learns the best position representations during training, just like it learns word embeddings.\n",
    "\n",
    "However, in future notebooks (starting with the Encoder lab), you'll see an alternative approach: **sinusoidal positional encodings**. Instead of learning position embeddings, sinusoidal encodings use fixed mathematical functions (sine and cosine waves at different frequencies) to represent positions.\n",
    "\n",
    "Both approaches work well, but they have different trade-offs:\n",
    "\n",
    "**Learned Embeddings** (used in this lab):\n",
    "- Flexible and can adapt to your specific data patterns\n",
    "- Simple to understand and implement\n",
    "- Limited to the maximum sequence length seen during training\n",
    "- Require `max_len × d_model` parameters\n",
    "\n",
    "**Sinusoidal Encodings** (used in later labs):\n",
    "- Fixed mathematical pattern that extends infinitely—can handle sequences longer than seen during training\n",
    "- Zero learnable parameters—the encodings are completely deterministic\n",
    "- Enable the model to easily learn relative positions (e.g., \"3 words to the left\")\n",
    "- Different frequencies capture both fine-grained (nearby words) and coarse-grained (distant words) relationships\n",
    "- Have become the standard in modern transformer implementations\n",
    "\n",
    "For this introduction to attention, learned embeddings are simpler to understand and work perfectly fine for our fixed-length sequences. You'll explore sinusoidal encodings in depth in the next lab.\n",
    "\n",
    "#### The Complete Architecture\n",
    "\n",
    "The code below implements a complete position-aware attention model with four key components:\n",
    "\n",
    "1. **Token Embeddings** (`tok_embed`): Converts token IDs to semantic vectors capturing word meaning\n",
    "2. **Position Embeddings** (`pos_embed`): Learnable vectors for each position (1st word, 2nd word, etc.)\n",
    "3. **Self-Attention** (`attn`): Computes context-aware representations using the manual attention from earlier\n",
    "4. **Output Projection** (`fc`): Maps the final hidden state to vocabulary size for next-word prediction\n",
    "\n",
    "The forward pass:\n",
    "1. Generates position indices (0, 1, 2, ...) for each token in the batch\n",
    "2. Looks up both word and position embeddings\n",
    "3. Sums them together (this is how position information gets injected)\n",
    "4. Applies self-attention to these position-aware embeddings\n",
    "5. Takes the last position's output to predict the next word\n",
    "6. Projects to vocabulary size to get prediction scores\n",
    "\n",
    "This architecture demonstrates the minimal components needed for a position-aware language model—the foundation that, when scaled up with multiple layers and heads, becomes the transformer models powering modern NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b1176b-61bf-405b-8db0-f417554224ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttnWithPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len, emb_dim):\n",
    "        super().__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, emb_dim)     # Word embedding lookup/table\n",
    "        self.pos_embed = nn.Embedding(seq_len, emb_dim)        # Learnable positional embeddings\n",
    "        self.attn = ManualSelfAttention(emb_dim)               # Our custom self-attention module\n",
    "        self.fc = nn.Linear(emb_dim, vocab_size)               # Final linear layer for next-word prediction\n",
    "        self.seq_len = seq_len                                 # Store sequence length\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        \"\"\"\n",
    "        token_ids: [batch_size, seq_len]  # Each int is a token index\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = token_ids.shape\n",
    "        # Create position indices for each token in the input sequence, shape: [batch_size, seq_len]\n",
    "        positions = torch.arange(seq_len, device=token_ids.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        word_vecs = self.tok_embed(token_ids)    # Look up word embeddings: [batch, seq, emb_dim]\n",
    "        pos_vecs  = self.pos_embed(positions)    # Look up position embeddings: [batch, seq, emb_dim]\n",
    "        input_vecs = word_vecs + pos_vecs        # Sum word and position embeddings (brings in order info)\n",
    "        attn_out, attn_weights = self.attn(input_vecs)  # Apply self-attention to order-aware embeddings\n",
    "        # For language modeling tasks, pick only the output at the last position to predict the next word\n",
    "        last_hidden = attn_out[:, -1, :]         \n",
    "        logits = self.fc(last_hidden)            # Map to prediction over vocab\n",
    "        return logits, attn_weights              # Return scores and raw attention weights for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894cd40e-4858-4bd1-8e3b-019dff8c9edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[[ 0.12, -0.55,  0.33,  0.10],\n",
    "                   [-0.44,  0.91, -0.12, -0.77],\n",
    "                   [ 0.48,  0.02,  0.05,  0.39],\n",
    "                   [ 0.12, -0.55,  0.33,  0.10],\n",
    "                   [-0.30,  0.14, -0.70,  0.81]]])  # [1, 5, 4]\n",
    "\n",
    "attn = ManualSelfAttention(d=4)\n",
    "out, attn_weights = attn(x)\n",
    "\n",
    "print(\"Attention weights matrix (attn_weights):\\n\", attn_weights[0].detach().numpy())\n",
    "print('\\nExplanation:')\n",
    "print(\"Each row i shows the attention distribution (softmaxed) over all positions in the input sequence,\")\n",
    "print(\"when computing the updated representation for token i. Rows sum to 1.\\n\")\n",
    "\n",
    "print(\"Self-attention output (out):\\n\", out[0].detach().numpy())\n",
    "print('\\nExplanation:')\n",
    "print(\"Each row is the new vector for input position i, computed as a weighted sum\")\n",
    "print(\"of the original value vectors, using that row from the attention weights matrix as weights.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671a4000-51d8-4977-b4a3-aa45e162524e",
   "metadata": {},
   "source": [
    "### 2.3 A Practical Example\n",
    "\n",
    "Now you’ll see self-attention with positional encoding in action!  \n",
    "In this section, you’ll:\n",
    "- Instantiate your custom self-attention model,\n",
    "- Select a real sample from your dataset,\n",
    "- Run the model to obtain predictions and attention maps,\n",
    "- Visualize exactly how the model \"pays attention\" to different words in the input.\n",
    "\n",
    "The code below walks you through these steps. You can run it before and after training to see how the model's focus changes as it learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085d6fa7-84c7-4beb-a3be-59846ccfc11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Set up model parameters and create your attention model\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 8         # Number of embedding dimensions (try 4, 8, 16, etc.)\n",
    "seq_len = SEQ_LEN     # Length of your training window\n",
    "model = SelfAttnWithPositionalEmbedding(vocab_size, seq_len, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336fa9bc-6cc1-48a5-8861-bc38c2811ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attn_weights, tokens, title=\"Self-Attention Map\"):\n",
    "    \"\"\"\n",
    "    Plots a self-attention map for a single input sequence.\n",
    "    attn_weights: tensor of shape [batch, seq_len, seq_len], typically from your model\n",
    "    tokens: list of token strings (for axis labels)\n",
    "    title: plot title string\n",
    "    \"\"\"\n",
    "    # Take attention weights for the first sample in the batch and move to CPU\n",
    "    aw = attn_weights[0].detach().cpu().numpy()\n",
    "    plt.figure(figsize=(1.2 * len(tokens), 5))  # Adjust figure size by sequence length\n",
    "    # Show the attention matrix as an image (color = strength of attention)\n",
    "    plt.imshow(aw, cmap='Blues')\n",
    "    # Label x- and y-axes with token words\n",
    "    plt.xticks(range(len(tokens)), tokens, rotation=45)\n",
    "    plt.yticks(range(len(tokens)), tokens)\n",
    "    # Add a colorbar to show attention strength scale\n",
    "    plt.colorbar()\n",
    "    # Display the plot title\n",
    "    plt.title(title)\n",
    "    # Neatly fit everything in the figure area\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1f3a34-765b-4ed3-870c-ad6b57e74da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Select a sample input from your training data\n",
    "ex_ix = 0  # or any valid index into your windowed input dataset\n",
    "input_ids = inputs[ex_ix]                          # e.g., [2, 3, 4, 2]\n",
    "tokens = [idx2word[i] for i in input_ids]          # Human-readable tokens\n",
    "\n",
    "model.eval()\n",
    "x_example = torch.tensor([input_ids], dtype=torch.long)  # [batch, seq]\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits, attn_weights = model(x_example)\n",
    "\n",
    "# 3. Plot attention map before OR after training\n",
    "plot_attention(attn_weights, tokens, title=\"Self-Attention Map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56c789a-3f9d-4c95-856c-1d528c27f8d2",
   "metadata": {},
   "source": [
    "### 2.4 The Training Loop\n",
    "\n",
    "Now that you have your model, data, and attention visualization tools ready, it’s time to train your model!  \n",
    "You’ll use a classic PyTorch training loop that:\n",
    "\n",
    "- Batches your data for efficient processing,\n",
    "- Runs each batch through the model,\n",
    "- Uses the loss function to measure how well your model predicts the next word,\n",
    "- Updates the model parameters using the Adam optimizer,\n",
    "- And tracks your progress using a live progress bar.\n",
    "\n",
    "As the epochs progress, you should see the average loss drop—evidence that your model is learning to predict the next word given context!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958afb43-8c8f-4a1f-86d0-7b50e0530cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loader, loss_fn, optimizer, epochs=20, device='cpu'):\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()                  # Set the model to training mode (enables dropout/training behaviors)\n",
    "        total_loss = 0\n",
    "        with tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\") as pbar:\n",
    "            for xb, yb in pbar:\n",
    "                xb, yb = xb.to(device), yb.to(device)  # Move mini-batch to the right device\n",
    "                optimizer.zero_grad()                  # Clear previous gradients\n",
    "                logits, _ = model(xb)                  # Forward pass for this batch\n",
    "                loss = loss_fn(logits, yb)             # Calculate loss for this batch\n",
    "                loss.backward()                        # Compute gradients\n",
    "                optimizer.step()                       # Update model weights\n",
    "                total_loss += loss.item() * xb.size(0) # Accumulate total loss for averaging\n",
    "                pbar.set_postfix(loss=loss.item())     # Show live loss in the tqdm bar\n",
    "        avg_loss = total_loss / len(loader.dataset)    # Average loss for entire dataset\n",
    "        print(f\"Epoch {epoch+1}: avg loss = {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d096870e-0039-49e6-91b7-4e15a7a92a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)     # Adam is a popular optimizer for NLP\n",
    "loss_fn = nn.CrossEntropyLoss()                         # Classic loss for next-token prediction\n",
    "\n",
    "# Assume 'loader' is your DataLoader for (input_window, target_next_token) pairs,\n",
    "# and 'device' is set to \"cuda\" if available, else \"cpu\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_model(model, loader, loss_fn, optimizer, epochs=25, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edc9ddf-5bff-479b-902a-b0323a654c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Select a sample input from your training data\n",
    "ex_ix = 0  # or any valid index into your windowed input dataset\n",
    "input_ids = inputs[ex_ix]                          # e.g., [2, 3, 4, 2]\n",
    "tokens = [idx2word[i] for i in input_ids]          # Human-readable tokens\n",
    "\n",
    "model.eval()\n",
    "x_example = torch.tensor([input_ids], dtype=torch.long, device = device)  # [batch, seq]\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits, attn_weights = model(x_example)\n",
    "\n",
    "# 3. Plot attention map before OR after training\n",
    "plot_attention(attn_weights, tokens, title=\"Self-Attention Map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4193ca-6c01-4b37-b0aa-86d563eb4107",
   "metadata": {},
   "source": [
    "In the input sequence—\"the dog chased the\"—the model starts by assigning relatively even attention to all tokens, meaning each word's new representation is influenced by almost all other words. This is shown in the first (before training) map, where each row has smoothly distributed blues, and no single cell stands out.\n",
    "\n",
    "However, after training, the map changes dramatically: you see much darker squares in particular positions of each row. For example, the first \"the\" might now strongly attend to \"dog\" while \"dog\" might heavily attend to \"chased\" and so on. What this means is that the model has learned, through exposure to many sentence patterns, that certain words in this context are especially important for predicting or understanding others. \n",
    "\n",
    "In practical terms:  \n",
    "- When the model processes \"the\" it recognizes from training that \"dog\" is the most relevant context in this position—perhaps because \"the dog\" is a frequent phrase structure.\n",
    "- When it processes \"dog\" it attends more strongly to \"chased\" learning that verbs often follow nouns in your data.\n",
    "- The strong attention from the last \"the\" to \"dog\" (or \"chased\" depending on weights) reflects the model's understanding of typical sentence continuations in this specific dataset.\n",
    "\n",
    "So, this focused attention after training visually confirms that the self-attention mechanism has picked up on real relationships in your sentence structure, allowing the model to make smarter predictions by leveraging learned language patterns, not just treating all words equally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ac5ef5-c0a3-4d67-bd82-5553a5204da4",
   "metadata": {},
   "source": [
    "### 2.5 Predicting Multiple Next Words\n",
    "\n",
    "After training, you want to see your model in action—not just on a single token, but generating entire continuations from a given prompt.\n",
    "\n",
    "The function below takes a starting sentence, uses your tokenizer to split it into tokens, and then generates as many new words as you want, one at a time. Each prediction uses the most recent `seq_len` words as context, just like during training. The function maps between words and token IDs using your vocabulary.\n",
    "\n",
    "This allows you to explore how your self-attention model “writes” language, step by step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dd9810-1aca-41c0-a317-11983973da08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_words(model, sentence, tokenizer, word2idx, idx2word, max_tokens=5, seq_len=4, device='cpu'):\n",
    "    \"\"\"\n",
    "    Given an input sentence (string), tokenize it and generate the next `max_tokens` words.\n",
    "    - model: trained self-attention model\n",
    "    - sentence: the starting sequence, as a raw string\n",
    "    - tokenizer: function to split sentence into tokens\n",
    "    - word2idx, idx2word: vocab dicts for conversions\n",
    "    - seq_len: length of context window, should match model/data\n",
    "    - device: 'cpu' or 'cuda'\n",
    "    \"\"\"\n",
    "    model.eval()                         # Set model to evaluation mode (turn off dropout etc.)\n",
    "    generated = tokenizer(sentence)      # Tokenize the input sentence\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        # Prepare the context window (most recent seq_len tokens, pad left if needed)\n",
    "        window = generated[-seq_len:] if len(generated) >= seq_len \\\n",
    "                 else ['<pad>'] * (seq_len - len(generated)) + generated\n",
    "\n",
    "        # Convert tokens to their indices (IDs) for the model\n",
    "        input_ids = torch.tensor([[word2idx.get(w, word2idx['<unk>']) for w in window]], dtype=torch.long).to(device)\n",
    "\n",
    "        # Model predicts logits for the next word\n",
    "        with torch.no_grad():\n",
    "            logits, _ = model(input_ids)\n",
    "            next_id = logits.argmax(dim=-1).item()          # Use the most probable word\n",
    "\n",
    "        next_word = idx2word[next_id]                       # Convert ID back to word\n",
    "        generated.append(next_word)                         # Append prediction to the sequence\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19048a1c-071d-4c14-bf04-a49c70f855cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "sentence = \"the dog chased the\"\n",
    "output = generate_next_words(model, sentence, tokenizer, word2idx, idx2word, max_tokens=1, seq_len=SEQ_LEN, device=device)\n",
    "print(\"Generated sequence:\", \" \".join(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a87c395-59e5-4c8f-a63b-7ab0e3c39c2c",
   "metadata": {},
   "source": [
    "## 3 - To Train, or Not to Train: A Shakespeare Generator Example\n",
    "\n",
    "Now you’ll put all the pieces together for a real application:  \n",
    "a mini “Shakespeare Generator” trained on actual lines from the Bard’s plays.\n",
    "\n",
    "In this section, you’ll apply your attention-based model to generate new Shakespearean text.  \n",
    "You'll use the tools and code you built above to:\n",
    "\n",
    "- Prepare and encode the Shakespeare dataset\n",
    "- Train your self-attention model on it\n",
    "- Generate new lines of “Shakespeare” by predicting multiple words from a chosen prompt\n",
    "\n",
    "Let’s see how your custom transformer becomes a playwright!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926b6520-ee22-4f73-9cdc-5027c089d3b1",
   "metadata": {},
   "source": [
    "### 3.1 Data Preparation\n",
    "\n",
    "To train your model on real Shakespearean language, you first need to get the dataset and inspect its format. In this step, you’ll automatically download a classic, open-source collection of Shakespeare’s works if it’s not already present. You’ll also peek at the start of the file to understand the kind of text you’re working with.\n",
    "\n",
    "This raw text will form the basis for all your training and generation experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18fede0-d15e-4e10-a24c-59ca47ec229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download if needed\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filename = \"shakespeare.txt\"\n",
    "\n",
    "# Check if file already exists\n",
    "if os.path.exists(filename):\n",
    "    print(f\"'{filename}' already exists, skipping download.\")\n",
    "else:\n",
    "    print(f\"Downloading '{filename}'...\")\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "    print(f\"Download complete!\")\n",
    "\n",
    "# Read the text\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:300])  # Just peek at the start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9b96f4-2c9a-4a8e-841c-bad87fe5368d",
   "metadata": {},
   "source": [
    "Next, you’ll need to split Shakespeare’s text into tokens so your model can process it.  \n",
    "This custom tokenizer is designed to:\n",
    "\n",
    "- Replace every line break (`\\n`) with a special `<nl>` token, so your model can learn line boundaries or poetic structure if needed.\n",
    "- Split the text into words (including contractions), standalone punctuation marks, and `<nl>` as individual tokens.\n",
    "\n",
    "This approach gives your model clear, structured input ready for building a vocabulary and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32dc05d-941a-417b-81aa-35692f51c62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareTokenizer:\n",
    "    def __call__(self, text):\n",
    "        # Replace line breaks with a special token\n",
    "        text = text.replace('\\n', ' <nl> ')\n",
    "        # Tokenize words, contractions, <nl>, and punctuation\n",
    "        return re.findall(r\"\\w+(?:'\\w+)?|<nl>|[^\\w\\s]\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee80025-90a1-4089-b385-6d79b9fb175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate your tokenizer\n",
    "tokenizer = ShakespeareTokenizer()\n",
    "\n",
    "# Build vocabulary from all Shakespeare lines using your tokenizer\n",
    "vocab, word2idx, idx2word = build_vocab([text], tokenizer, min_freq=1)\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "print(\"First 20 vocab words:\", vocab[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504d2375-5f8a-4449-8b68-c960c2ec95ba",
   "metadata": {},
   "source": [
    "To prepare your data for training, you need to break up the full Shakespeare text into many (input, target) pairs using a **sliding window** approach:\n",
    "\n",
    "- Each input consists of a sequence of `SEQ_LEN` consecutive tokens.\n",
    "- The target is the immediate next token that follows this window in the text.\n",
    "\n",
    "By sliding this window over the entire dataset, you create thousands of training pairs that help your model learn the flow and structure of real language—even across line breaks and scene changes.\n",
    "\n",
    "Below, you’ll build these pairs and print out an example to see exactly what your model will use as context and what it's being trained to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9a6b8a-6145-47af-8214-1f48867e533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 25\n",
    "tokens = tokenizer(text)    # Tokenize the full text as one sequence!\n",
    "inputs = []\n",
    "targets = []\n",
    "for i in range(len(tokens) - SEQ_LEN):\n",
    "    window = tokens[i:i+SEQ_LEN]\n",
    "    target = tokens[i+SEQ_LEN]\n",
    "    input_ids = [word2idx.get(w, word2idx['<unk>']) for w in window]\n",
    "    target_id = word2idx.get(target, word2idx['<unk>'])\n",
    "    inputs.append(input_ids)\n",
    "    targets.append(target_id)\n",
    "        \n",
    "print(\"Number of (input, target) pairs:\", len(inputs))\n",
    "print(\"Example input:\", [idx2word[i] for i in inputs[0]])\n",
    "print(\"Example target:\", idx2word[targets[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34152a8-60e7-4ae2-969e-4066d617da72",
   "metadata": {},
   "source": [
    "### 3.2 Building The Dataset\n",
    "\n",
    "With your `(input, target)` pairs ready, you now need to convert them into a format that's easy for PyTorch to use during training.\n",
    "\n",
    "By defining a custom `ShakespeareDataset`, you make sure each example is quickly and reliably available by index, allowing PyTorch's `DataLoader` to:\n",
    "- batch sequences together,\n",
    "- shuffle the training data,\n",
    "- and prepare everything for efficient model training.\n",
    "\n",
    "This setup is not only standard for PyTorch workflows, but also makes scaling up to bigger datasets or experiments much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bda63d-c649-400f-81ae-e9b163c79719",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        # Store inputs and targets as PyTorch tensors for fast indexing\n",
    "        self.inputs = torch.tensor(inputs, dtype=torch.long)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)  # Number of samples in the dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return (input_window, target) tuple for the given index\n",
    "        return self.inputs[idx], self.targets[idx]\n",
    "\n",
    "dataset = ShakespeareDataset(inputs, targets)\n",
    "loader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=0)\n",
    "vocab_size = len(vocab)  # Store vocab size for easy reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb1f3c6-ea39-4bad-8d81-1d9be9035288",
   "metadata": {},
   "source": [
    "### 3.3 PyTorch Multihead Attention\n",
    "\n",
    "Up to this point, you've manually implemented every piece of self-attention, giving you a deep, hands-on understanding of how it works. Now, you'll take the next step towards building state-of-the-art models: using PyTorch's built-in `nn.MultiheadAttention` layer.\n",
    "\n",
    "Why use the built-in layer instead of your manual version?\n",
    "- **Efficiency:** PyTorch's implementation is highly optimized and leveraged in real production models.\n",
    "- **Complexity:** The built-in module supports multi-head attention out of the box, allowing your model to learn multiple types of linguistic relationships in parallel—something that gets cumbersome to code by hand.\n",
    "- **Scalability:** You can easily adjust the number of heads, dropout, or embedding size, and take advantage of GPU acceleration and batched computation.\n",
    "- **Compatibility:** This layer is the exact one used in modern transformers, making it easy to transition your knowledge and code to larger or more sophisticated projects.\n",
    "\n",
    "<details>\n",
    "<summary><b>How Attention Works (Formula Explanation)</b></summary>\n",
    "\n",
    "Self-attention for a sequence is computed as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $Q = X W^Q$ are the query vectors,\n",
    "- $K = X W^K$ are the key vectors,\n",
    "- $V = X W^V$ are the value vectors,\n",
    "- $X$ is the input matrix (sequence of embeddings),\n",
    "- $W^Q, W^K, W^V$ are the learned projection matrices,\n",
    "- $d_k$ is the dimension of the key/query,\n",
    "- The output is a weighted sum of the value vectors, with weights determined by the similarity (dot product) between queries and keys.\n",
    "\n",
    "For multi-head attention, you concatenate several such outputs (with parallel projections), and then linearly project again to the original embedding size.\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "Below, you'll see how easy it is to swap out your custom self-attention for PyTorch’s flexible, production-level multi-head attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98056327-4a0d-49cd-bdad-10b8e7cadb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttnWithMHA(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len, embed_dim=768, num_heads=12, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Embedding layer: maps each token ID to a vector of size embed_dim\n",
    "        self.tok_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        # Positional embedding: lets the model know the order of words in the sequence\n",
    "        self.pos_embed = nn.Embedding(seq_len, embed_dim)\n",
    "        # Built-in PyTorch Multi-Head Self-Attention\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dim,      # Size of embeddings/hidden states\n",
    "            num_heads=num_heads,      # How many attention 'heads' to use in parallel\n",
    "            dropout=dropout,          # Dropout on attention weights for regularization\n",
    "            batch_first=True          # Data shape will be (batch, seq, embed_dim)\n",
    "        )\n",
    "        # Linear layer to map attention output to vocab size (scores for next-word prediction)\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        # token_ids: shape [batch_size, seq_len], where each entry is a word index\n",
    "        batch_size, seq_len = token_ids.shape\n",
    "        # Compute position indices for each token in the batch\n",
    "        positions = torch.arange(seq_len, device=token_ids.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        # Get word embeddings and add position embeddings (to encode word order)\n",
    "        input_vecs = self.tok_embed(token_ids) + self.pos_embed(positions)  # Shape: [batch, seq, embed_dim]\n",
    "        # Apply multihead self-attention: computes how each word in the sequence attends to every other word\n",
    "        # Inputs q/k/v are all input_vecs for self-attention\n",
    "        attn_out, attn_w = self.attn(input_vecs, input_vecs, input_vecs, need_weights=True)\n",
    "        # Take the output corresponding to the last token (predict the next word after the sequence)\n",
    "        last_hidden = attn_out[:, -1, :]  # Shape: [batch, embed_dim]\n",
    "        # Final linear layer: outputs scores (logits) for each possible next word in the vocab\n",
    "        logits = self.fc(last_hidden)     # Shape: [batch, vocab_size]\n",
    "        # Return logits and attention weights (for visualization or interpretation)\n",
    "        return logits, attn_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7b69d8-a37c-458c-8b3e-1e38d736ddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SelfAttnWithMHA(vocab_size=vocab_size, seq_len=SEQ_LEN)\n",
    "# Create a new instance of your (multi-head attention) model, sized to your vocab and sequence length\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()  \n",
    "# Define the loss function (CrossEntropy is standard for language modeling);\n",
    "# Redefine here to ensure it's fresh and configured for your current task\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Define the optimizer (Adam adjusts model parameters during training);\n",
    "# Always recreate the optimizer after making a new model, or when model params change\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
    "# Select GPU (\"cuda\") if available, otherwise use CPU\n",
    "\n",
    "model.to(device)\n",
    "# Move your model’s parameters to the chosen device (CPU or GPU)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "# (Optional) Clear unused memory from the GPU to avoid memory leaks or OOM errors;\n",
    "# Especially useful if you ran a different model before in the same session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c7bbd0-c131-481b-9174-f70e547f67ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "sentence = \"to be or not\"\n",
    "output = generate_next_words(model, sentence, tokenizer, word2idx, idx2word, max_tokens=50, seq_len=SEQ_LEN, device=device)\n",
    "print(\"Generated sequence:\", \" \".join(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eeba49-6c61-491f-93a2-282922bdd12a",
   "metadata": {},
   "source": [
    "### 3.4 Training Your Attention Model\n",
    "\n",
    "Now you’re ready to train your multi-head attention language model!  \n",
    "This function wraps the entire training process: it takes batches from your DataLoader, computes the model’s predictions, compares them to the true next words using the loss function, and updates the model’s parameters using the optimizer.  \n",
    "You’ll see a progress bar for each epoch, along with average loss—which should go down as your model learns from the data.\n",
    "\n",
    "Run the cell below to begin training. You can increase the number of epochs to achieve better performance, but keep in mind that this will also increase the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63524ded-fa7a-4aa8-89f3-97d447744004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loader, loss_fn, optimizer, epochs=10, device='cpu', vocab_size=None):\n",
    "    model.to(device)  # Ensure model is on the right device (CPU or GPU)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set to training mode (important for some layers)\n",
    "        total_loss = 0\n",
    "        n_batches = len(loader)\n",
    "        with tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\") as pbar:\n",
    "            for xb, yb in pbar:\n",
    "                xb, yb = xb.to(device), yb.to(device)    # Move batch to correct device\n",
    "                optimizer.zero_grad()                    # Clear previous gradients\n",
    "                logits, _ = model(xb)                    # Forward pass: get model predictions and attention\n",
    "                loss = loss_fn(logits, yb)               # Compute loss\n",
    "                loss.backward()                          # Backpropagation: compute gradients\n",
    "                optimizer.step()                         # Update the model's parameters\n",
    "                total_loss += loss.item() * xb.size(0)   # Accumulate total loss for reporting\n",
    "                pbar.set_postfix(loss=loss.item())       # Show current batch loss in progress bar\n",
    "\n",
    "        avg_loss = total_loss / len(loader.dataset)\n",
    "        print(f\"Epoch {epoch+1:2d}: avg loss = {avg_loss:.4f}\")\n",
    "\n",
    "# Usage:\n",
    "EPOCHS = 5\n",
    "vocab_size = len(vocab)  # Pass this in!\n",
    "train_model(model, loader, loss_fn, optimizer, epochs=EPOCHS, device=device, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412437e5-e737-4f07-b5a7-cf6b19e59697",
   "metadata": {},
   "source": [
    "When you trained your model, you used a special token `<nl>` to represent newlines (line breaks) in the Shakespeare dataset.  \n",
    "This allowed your tokenizer and model to recognize and generate line boundaries just like any other word.\n",
    "\n",
    "However, when you actually want to read or display the generated text, you want to see real line breaks—not the literal string `<nl>`.  \n",
    "That's why, in the generation function, you replace every occurrence of `<nl>` with the actual newline character `\\n`.  \n",
    "This makes your generated samples readable and visually closer to real Shakespearean poetry or drama!\n",
    "\n",
    "In other words:  \n",
    "- **Inside the model:** `<nl>` acts as a \"word\" that teaches your network where lines start and end.  \n",
    "- **Outside the model:** you convert `<nl>` back to a true line break so your output looks natural.\n",
    "\n",
    "This small transformation turns a list like  \n",
    "`[\"to\", \"be\", \"<nl>\", \"or\", \"not\", \"<nl>\", \"to\", \"be\"]`  \n",
    "into something that prints as  \n",
    "```\n",
    "to be\n",
    "or not\n",
    "to be\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85d7956-f774-432a-9eb6-1b6959a937ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A small helper class to generate next words from a model with temperature sampling.\n",
    "\n",
    "\n",
    "class NextWordGenerator:\n",
    "    \"\"\"\n",
    "    Generate next tokens from a language model using a fixed context window and temperature sampling.\n",
    "\n",
    "    - Uses left-padding with <pad> to fill the initial context window.\n",
    "    - Applies temperature scaling to logits, then samples with multinomial.\n",
    "    - Converts special <nl> token to '\\n' in the final output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        tokenizer: Callable[[str], Sequence[str]],\n",
    "        word2idx: Dict[str, int],\n",
    "        idx2word: Dict[int, str],\n",
    "        *,\n",
    "        seq_len: int = 6,\n",
    "        device: str = \"cpu\",\n",
    "        pad_token: str = \"<pad>\",\n",
    "        unk_token: str = \"<unk>\",\n",
    "        nl_token: str = \"<nl>\",\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "        self.seq_len = seq_len\n",
    "        self.device = device\n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "        self.nl_token = nl_token\n",
    "\n",
    "        # Cache common ids\n",
    "        self._unk_id = self.word2idx[self.unk_token]\n",
    "\n",
    "    def _make_window(self, generated: List[str]) -> List[str]:\n",
    "        if len(generated) >= self.seq_len:\n",
    "            return generated[-self.seq_len:]\n",
    "        pad_count = self.seq_len - len(generated)\n",
    "        return [self.pad_token] * pad_count + generated\n",
    "\n",
    "    def _encode(self, tokens: Sequence[str]) -> torch.Tensor:\n",
    "        ids = [self.word2idx.get(t, self._unk_id) for t in tokens]\n",
    "        return torch.tensor([ids], dtype=torch.long, device=self.device)\n",
    "\n",
    "    def _decode_id(self, idx: int) -> str:\n",
    "        return self.idx2word[idx]\n",
    "\n",
    "    def _postprocess(self, tokens: List[str]) -> List[str]:\n",
    "        # Replace <nl> with actual newlines for display/printing\n",
    "        return [(\"\\n\" if t == self.nl_token else t) for t in tokens]\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        sentence: str,\n",
    "        *,\n",
    "        max_tokens: int = 20,\n",
    "        temperature: float = 1.0,\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate up to `max_tokens` tokens continuing from `sentence`.\n",
    "        Returns the full sequence including the original tokens, with <nl> converted to '\\n'.\n",
    "        \"\"\"\n",
    "        if temperature <= 0:\n",
    "            raise ValueError(\"temperature must be > 0\")\n",
    "\n",
    "        self.model.eval()\n",
    "        generated = list(self.tokenizer(sentence))  # mutate in place; no need to copy\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_tokens):\n",
    "                window = self._make_window(generated)\n",
    "                input_ids = self._encode(window)\n",
    "\n",
    "                logits, _ = self.model(input_ids)  # expects shape [1, vocab]\n",
    "                logits = logits / temperature\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "                # Sample the next token id; returns shape [1, 1]\n",
    "                next_id = torch.multinomial(probs, num_samples=1).item()\n",
    "                next_token = self._decode_id(next_id)\n",
    "                generated.append(next_token)\n",
    "\n",
    "        return self._postprocess(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67644a5-7050-4673-b3c9-188743ea4857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "generator = NextWordGenerator(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    word2idx=word2idx,\n",
    "    idx2word=idx2word,\n",
    "    seq_len=SEQ_LEN,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "output = generator.generate(\"To be or not to\", max_tokens=50, temperature=1.0)\n",
    "print(\"Generated sequence:\", \" \".join(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1abe13d-c31b-48c0-bf8b-265e0352f0ed",
   "metadata": {},
   "source": [
    "## 4 - Conclusion\n",
    "\n",
    "In this lab, you’ve worked through every step of building a modern attention-based language model from scratch:\n",
    "- You started by preparing your text data, creating a vocabulary, and organizing your dataset using sliding windows.\n",
    "- You implemented self-attention both manually and using PyTorch’s powerful `nn.MultiheadAttention` layer, learning exactly how these models “pay attention” to context.\n",
    "- You trained your model on real Shakespearean language, and were able to generate new text—sometimes even capturing the style and rhythm of the Bard himself!\n",
    "\n",
    "By working hands-on with both custom and built-in attention, you gained a deep understanding of what makes transformers so effective in modern NLP:\n",
    "- The ability to learn flexible, context-dependent relationships within text;\n",
    "- The scalability and practicality of using optimized library components;\n",
    "- The importance of good data preparation and robust, interpretable generation.\n",
    "\n",
    "Keep exploring! Try different seeds, experiment with model hyperparameters, or dive deeper into visualizing attention maps.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8638a9-797d-4426-9929-13631afc7ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
