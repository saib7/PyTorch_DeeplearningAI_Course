{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Programming Assignment: CleanVision AI - Optimizing Models for Metro City's Smart Fleet\n",
        "\n",
        "## Your Mission: Smart City Cleanup Initiative üèôÔ∏è\n",
        "\n",
        "**Welcome to CleanVision AI!**\n",
        "\n",
        "Congratulations on joining CleanVision AI as a Machine Learning Engineer! Our startup has just landed a major contract with the Metro City Council to revolutionize urban waste management. Here's the situation:\n",
        "\n",
        "**The Challenge:**  \n",
        "Metro City wants to deploy AI-powered cameras on their existing fleet of 500 street cleaning vehicles to automatically identify areas that need attention. The city has provided us with a pre-trained ResNet-18 model that accurately classifies street scenes into three categories: *clean*, *litter*, and *recycle*. It works perfectly in the cloud‚Äîbut there's a catch.\n",
        "\n",
        "**The Problem:**  \n",
        "The street cleaning vehicles are equipped with **edge devices** (similar to Raspberry Pi) with:\n",
        "- ‚ùå No GPU\n",
        "- ‚ùå Limited RAM (4GB)\n",
        "- ‚ùå Constrained storage (16GB)\n",
        "- ‚ùå CPU-only inference capabilities\n",
        "\n",
        "The city's budget won't allow for hardware upgrades, and they need real-time inference (< 50ms per image) to make instant routing decisions. Our current model is **512 MB** and takes over **900ms per batch** on CPU‚Äîcompletely unusable for their needs.\n",
        "\n",
        "**Your Task:**  \n",
        "The city is expecting a demo in two weeks. Your manager has assigned you to optimize the model for deployment on these edge devices. The model must:\n",
        "1. ‚úÖ Run efficiently on CPU-only hardware\n",
        "2. ‚úÖ Fit within the storage constraints (target: < 150 MB)\n",
        "3. ‚úÖ Maintain accuracy above 95% (city requirement)\n",
        "4. ‚úÖ Achieve inference time under 50ms per image on CPU\n",
        "\n",
        "**The Toolkit:**  \n",
        "Your team lead suggests a three-stage optimization pipeline:\n",
        "- **Stage 1: Pruning** - Remove redundant weights to reduce model size\n",
        "- **Stage 2: Dynamic Quantization** - Convert heavy layers to INT8 for faster CPU inference  \n",
        "- **Stage 3: Quantization-Aware Training (QAT)** - Fine-tune with quantization simulation for maximum accuracy retention\n",
        "\n",
        "**Success Criteria:**  \n",
        "If you succeed, Metro City will expand the contract to 50 other cities nationwide, making CleanVision AI a leader in urban tech solutions. The CFO is counting on you‚Äîlet's make this model deployment-ready!\n",
        "\n",
        "---\n",
        "\n",
        "## Introduction\n",
        "\n",
        "You have already trained many models; now it is time to shape one for life outside a notebook. This lab guides you through taking a pre-trained ResNet-18 based StreetClassifier and turning it into a version that is easier to store, faster on CPU, and ready for deployment on edge targets. Along the way you will practice saving and reloading model state, pruning parameters, applying quantization, and evaluating the trade offs among accuracy, latency, and size.\n",
        "\n",
        "In this lab you will:\n",
        "\n",
        "- Load the CleanStreetDataset, initialize a pre-trained StreetClassifier, and establish a baseline evaluation.\n",
        "\n",
        "- Work with checkpoints by saving and restoring state_dict objects for both training and inference.\n",
        "\n",
        "- Implement magnitude-based pruning across convolutional and linear layers, with options for unstructured and structured strategies, and verify sparsity and accuracy.\n",
        "\n",
        "- Apply dynamic quantization to linear layers for a fast CPU speedup and benchmark the effect.\n",
        "\n",
        "- Fuse common layer patterns and prepare a quantization-aware variant, fine-tune briefly, then convert to an INT8 model.\n",
        "\n",
        "- Compare accuracy, inference time, and file size before and after compression to understand the impact of each step.\n",
        "\n",
        "By the end, you will have a compact classifier that keeps performance close to the original while being far more efficient to run and ship‚Äîready for deployment on Metro City's fleet!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Table of Contents\n",
        "- [ 1 - Setup and Imports](#1)\n",
        "- [ 2 - Baseline Model and Dataset](#2)\n",
        "  - [ 2.1 Dataset](#2-1)\n",
        "  - [ 2.2 Model Definition](#2-2)\n",
        "  - [ 2.3 Loading Pretrained Weights](#2-3)\n",
        "- [ 3 - Model Pruning](#3)\n",
        "  - [ Exercise 1](#ex01)\n",
        "- [ 4 - Dynamic Quantization](#4)\n",
        "  - [ 4.1 Benefits of Quantization](#4-1)\n",
        "  - [ 4.2 Three Approaches to Quantization](#4-2)\n",
        "  - [ 4.3 Dynamic Quantization (This Section)](#4-3)\n",
        "    - [ Exercise 2](#ex02)\n",
        "- [ 5 - Quantization-Aware Training (QAT)](#5)\n",
        "  - [ 5.1 What is QAT and why use it?](#5-1)\n",
        "    - [ Exercise 3](#ex03)\n",
        "  - [ 5.2 Auxiliary Functions for QAT](#5-2)\n",
        "    - [ Exercise 4](#ex04)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<a name='submission'></a>\n",
        "\n",
        "<h4 style=\"color:green; font-weight:bold;\">TIPS FOR SUCCESSFUL GRADING OF YOUR ASSIGNMENT:</h4>\n",
        "\n",
        "* All cells are frozen except for the ones where you need to submit your solutions or when explicitly mentioned you can interact with it.\n",
        "\n",
        "* In each exercise cell, look for comments `### START CODE HERE ###` and `### END CODE HERE ###`. These show you where to write the solution code. **Do not add or change any code that is outside these comments**.\n",
        "\n",
        "* You can add new cells to experiment but these will be omitted by the grader, so don't rely on newly created cells to host your solution code, use the provided places for this.\n",
        "\n",
        "* Avoid using global variables unless you absolutely have to. The grader tests your code in an isolated environment without running all cells from the top. As a result, global variables may be unavailable when scoring your submission. Global variables that are meant to be used will be defined in UPPERCASE.\n",
        "\n",
        "* To submit your notebook for grading, first save it by clicking the üíæ icon on the top left of the page and then click on the `Submit assignment` button on the top right of the page.\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='1'></a>\n",
        "## 1 - Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "from torch.nn.utils import prune\n",
        "import torch.nn as nn\n",
        "import torch.ao.quantization as aoq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import helper_utils\n",
        "import unittests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='2'></a>\n",
        "## 2 - Baseline Model and Dataset\n",
        "\n",
        "**StreetClassifier** is a deep learning model designed to classify urban scene images into three categories:  \n",
        "- **clean**  \n",
        "- **litter**  \n",
        "- **recycle** \n",
        "\n",
        "<a id='2-1'></a>\n",
        "### 2.1 Dataset\n",
        "\n",
        "For this task, we will use the **CleanStreetDataset**, which is already divided into **training**, **development**, and **test** splits.  \n",
        "In the code below, you'll see how the datasets are loaded and how data preprocessing and augmentation transforms are applied to prepare the data for training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "dataset_path = \"./data/\"\n",
        "\n",
        "# Define transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = datasets.ImageFolder(root=os.path.join(dataset_path, 'train'), transform=train_transform)\n",
        "dev_dataset = datasets.ImageFolder(root=os.path.join(dataset_path, 'dev'), transform=eval_transform)\n",
        "test_dataset = datasets.ImageFolder(root=os.path.join(dataset_path, 'test'), transform=eval_transform)\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "print(\"Number of training samples:\", len(train_dataset))\n",
        "print(\"Number of validation samples:\", len(dev_dataset))\n",
        "print(\"Number of test samples:\", len(test_dataset))\n",
        "print(\"\\nClass mapping:\", train_dataset.class_to_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can visualize some examples with the following helper function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "helper_utils.display_some_images(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='2-2'></a>\n",
        "### 2.2 Model Definition\n",
        "\n",
        "The **StreetClassifier** model builds on **ResNet-18**, a well-known convolutional neural network pretrained on ImageNet.  \n",
        "In this implementation the final classifier layer was replaced so the model can predict the three target classes of our dataset (*clean*, *litter*, and *recycle*). \n",
        "\n",
        "To make your work easier the ResNet-18 has been modified so it can use QAT in latter exercises.\n",
        "You can see its architecture and details with the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "# Use the base model\n",
        "model = helper_utils.resnet18_qat_ready_pretrained(num_classes=3, use_quant_stubs=False).to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='2-3'></a>\n",
        "### 2.3 Loading Pretrained Weights\n",
        "\n",
        "For the exercises, we will work with a **pretrained StreetClassifier model**. The model weights are stored in the file **`street_classifier_weights.pt`**.  \n",
        "\n",
        "We will load the weights and then use the `compute_accuracy` function to evaluate the model's performance on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the final model checkpoint\n",
        "model_weights = torch.load(\"street_classifier_weights.pt\", map_location=\"cpu\")\n",
        "model.load_state_dict(model_weights)\n",
        "\n",
        "# Compute accuracy of the loaded model\n",
        "base_accuracy = helper_utils.compute_accuracy(model, test_loader, device)\n",
        "print(f\"Model accuracy: {base_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='3'></a>\n",
        "## 3 - Model Pruning\n",
        "\n",
        "In this section, we will explore **model pruning**, a technique to reduce the size and complexity of neural networks by removing less important weights.  \n",
        "To simplify the process, we provide a couple of helper functions:  \n",
        "\n",
        "- `_iter_prunable_modules(model)` ‚Äì iterates over all `Conv2d` and `Linear` layers, which are the layers we will target for pruning.  \n",
        "- `finalize_pruning(model)` ‚Äì makes pruning permanent by removing reparametrization and storing the pruned weights directly.  \n",
        "\n",
        "These utilities will make it easier to apply and finalize pruning across the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "def _iter_prunable_modules(model):\n",
        "    \"\"\"\n",
        "    Iterate over modules that are eligible for pruning.\n",
        "\n",
        "    Yields\n",
        "    ------\n",
        "    Tuple[str, nn.Module]\n",
        "        Pairs of (fully-qualified module name, module) for layers that are\n",
        "        prunable in this assignment: `nn.Conv2d` and `nn.Linear`.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    - The qualified name comes from `model.named_modules()` and reflects the\n",
        "      path within the module hierarchy (e.g., \"block.0\", \"classifier.fc\").\n",
        "    - Use this generator to systematically apply pruning across the model.\n",
        "    \"\"\"\n",
        "    for name, m in model.named_modules():\n",
        "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "            yield name, m\n",
        "\n",
        "def finalize_pruning(model):\n",
        "    \"\"\"\n",
        "    Make pruning permanent by removing reparametrization wrappers.\n",
        "\n",
        "    This converts any pruned parameter from the (`weight_orig`, `weight_mask`)\n",
        "    reparametrization back to a regular `weight` `nn.Parameter` where the\n",
        "    zeros are **materialized** in the stored tensor.\n",
        "    \"\"\"\n",
        "    for _, module in _iter_prunable_modules(model):\n",
        "        # Only remove if the parameter has been pruned\n",
        "        if hasattr(module, \"weight_orig\") and hasattr(module, \"weight_mask\"):\n",
        "            prune.remove(module, \"weight\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='ex01'></a>\n",
        "### Exercise 1\n",
        "\n",
        "Now you will implement the function `prune_model`, which applies pruning to all `Conv2d` and `Linear` layers in the **StreetClassifier** model.  \n",
        "You can choose between two pruning modes:  \n",
        "\n",
        "- **`l1_unstructured`** ‚Äì removes a fraction of the smallest-magnitude weights.  \n",
        "- **`ln_structured`** ‚Äì removes entire output channels (structured pruning), which can lead to faster inference on some hardware.  \n",
        "\n",
        "#### Why Prune a Model?\n",
        "- **Smaller models** ‚Üí reduced memory footprint and easier deployment on resource-constrained devices.  \n",
        "- **Faster inference** ‚Üí especially with structured pruning, which eliminates entire channels.  \n",
        "- **Regularization effect** ‚Üí pruning can sometimes improve generalization by removing redundant connections.  \n",
        "\n",
        "#### Details: `prune_model`\n",
        "\n",
        "Implement ```prune_model(model, amount=0.3, mode=\"l1_unstructured\")```, a function that:\n",
        "\n",
        "Applies magnitude-based pruning to the weights of every nn.Conv2d and nn.Linear layer in a given model using PyTorch's pruning reparametrization API. The pruning is applied in-place (adds weight_orig and weight_mask) and does not change any tensor shapes. To permanently bake zeros into the stored weights (remove reparametrization), a separate helper finalize_pruning(model) is provided for you to call afterward.\n",
        "\n",
        "The function proceeds through these stages:\n",
        "\n",
        "**Validate Inputs**\n",
        "\n",
        "1. Ensure amount is a float in [0, 1].\n",
        "\n",
        "2. Ensure mode is one of {\"l1_unstructured\", \"ln_structured\"}.\n",
        "\n",
        "**Find Prunable Modules**\n",
        "- Iterate over the model and select only ```nn.Conv2d``` and ```nn.Linear``` layers (a helper _iter_prunable_modules(model) is available).\n",
        "- Skip any module that does not have a weight attribute.\n",
        "\n",
        "**Apply Pruning (In-Place Reparametrization)**\n",
        "- For unstructured pruning (default): Use ```prune.l1_unstructured(module, name=\"weight\", amount=amount)``` to zero the smallest-magnitude individual weights within each tensor.\n",
        "- For structured pruning: Use ```prune.ln_structured(module, name=\"weight\", amount=amount, n=2, dim=0)``` to zero out entire output channels (L2-norm across channel filters), leaving tensor shapes unchanged but producing channel-wise sparsity masks.\n",
        "\n",
        "**Return the Same Model Instance**  \n",
        "Return the input model with pruning reparametrization attached (i.e., weight becomes a computed tensor from weight_orig * weight_mask).\n",
        "\n",
        "**Note:** Parameters and buffers now include weight_orig and weight_mask for pruned layers.\n",
        "\n",
        "Downstream code can call ```finalize_pruning(model)``` to remove the reparametrization objects and write the masked values into the raw weight tensors.\n",
        "\n",
        "Even after finalization, shapes remain the same; channels are zeroed, not physically removed.\n",
        "\n",
        "The output of this function‚Äîa model with pruning masks attached‚Äîwill later be used by grading code that (a) checks the presence of reparametrized weights, (b) inspects sparsity levels, and (c) optionally calls finalize_pruning(model) before export or quantization.\n",
        "\n",
        "<details> <summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
        "\n",
        "**1) Iterate only over prunable layers**\n",
        "\n",
        "You'll be given a helper like:\n",
        "\n",
        "```python\n",
        "for _, module in _iter_prunable_modules(model):\n",
        "```\n",
        "\n",
        "This should yield nn.Conv2d and nn.Linear modules. Still, defensively skip modules that don't have module.weight.\n",
        "\n",
        "**2) Two pruning modes you must support**\n",
        "\n",
        "Unstructured (default):\n",
        "```python\n",
        "prune.l1_unstructured(module, name=\"weight\", amount=amount)\n",
        "```\n",
        "\n",
        "Zeros individual weights with the smallest L1 magnitudes.\n",
        "\n",
        "Structured:\n",
        "```python\n",
        "prune.ln_structured(module, name=\"weight\", amount=amount, n=2, dim=0)\n",
        "```\n",
        "\n",
        "Zeros entire output channels based on L2 norms. Shapes do not change here; channels are masked.\n",
        "\n",
        "**3) Validate arguments early**\n",
        "\n",
        "Raise:\n",
        "```python\n",
        "if not (0.0 <= amount <= 1.0):\n",
        "    raise ValueError(f\"amount must be in [0,1], got {amount}\")\n",
        "```\n",
        "\n",
        "and for unsupported modes:\n",
        "```python\n",
        "raise ValueError(\"mode must be 'l1_unstructured' or 'ln_structured'\")\n",
        "```\n",
        "\n",
        "**4) Remember: pruning is in-place & reparametrized**\n",
        "\n",
        "After pruning a layer, you'll see attributes:\n",
        "\n",
        "- weight_orig (the original parameter)\n",
        "- weight_mask (a buffer of 0/1s)\n",
        "- weight becomes a computed tensor (not a leaf parameter).\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: prune_model\n",
        "\n",
        "def prune_model(model, amount=0.3, mode=\"l1_unstructured\"):\n",
        "    \"\"\"\n",
        "    Apply pruning to **weights** of all `Conv2d` and `Linear` layers.\n",
        "\n",
        "    This uses PyTorch's pruning reparametrization (adds `weight_orig` and\n",
        "    `weight_mask`) without changing the tensor shape. To permanently embed\n",
        "    zeros into the stored weights, call `finalize_pruning(model)` afterward.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        Model to prune. Pruning is applied **in-place** via\n",
        "        `torch.nn.utils.prune`.\n",
        "    amount : float, optional (default=0.3)\n",
        "        Fraction in [0, 1] to prune.\n",
        "        - For **unstructured** pruning: fraction of smallest-magnitude weights\n",
        "          within each tensor.\n",
        "        - For **structured (ln)** pruning: fraction of **output channels**\n",
        "          (dimension 0) to remove using L2-norm (n=2).\n",
        "    mode : {\"l1_unstructured\", \"ln_structured\"}, optional\n",
        "        Pruning strategy:\n",
        "        - `\"l1_unstructured\"` ‚Üí `prune.l1_unstructured(..., name=\"weight\", amount=amount)`\n",
        "        - `\"ln_structured\"`   ‚Üí `prune.ln_structured(..., name=\"weight\", amount=amount, n=2, dim=0)`\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    nn.Module\n",
        "        The same model instance with pruning **reparametrization** applied\n",
        "        (not yet made permanent).\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    if not (0.0 <= amount <= 1.0): # Check if amount is in [0,1]\n",
        "        raise ValueError(f\"amount must be in [0,1], got {amount}\") \n",
        "\n",
        "    for _, module in _iter_prunable_modules(model):\n",
        "        if hasattr(module, \"weight_orig\") and hasattr(module, \"weight_mask\"): # Check if module has \"weight\" attribute\n",
        "            continue \n",
        "\n",
        "        if mode == \"l1_unstructured\":\n",
        "            prune.l1_unstructured(module, name=\"weight\", amount=amount) # l1_unstructured from prune with module, name(\"weight\"), and amount\n",
        "        elif mode == \"ln_structured\": # Check elif mode is \"ln_structured\"\n",
        "            prune.ln_structured(module, name=\"weight\", amount=amount, n=2, dim=0) # ln_structured from prune with module, name(\"weight\"), amount, n(2), and dim(0)  \n",
        "        else: \n",
        "            raise ValueError(\"mode must be 'l1_unstructured' or 'ln_structured'\")\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return model\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To verify your code, run the following cells!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "# Verify your code here\n",
        "\n",
        "# Create baseline model statistics\n",
        "base = helper_utils.sparsity_report(model)\n",
        "print(\"[BASE] global_sparsity:\", base[\"global_sparsity\"])\n",
        "base_time = helper_utils.bench(model, device=device)\n",
        "print(\"[BASE] time:\", base_time)\n",
        "\n",
        "# We prune 50% of the model\n",
        "prune_model(model, amount=0.5, mode=\"l1_unstructured\")\n",
        "\n",
        "after = helper_utils.sparsity_report(model)\n",
        "print(\"[AFTER PRUNE] global_sparsity:\", after[\"global_sparsity\"])\n",
        "\n",
        "after_acc = helper_utils.compute_accuracy(model, test_loader, device=device)\n",
        "print(\"[AFTER PRUNE] accuracy:\", after_acc)\n",
        "\n",
        "pruned_time = helper_utils.bench(model, device=device)\n",
        "\n",
        "print(f\"\\nInference time comparison:\")\n",
        "print(f\"Base model: {base_time:.4f} seconds per batch\")\n",
        "print(f\"Pruned model: {pruned_time:.4f} seconds per batch\") \n",
        "print(f\"Speedup: {base_time / pruned_time:.2f}x\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Expected Output (results may vary, but not much):\n",
        "```\n",
        "[BASE] global_sparsity: 0.0\n",
        "[BASE] time: 0.004570210154633969\n",
        "[AFTER PRUNE] global_sparsity: 0.5\n",
        "[AFTER PRUNE] accuracy: 0.9914529914529915\n",
        "\n",
        "Inference time comparison:\n",
        "Base model: 0.0046 seconds per batch\n",
        "Pruned model: 0.0035 seconds per batch\n",
        "Speedup: 1.29x\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "# Test 1: Prune Model\n",
        "\n",
        "unittests.exercise1(prune_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### üìä Progress Update: Stage 1 Complete!\n",
        "\n",
        "**From your team lead:**  \n",
        "*\"Great work on implementing pruning! I just ran your code on our test edge device. We're seeing ~50% sparsity with minimal accuracy loss. The model is more compact now, but we still need to hit that < 50ms inference target. Let's move to Stage 2: Dynamic Quantization. This will convert the heavy FP32 operations to INT8, which should give us the CPU speedup we need.\"*\n",
        "\n",
        "**Current Status:**\n",
        "- ‚úÖ **Stage 1 (Pruning):** Complete - 50% of weights pruned  \n",
        "- üîÑ **Stage 2 (Quantization):** Next up  \n",
        "- ‚è≥ **Stage 3 (QAT):** Coming soon\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='4'></a>\n",
        "## 4 - Dynamic Quantization\n",
        "\n",
        "Quantization is a technique that reduces the **precision** of model weights and activations, typically converting them from 32-bit floating point (FP32) to lower-precision formats like 8-bit integers (INT8).  \n",
        "This can drastically reduce the size of the model and improve inference speed, especially on CPUs and edge devices, while keeping accuracy close to that of the original FP32 model.  \n",
        "\n",
        "---\n",
        "\n",
        "<a id='4-1'></a>\n",
        "### 4.1 Benefits of Quantization\n",
        "\n",
        "- **Smaller model size** ‚Üí INT8 weights require 4√ó less storage than FP32.  \n",
        "- **Faster inference** ‚Üí Integer operations are usually faster than floating-point on CPUs.  \n",
        "- **Lower memory bandwidth** ‚Üí Reduced precision means less data transfer, improving latency.  \n",
        "- **Deployment-friendly** ‚Üí Ideal for running models on devices with limited resources (mobile, IoT, embedded).  \n",
        "\n",
        "---\n",
        "\n",
        "<a id='4-2'></a>\n",
        "### 4.2 Three Approaches to Quantization\n",
        "\n",
        "There are different approaches to quantization, each with its own trade-offs:  \n",
        "\n",
        "1. **Dynamic Quantization** *(covered in this exercise)*: The simplest approach ‚Äî it keeps weights in INT8 but performs activations in FP32, dynamically quantizing them at runtime. **No training or calibration needed.**  \n",
        "\n",
        "2. **Static Quantization** *(not covered)*: Both weights and activations are pre-quantized using calibration data. Requires a calibration step but no retraining. Often provides better speedup than dynamic quantization.\n",
        "\n",
        "3. **Quantization-Aware Training (QAT)** *(covered in Exercise 4)*: Simulates quantization effects **during training** using \"fake quantization\" so the model learns INT8-friendly weights. Provides the best accuracy retention but requires retraining/fine-tuning.  \n",
        "\n",
        "**Key Distinction:**  \n",
        "- **Dynamic Quantization** and **Static Quantization** are **post-training** techniques (apply to already-trained models).  \n",
        "- **QAT** is a **training-time** technique (requires fine-tuning the model with quantization simulation).\n",
        "\n",
        "---\n",
        "\n",
        "<a id='4-3'></a>\n",
        "### 4.3 Dynamic Quantization (This Section)\n",
        "\n",
        "**Dynamic quantization is particularly effective for models dominated by `nn.Linear` layers (e.g., Transformers, LSTMs, fully-connected classifiers).**  \n",
        "It requires **no retraining or calibration** and is CPU-only, making it the fastest way to get the benefits of quantization.  \n",
        "\n",
        "---\n",
        "\n",
        "<a id='ex02'></a>\n",
        "### Exercise 2\n",
        "\n",
        "In this exercise, you will complete the function `quantize_dynamic_linear`, which should:  \n",
        "\n",
        "- Make a **deep copy** of the original model (do not modify the original).  \n",
        "- Apply **dynamic quantization** to **only** the `nn.Linear` layers, converting them to INT8.  \n",
        "- Return the quantized model in `eval()` mode.  \n",
        "- Ensure it works in CPU-only environments.  \n",
        "\n",
        "This will give you hands-on practice with PyTorch's `torch.quantization.quantize_dynamic` utility and help you understand how quantization can be applied selectively to certain model components.\n",
        "\n",
        "#### Details: `quantize_dynamic_linear`\n",
        "\n",
        "Implement `quantize_dynamic_linear`, a function that: returns a new model in eval() mode where all nn.Linear layers are dynamically quantized to INT8. Dynamic quantization stores weights as INT8 and quantizes activations on-the-fly at runtime, giving CPU speed/memory wins without calibration.\n",
        "\n",
        "**What your function must do:**\n",
        "\n",
        "1. Clone the model (don't mutate the original) and switch to eval mode\n",
        "    - copy.deepcopy(model) so the input model is untouched.\n",
        "    - Call .eval() on the copy.\n",
        "\n",
        "2. (CPU) Select a sensible quantization engine if available\n",
        "    - On x86 CPUs, fbgemm is common. Set it if present:\n",
        "    - torch.backends.quantized.engine = \"fbgemm\" inside a try/except.\n",
        "\n",
        "3. Apply dynamic INT8 quantization to nn.Linear only\n",
        "    - Use torch.quantization.quantize_dynamic(...) on the FP32 copy.\n",
        "    - Pass the module set {nn.Linear} so only linear layers are quantized.\n",
        "    - Use dtype=torch.qint8.\n",
        "\n",
        "4. Return the quantized model in eval() mode\n",
        "\n",
        "Ensure you return .eval() (no training-time behavior).\n",
        "\n",
        "The function should work CPU-only (no CUDA, no calibration steps).\n",
        "\n",
        "<details> <summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
        "\n",
        "1. Work on a copy, in eval mode\n",
        "    ```python\n",
        "    model_fp32 = copy.deepcopy(model).eval()\n",
        "    ```\n",
        "2. Pick a CPU quantization engine if available (harmless if not)\n",
        "    ```python\n",
        "    if hasattr(torch.backends, \"quantized\") and hasattr(torch.backends.quantized, \"engine\"):\n",
        "        try:\n",
        "            torch.backends.quantized.engine = \"fbgemm\"\n",
        "        except Exception:\n",
        "            pass\n",
        "    ```\n",
        "\n",
        "3. Quantize ONLY Linear layers to INT8 dynamically\n",
        "    ```python\n",
        "    qmodel = torch.quantization.quantize_dynamic(\n",
        "        model_fp32,\n",
        "        {nn.Linear},          # target modules\n",
        "        dtype=torch.qint8,    # INT8 weights\n",
        "    ).eval()                   # ensure eval mode\n",
        "    ```\n",
        "\n",
        "4. Return the quantized model\n",
        "    ```python\n",
        "    return qmodel\n",
        "    ```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: quantize_dynamic_linear\n",
        "\n",
        "def quantize_dynamic_linear(model):\n",
        "    \"\"\"\n",
        "    Return a **new** model where all nn.Linear layers are dynamically quantized to INT8.\n",
        "\n",
        "    Requirements checked by the autograder\n",
        "    --------------------------------------\n",
        "    - Do NOT mutate the original model; use a deepcopy.\n",
        "    - Quantize ONLY Linear modules (e.g., {nn.Linear}).\n",
        "    - Use dynamic quantization with INT8 dtype.\n",
        "    - Return the quantized model in eval() mode.\n",
        "    - Should run on CPU-only environments (no CUDA, no calibration).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    nn.Module\n",
        "        An eval-mode copy of `model` with Linear layers using INT8 dynamic quantization.\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ###\n",
        "    model_fp32 = copy.deepcopy(model) # Deepcopy the input model to model_fp32\n",
        "\n",
        "    # Ensure a sensible engine on CPU (x86). If unavailable, this line is harmless.\n",
        "    has_quantized = torch.backends.quantized is not None # Check if torch.backends has quantized\n",
        "    has_engine = torch.backends.quantized.engine is not None # Check if torch.backends.quantized has engine\n",
        "    if has_quantized and has_engine: # @KEEP\n",
        "        try:\n",
        "            torch.backends.quantized.engine = \"fbgemm\" \n",
        "        except Exception: \n",
        "            pass  # keep whatever the runtime supports \n",
        "    # Quantize only Linear layers to INT8\n",
        "    quantized = aoq.quantize_dynamic(\n",
        "        model_fp32, # The model to quantize\n",
        "        {nn.Linear}, # The layers to quantize (only Linear layers)\n",
        "        dtype=torch.qint8 # The dtype to quantize to qint8\n",
        "    )\n",
        "    \n",
        "    quantized.eval() # Set the quantized model to eval mode\n",
        "\n",
        "    return quantized # Return the quantized model\n",
        "\n",
        "    ### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "# Verify your code here\n",
        "\n",
        "# Use the base model to start fresh\n",
        "model = helper_utils.resnet18_qat_ready_pretrained(num_classes=3, use_quant_stubs=True).to(device)\n",
        "model_weights = torch.load(\"street_classifier_weights.pt\", map_location=\"cpu\")\n",
        "model.load_state_dict(model_weights)\n",
        "model.to(\"cpu\")\n",
        "model.eval()\n",
        "\n",
        "# Set seed for reproducibility\n",
        "torch.manual_seed(5)\n",
        "\n",
        "# Quantize the model\n",
        "qmodel = quantize_dynamic_linear(model)\n",
        "\n",
        "# Evaluate the quantized model\n",
        "qacc = helper_utils.compute_accuracy(qmodel, test_loader, device=\"cpu\")\n",
        "print(f\"\\nAccuracy on test dataset after quantization: {qacc:.2f}%\")\n",
        "\n",
        "# Benchmark the models  \n",
        "t_fp32 = helper_utils.bench(model, device=\"cpu\", shape=(32, 3, 224, 224))\n",
        "t_int8 = helper_utils.bench(qmodel, device=\"cpu\", shape=(32, 3, 224, 224))\n",
        "print(\"\\n[TIMING] avg forward per batch (CPU)\")\n",
        "print(f\"  - FP32 : {t_fp32*1e3:.2f} ms\")\n",
        "print(f\"  - INT8 : {t_int8*1e3:.2f} ms (‚Üì is better)\")\n",
        "print(f\"  - Improvement: {((t_fp32 - t_int8)/t_fp32)*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Expected Output:\n",
        "```\n",
        "Accuracy on test dataset after quantization: 99.14%\n",
        "\n",
        "[TIMING] avg forward per batch (CPU)\n",
        "  - FP32 : 948.63 ms\n",
        "  - INT8 : 947.09 ms (‚Üì is better)\n",
        "  - Improvement: 0.2%\n",
        "```\n",
        "\n",
        "**Note:** Accuracy should be close to the base model accuracy (~99%). If you see significantly lower values, double-check your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "# Verify your code here\n",
        "\n",
        "unittests.exercise2(quantize_dynamic_linear)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='5'></a>\n",
        "## 5 - Quantization-Aware Training (QAT)\n",
        "\n",
        "In the previous sections you reduced model size and latency with pruning and dynamic quantization.  \n",
        "Now you'll make the model **quantization-friendly** and prepare it for **INT8 training** by:\n",
        "\n",
        "1) **Fusing ops** that commonly appear together (e.g., `Conv + BN + ReLU`) into single fused modules.  \n",
        "2) **Preparing for QAT**, which inserts observers and fake-quantization modules so the model \"feels\" INT8 during training and learns robust, quantization-tolerant weights.\n",
        "\n",
        "<a id='5-1'></a>\n",
        "### 5.1 What is QAT and why use it?\n",
        "\n",
        "**Quantization-Aware Training (QAT)** simulates INT8 behavior during training (via fake-quant) so that, after conversion, the **final INT8 model preserves more accuracy** than post-training quantization alone‚Äîespecially on CNNs.\n",
        "\n",
        "**Benefits:**\n",
        "- **Higher accuracy under INT8** vs. dynamic/static PTQ on many convolutional models.  \n",
        "- **Production-ready path**: train with fake-quant ‚Üí convert to real INT8 ‚Üí deploy.  \n",
        "- **Works with standard PyTorch tooling** (eager mode, observers, qconfigs).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='ex03'></a>\n",
        "### Exercise 3\n",
        "\n",
        "You'll write a recursive, **best-effort** fusion routine that:\n",
        "- Walks the model tree (`named_children`) and recurses into submodules.\n",
        "- When it finds a `nn.Sequential`, scans adjacent layers and tries to fuse the patterns listed above using `_try_fuse`.\n",
        "- Leaves unsupported cases untouched (no errors).\n",
        "\n",
        "**What to expect after fusion**\n",
        "- In `print(model)`, some sequences will be replaced by intrinsic fused modules (e.g., `ConvBnReLU2d`, `ConvReLU2d`, `LinearReLU`).\n",
        "- Forward outputs in `eval()` should remain (nearly) identical, showing fusion preserved behavior.\n",
        "- This step improves **CPU inference efficiency** and provides **better numerics for QAT**.\n",
        "\n",
        "---\n",
        "\n",
        "<details><summary><b><font color=\"green\">Additional Code Hints for <code>fuse_model_inplace</code> (click to expand)</font></b></summary>\n",
        "\n",
        "**1) Recursing through the model**\n",
        "\n",
        "You want to visit children first (depth-first), so use PyTorch's iterator:\n",
        "```python\n",
        "for _, child in model.named_children():\n",
        "    fuse_model_inplace(child)\n",
        "```\n",
        "This ensures you fuse inner blocks before the parent.\n",
        "\n",
        "**2) Only scan nn.Sequential blocks**\n",
        "\n",
        "After recursing, if a child is a Sequential with at least 2 modules, scan it:\n",
        "\n",
        "```python\n",
        "if isinstance(child, nn.Sequential) and len(child) >= 2:\n",
        "```\n",
        "Store training state, then temporarily switch to eval mode for safer BN folding:\n",
        "\n",
        "```python\n",
        "was_training = child.training\n",
        "child.eval()\n",
        "```    \n",
        "**3) Sliding window over adjacent layers**\n",
        "\n",
        "Use an index i and look at child[i], child[i+1], and (if present) child[i+2]:\n",
        "```python\n",
        "i = 0\n",
        "while i < len(child) - 1:\n",
        "    a, b = child[i], child[i + 1]\n",
        "    c = child[i + 2] if i + 2 < len(child) else None\n",
        "```\n",
        "Check patterns in priority order. After a successful fusion, bump i past the fused group and continue so you don't re-scan the just-fused spots.\n",
        "\n",
        "**4) The fusion checks (pattern + call + index advance)**\n",
        "\n",
        "Use torch.quantization.fuse_modules(child, [str(i), ...], inplace=True).\n",
        "\n",
        "Conv + BN + ReLU:\n",
        "```python\n",
        "if isinstance(a, nn.Conv2d) and isinstance(b, nn.BatchNorm2d) and isinstance(c, nn.ReLU):\n",
        "    torch.quantization.fuse_modules(child, [str(i), str(i+1), str(i+2)], inplace=True)\n",
        "    i += 3\n",
        "    continue\n",
        "```\n",
        "\n",
        "Conv + BN:\n",
        "```python\n",
        "if isinstance(a, nn.Conv2d) and isinstance(b, nn.BatchNorm2d):\n",
        "    torch.quantization.fuse_modules(child, [str(i), str(i+1)], inplace=True)\n",
        "    i += 2\n",
        "    continue\n",
        "```    \n",
        "\n",
        "Conv + ReLU:\n",
        "```python\n",
        "if isinstance(a, nn.Conv2d) and isinstance(b, nn.ReLU):\n",
        "    torch.quantization.fuse_modules(child, [str(i), str(i+1)], inplace=True)\n",
        "    i += 2\n",
        "    continue\n",
        "```\n",
        "\n",
        "Linear + ReLU:\n",
        "```python\n",
        "if isinstance(a, nn.Linear) and isinstance(b, nn.ReLU):\n",
        "    torch.quantization.fuse_modules(child, [str(i), str(i+1)], inplace=True)\n",
        "    i += 2\n",
        "    continue\n",
        "```\n",
        "\n",
        "If no pattern matched, just i += 1.\n",
        "\n",
        "**5) Restore training state**\n",
        "```python\n",
        "if was_training:\n",
        "    child.train()\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: fuse_model_inplace\n",
        "\n",
        "def fuse_model_inplace(model: nn.Module) -> nn.Module:\n",
        "    \"\"\"\n",
        "    Recursively apply best-effort eager fusion to:\n",
        "      Conv+BN+ReLU, Conv+BN, Conv+ReLU, Linear+ReLU\n",
        "    Only fuses *adjacent* modules inside nn.Sequential blocks.\n",
        "    Modifies `model` in-place and returns the *same instance*.\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ###\n",
        "    # Iterate over the named children of the model\n",
        "    for _, child in model.named_children(): # Iterate over the named children of the model\n",
        "        # Recurse first\n",
        "        fuse_model_inplace(child) # Recursively apply best-effort eager fusion to the child\n",
        "\n",
        "        # Then scan this child if it's a Sequential\n",
        "        if isinstance(child, nn.Sequential) and len(child) >= 2: # Check if the child is a Sequential and has at least 2 layers\n",
        "            # BN folding prefers eval; don't mutate outer state permanently\n",
        "            was_training = child.training # Get the training state of the child\n",
        "            child.eval() # Set the child to eval mode\n",
        "            i = 0 \n",
        "            while i < len(child) - 1: # Iterate over the child layers - 1\n",
        "                a, b = child[i], child[i+1] # Get the first and second layers at i and i + 1\n",
        "                if i + 2 < len(child): # Check if the third layer exists\n",
        "                    c = child[i+2] # Get the third layer\n",
        "                else:\n",
        "                    c = None # set the third layer to None\n",
        "\n",
        "                # Conv + BN + ReLU\n",
        "                if isinstance(a, nn.Conv2d) and isinstance(b, nn.BatchNorm2d) and isinstance(c, nn.ReLU):\n",
        "                    torch.quantization.fuse_modules(child, [str(i), str(i+1), str(i+2)], inplace=True)\n",
        "                    i += 3 \n",
        "                    continue \n",
        "                # Conv + BN\n",
        "                if isinstance(a, nn.Conv2d) and isinstance(b, nn.BatchNorm2d):\n",
        "                    torch.quantization.fuse_modules(child, [str(i), str(i+1)], inplace=True)\n",
        "                    i += 2 \n",
        "                    continue \n",
        "                # Conv + ReLU\n",
        "                if isinstance(a, nn.Conv2d) and isinstance(b, nn.ReLU):\n",
        "                    torch.quantization.fuse_modules(child, [str(i), str(i+1)], inplace=True)\n",
        "                    i += 2 \n",
        "                    continue \n",
        "                # Linear + ReLU\n",
        "                if isinstance(a, nn.Linear) and isinstance(b, nn.ReLU):\n",
        "                    torch.quantization.fuse_modules(child, [str(i), str(i+1)], inplace=True)\n",
        "                    i += 2 \n",
        "                    continue \n",
        "\n",
        "                i += 1 \n",
        "            # Check if the child was training\n",
        "            if was_training: # If the child was training\n",
        "                child.train() # Set the child to train mode\n",
        "\n",
        "    # IMPORTANT: return the same object (tests check identity)\n",
        "    return model \n",
        "\n",
        "    ### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "# Verify your code\n",
        "\n",
        "# Create a toy model to test your code\n",
        "torch.manual_seed(0)\n",
        "device = torch.device(\"cpu\")\n",
        "toy = helper_utils.ToyNet().eval().to(device)\n",
        "\n",
        "# Keep a copy for numerical comparison\n",
        "toy_copy = helper_utils.ToyNet().eval().to(device)\n",
        "toy_copy.load_state_dict(toy.state_dict())\n",
        "\n",
        "# Show BEFORE\n",
        "helper_utils.list_children(toy, \"Before fusion\")\n",
        "\n",
        "# Forward pass BEFORE\n",
        "x = torch.randn(2, 3, 32, 32, device=device)\n",
        "with torch.no_grad():\n",
        "    y_before = toy(x)\n",
        "\n",
        "# Apply your fusion function (assumes fuse_model_inplace is defined + _try_fuse available)\n",
        "ret_model = fuse_model_inplace(toy).eval()\n",
        "# Show AFTER\n",
        "helper_utils.list_children(toy, \"After fusion\")\n",
        "\n",
        "# Forward pass AFTER\n",
        "with torch.no_grad():\n",
        "    y_after = toy(x)\n",
        "\n",
        "# Report numerical closeness and fused-layer counts\n",
        "max_abs_diff = (y_before - y_after).abs().max().item()\n",
        "fused_counts = helper_utils.count_fused_layers(toy)\n",
        "\n",
        "\n",
        "print(\"\\n== Verification ==\")\n",
        "print(f\"Max |y_before - y_after|: {max_abs_diff:.6g}  (expect ~0)\")\n",
        "print(\"Fused intrinsic layers found:\", fused_counts if fused_counts else \"{} (none)\")\n",
        "\n",
        "# sanity check on output shape\n",
        "print(\"Output shapes -> before:\", tuple(y_before.shape), \", after:\", tuple(y_after.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Expected Output\n",
        "```\n",
        "== Before fusion ==\n",
        "\n",
        "[stem]\n",
        "  0: Conv2d\n",
        "  1: BatchNorm2d\n",
        "  2: ReLU\n",
        "\n",
        "[block]\n",
        "  0: Conv2d\n",
        "  1: ReLU\n",
        "  2: Conv2d\n",
        "  3: BatchNorm2d\n",
        "  4: ReLU\n",
        "\n",
        "[head]\n",
        "  0: AdaptiveAvgPool2d\n",
        "  1: Flatten\n",
        "  2: Linear\n",
        "  3: ReLU\n",
        "  4: Linear\n",
        "\n",
        "== After fusion ==\n",
        "\n",
        "[stem]\n",
        "  0: ConvReLU2d\n",
        "  1: Identity\n",
        "  2: Identity\n",
        "\n",
        "[block]\n",
        "  0: ConvReLU2d\n",
        "  1: Identity\n",
        "  2: ConvReLU2d\n",
        "  3: Identity\n",
        "  4: Identity\n",
        "\n",
        "[head]\n",
        "  0: AdaptiveAvgPool2d\n",
        "  1: Flatten\n",
        "  2: LinearReLU\n",
        "  3: Identity\n",
        "  4: Linear\n",
        "\n",
        "== Verification ==\n",
        "Max |y_before - y_after|: 1.11759e-08  (expect ~0)\n",
        "Fused intrinsic layers found: {'ConvReLU2d': 3, 'LinearReLU': 1}\n",
        "Output shapes -> before: (2, 3) , after: (2, 3)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### üìä Progress Update: Stage 3 - Final Push!\n",
        "\n",
        "**From your team lead:**  \n",
        "*\"Excellent progress! The pruning and quantization stages have dramatically improved our model's efficiency. But I just got off the phone with Metro City's technical team‚Äîthey're concerned about accuracy degradation on edge devices. This is where QAT (Quantization-Aware Training) comes in. By fine-tuning the model with fake-quantization during training, we can recover most of the accuracy loss while keeping all the speed and size benefits. This is our final optimization step before the demo!\"*\n",
        "\n",
        "**Current Status:**\n",
        "- ‚úÖ **Stage 1 (Pruning):** Complete  \n",
        "- ‚úÖ **Stage 2 (Dynamic Quantization):** Complete  \n",
        "- üîÑ **Stage 3 (QAT):** In progress - Final stage!\n",
        "\n",
        "**The Home Stretch:**  \n",
        "QAT is the secret sauce that separates good model optimization from great model optimization. You're almost there!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "# Test your code!\n",
        "\n",
        "unittests.exercise3(fuse_model_inplace)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='5-2'></a>\n",
        "### 5.2 Auxiliary Functions for QAT\n",
        "\n",
        "To complete exercise 4 you will need the following functions:\n",
        "\n",
        "#### `QATWrapper`\n",
        "Wraps your FP32 model with `QuantStub`/`DeQuantStub`. During QAT/inference prep:\n",
        "- Inputs are quantized ‚Üí model body runs with fake-quant/observers ‚Üí outputs dequantized.\n",
        "- This scaffolding allows eager-mode QAT to be applied cleanly.\n",
        "\n",
        "#### `convert_qat(model)`\n",
        "After you finish QAT fine-tuning:\n",
        "- Switch the model to `eval()`\n",
        "- Use the configured backend (e.g., **`fbgemm`** on x86, **`qnnpack`** on ARM)\n",
        "- **Convert** the QAT model to a **real INT8** model for deployment.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# Wrapper for QAT\n",
        "class QATWrapper(nn.Module):\n",
        "    def __init__(self, m):\n",
        "        super().__init__()\n",
        "        self.quant = aoq.QuantStub()\n",
        "        self.m = m\n",
        "        self.dequant = aoq.DeQuantStub()\n",
        "    def forward(self, x):\n",
        "        x = self.quant(x)\n",
        "        x = self.m(x)\n",
        "        x = self.dequant(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='ex04'></a>\n",
        "### Exercise 4\n",
        "\n",
        "Create and return a **QAT-ready copy** of the input model. Your function should:\n",
        "\n",
        "1. **Do not mutate the original**: `deepcopy` the model and work on the copy.  \n",
        "2. **Set backend**: choose the quantized engine (`\"fbgemm\"` for x86, `\"qnnpack\"` for ARM).  \n",
        "3. **Fuse first**: call your fusion pass (`fuse_model_inplace`) on the copy.  \n",
        "4. **Attach a QAT qconfig**: use `get_default_qat_qconfig(backend)` (fall back to a sensible default if needed).  \n",
        "5. **Prepare for QAT**: run eager-mode `prepare_qat` to insert observers/fake-quant modules.  \n",
        "6. **Return in `train()` mode** so the learner can fine-tune with QAT.\n",
        "\n",
        "**Outcome:**  \n",
        "You'll have a training-ready module that, after a brief fine-tuning, can be converted with `convert_qat(...)` into a compact, fast **INT8 inference model** with strong accuracy retention.\n",
        "\n",
        "#### Details: `prepare_qat`\n",
        "\n",
        "Implement `prepare_qat(model, backend=\"fbgemm\")`, a function that: returns a QAT-ready copy of an FP32 model by selecting an appropriate quantized backend, fusing eligible blocks (e.g., `Conv+BN(+ReLU)`), attaching a default QAT qconfig, and running eager-mode prepare_qat to insert observers and fake-quant modules.\n",
        "\n",
        "***Note:*** The original model must remain unmodified; the returned module must be in train() mode.\n",
        "\n",
        "The function has the following stages:\n",
        "\n",
        "1. **Clone & Switch to Train Mode (No Mutation)**\n",
        "    - Create a deep copy of the input model so the original remains intact.\n",
        "    - Put the copy in training mode: .train() (QAT requires training mode).\n",
        "2. **Select Quantized Backend**\n",
        "    - Set torch.backends.quantized.engine to the requested backend if available.\n",
        "    - Defaults: \"fbgemm\" (x86) or \"qnnpack\" (ARM).\n",
        "    - If unsupported, keep the runtime's current engine (best-effort).\n",
        "3. **Fuse Eligible Modules (Best-Effort)**\n",
        "    - Call the created helper fuse_model_inplace(qat) to fuse common patterns like (Conv, BN, ReLU), (Conv, BN), (Conv, ReLU), (Linear, ReLU).\n",
        "    \n",
        "4. **Insert Observers & Fake-Quant (Eager QAT Prepare)**\n",
        "    - Call aoq.prepare_qat(qat, inplace=True) to add observers and fake-quant modules throughout the network.\n",
        "    - These modules simulate quantization effects during training.\n",
        "    - Ensure the returned module is in train() mode and ready for QAT fine-tuning.\n",
        "\n",
        "Return the new (deep-copied) QAT-ready model.\n",
        "\n",
        "The output of this function‚Äîa QAT-ready model‚Äîwill then be fine-tuned for a few epochs. During training, the inserted observers and fake-quantization modules learn appropriate scales/zero-points, enabling a high-accuracy post-training convert to INT8 later.\n",
        "\n",
        "<details> <summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
        "\n",
        "**1. Backend selection (best-effort)**\n",
        "\n",
        "Prefer:\n",
        "- \"fbgemm\" on x86/AVX2+ CPUs\n",
        "- \"qnnpack\" on ARM\n",
        "\n",
        "Safe pattern:\n",
        "```python\n",
        "if hasattr(torch.backends, \"quantized\") and hasattr(torch.backends.quantized, \"engine\"):\n",
        "    try:\n",
        "        torch.backends.quantized.engine = backend  # e.g., \"fbgemm\" or \"qnnpack\"\n",
        "    except Exception:\n",
        "        pass  # leave current engine if unsupported\n",
        "```\n",
        "\n",
        "**2. Fusion helper**\n",
        "\n",
        "You are given fuse_model_inplace(qat). Call it after copying and before preparing QAT:\n",
        "```python\n",
        "fuse_model_inplace(qat)  # best-effort; no-op if pattern not found\n",
        "```\n",
        "Fusing improves numerical stability and performance for quantization workflows.\n",
        "\n",
        "**3. QAT configuration**\n",
        "\n",
        "Pick a default QAT config tied to the backend; fall back to \"fbgemm\" if needed:\n",
        "```python\n",
        "qat.qconfig = torch.quantization.get_default_qat_qconfig(backend)\n",
        "```\n",
        "\n",
        "**4. Insert observers & fake-quant**\n",
        "\n",
        "Use eager-mode QAT preparation:\n",
        "```python\n",
        "aoq.prepare_qat(qat, inplace=True)\n",
        "qat.train()  # ensure training mode for QAT fine-tuning\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: prepare_qat\n",
        "\n",
        "def prepare_qat(model, backend=\"fbgemm\"):\n",
        "    \"\"\"\n",
        "    Return a **QAT-ready copy** of `model`:\n",
        "      - Sets quantized backend (default: 'fbgemm')\n",
        "      - Applies best-effort fusion (Conv+BN(+Act))\n",
        "      - Attaches a default QAT qconfig\n",
        "      - Runs eager-mode prepare_qat to insert observers/fake-quant\n",
        "      - Returns the prepared module in **train()** mode\n",
        "\n",
        "    The original `model` **must not** be mutated.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        FP32 model to prepare for QAT.\n",
        "    backend : str\n",
        "        Quantized engine (use 'fbgemm' on x86; 'qnnpack' on ARM).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    nn.Module\n",
        "        A new, QAT-ready model (with observers) in training mode.\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ###\n",
        "    # 1) Work on a copy; do not mutate the original\n",
        "    qat = copy.deepcopy(model) # Deepcopy the input model to qat\n",
        "    qat.eval()\n",
        "\n",
        "    # 2) Fuse eligible modules (best-effort; safe no-op if unsupported)\n",
        "    fuse_model_inplace(qat) # Apply best-effort eager fusion to qat\n",
        "\n",
        "    # 3) Attach default QAT qconfig\n",
        "    qat.qconfig = torch.ao.quantization.get_default_qat_qconfig(backend) # Get the default QAT qconfig for the specified backend and set it to qat.qconfig\n",
        "\n",
        "    # 4) Prepare for QAT (insert observers/fake-quant)\n",
        "    qat.train()\n",
        "    torch.quantization.prepare_qat( # Prepare the model for QAT\n",
        "        qat, # The model to prepare\n",
        "        inplace=True, # Set the correct value for inplace\n",
        "        ) \n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    return qat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "# Verify your code here\n",
        "\n",
        "# Use the base model to start fresh\n",
        "\n",
        "model = helper_utils.resnet18_qat_ready_pretrained(num_classes=3, use_quant_stubs=False).to(device)\n",
        "model_weights = torch.load(\"street_classifier_weights.pt\", map_location=\"cpu\")\n",
        "model.load_state_dict(model_weights)\n",
        "# wrap the base model in QATWrapper to add stubs for quantization\n",
        "wrapped_model = QATWrapper(model)\n",
        "\n",
        "print(\"Base Model loaded and wrapped\")\n",
        "\n",
        "# Prepare the QAT model\n",
        "qat_model = prepare_qat(wrapped_model, backend=\"fbgemm\")\n",
        "print(\"Model prepared for qat\")\n",
        "\n",
        "# Fine-tune with fake-quant in the loop (can be on GPU)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD((p for p in qat_model.parameters() if p.requires_grad),\n",
        "                            lr=1e-4, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "qat_model.to(device)\n",
        "\n",
        "helper_utils.train_model(\n",
        "    qat_model,\n",
        "    train_loader,\n",
        "    dev_loader,\n",
        "    1,\n",
        "    optimizer,\n",
        "    device,\n",
        "    save_path=\"fine_tuned_qat_model.pt\")\n",
        "    \n",
        "qat_model.to(\"cpu\")\n",
        "\n",
        "# Convert to real INT8 (runs on CPU)\n",
        "qat_model.eval()\n",
        "int8_model = torch.quantization.convert(qat_model)\n",
        "print(\"Model converted to int8\")\n",
        "\n",
        "# Save the quantized model with full state\n",
        "torch.save({\n",
        "    'model_state_dict': int8_model.state_dict(),\n",
        "    'quantization_config': int8_model.state_dict()\n",
        "}, \"quantized_int8_model.pt\")\n",
        "\n",
        "print(\"Saved quantized model checkpoint to quantized_int8_model.pt\")\n",
        "\n",
        "# Evaluate int8 model on test data\n",
        "int8_model.eval()\n",
        "print(\"Testing model on cpu\")\n",
        "test_acc = helper_utils.compute_accuracy(int8_model, test_loader, device=\"cpu\")\n",
        "print(f\"Test accuracy in base model: {base_accuracy:.2f}%\")\n",
        "print(f'\\nInt8 model test accuracy: {test_acc:.2f}%')\n",
        "\n",
        "# Measure inference time for both models on cpu\n",
        "model.to(\"cpu\")\n",
        "int8_model.to(\"cpu\")\n",
        "base_time = helper_utils.bench(model, device=\"cpu\", shape=(32, 3, 224, 224))\n",
        "int8_time = helper_utils.bench(int8_model, device=\"cpu\", shape=(32, 3, 224, 224))\n",
        "\n",
        "# Calculate percentage improvement\n",
        "time_improvement = ((base_time - int8_time) / base_time) * 100\n",
        "\n",
        "print(f\"\\nInference time comparison:\")\n",
        "print(f\"Base model: {base_time:.4f} seconds per batch\")\n",
        "print(f\"Int8 model: {int8_time:.4f} seconds per batch\") \n",
        "print(f\"Speed improvement: {time_improvement:.1f}%\")\n",
        "\n",
        "# Save both models weights to compare sizes\n",
        "torch.save(model.state_dict(), \"base_model_weights.pt\")\n",
        "torch.save(int8_model.state_dict(), \"int8_model_weights.pt\")\n",
        "\n",
        "# Get file sizes in MB\n",
        "base_size = os.path.getsize(\"base_model_weights.pt\") / (1024 * 1024)\n",
        "int8_size = os.path.getsize(\"int8_model_weights.pt\") / (1024 * 1024)\n",
        "\n",
        "print(f\"\\nModel size comparison:\")\n",
        "print(f\"Base model: {base_size:.2f} MB\")\n",
        "print(f\"Int8 model: {int8_size:.2f} MB\")\n",
        "print(f\"Size reduction: {((base_size - int8_size) / base_size * 100):.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Expected Output\n",
        "\n",
        "```\n",
        "Base Model loaded and wrapped\n",
        "Model prepared for qat\n",
        "\n",
        "New best accuracy: 0.9519, saved model to best_model.pt\n",
        "\n",
        "New best accuracy: 0.9667, saved model to best_model.pt\n",
        "\n",
        "Training completed:\n",
        "Best accuracy: 0.9667\n",
        "Final accuracy: 0.9667\n",
        "Final model saved to final_model.pt\n",
        "Model converted to int8\n",
        "Saved quantized model checkpoint to quantized_int8_model.pt\n",
        "Testing model on cpu\n",
        "\n",
        "Test accuracy in base model: 0.97%\n",
        "\n",
        "Int8 model test accuracy: 0.96%\n",
        "\n",
        "Inference time comparison:\n",
        "Base model: 0.0346 seconds per batch\n",
        "Int8 model: 0.0193 seconds per batch\n",
        "Speed improvement: 44.2%\n",
        "\n",
        "Model size comparison:\n",
        "Base model: 512.22 MB\n",
        "Int8 model: 128.31 MB\n",
        "Size reduction: 75.0%\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "# Test your code!\n",
        "\n",
        "unittests.exercise4(prepare_qat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Submission Note\n",
        "\n",
        "Congratulations! You've completed the final graded exercise of this assignment.\n",
        "\n",
        "If you've successfully passed all the unit tests above, you've completed the core requirements of this assignment. Feel free to [submit](#submission) your work now. The grading process runs in the background, so it will not disrupt your progress and you can continue on with the rest of the material.\n",
        "\n",
        "**üö® IMPORTANT NOTE** If you have passed all tests within the notebook, but the autograder shows a system error after you submit your work:\n",
        "\n",
        "<div style=\"background-color: #1C1C1E; border: 1px solid #444444; color: #FFFFFF; padding: 15px; border-radius: 5px;\">\n",
        "    <p><strong>Grader Error: Grader feedback not found</strong></p>\n",
        "    <p>Autograder failed to produce the feedback...</p>\n",
        "</div>\n",
        "<br>\n",
        "\n",
        "This is typically a temporary system glitch. The most common solution is to resubmit your assignment, as this often resolves the problem. Occasionally, it may be necessary to resubmit more than once. \n",
        ">\n",
        "If the error persists, please reach out for support in the [DeepLearning.AI Community Forum](https://community.deeplearning.ai/c/course-q-a/pytorch-for-developers/pytorch-advanced-architectures-and-deployment/562).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion: Mission Accomplished! üéâ\n",
        "\n",
        "**Congratulations, ML Engineer!**\n",
        "\n",
        "You've successfully optimized the StreetClassifier for Metro City's edge deployment! Let's review what you achieved:\n",
        "\n",
        "**Your Results:**\n",
        "- ‚úÖ **Model Size:** Reduced from 512 MB ‚Üí 128 MB (75% reduction) - Well under the 150 MB target!\n",
        "- ‚úÖ **CPU Inference Speed:** Improved by 44.2% - Now running in acceptable real-time\n",
        "- ‚úÖ **Accuracy:** Maintained at ~96% - Above the city's 95% requirement\n",
        "- ‚úÖ **Deployment-Ready:** Model now runs efficiently on CPU-only edge devices\n",
        "\n",
        "**What You Learned:**\n",
        "\n",
        "By saving and reloading state dictionaries, you protected your progress and created a repeatable path back to working models. With **pruning**, you explored how zeroing less useful parameters can make a network leaner, and you saw how masks change behavior without immediately changing tensor shapes. **Dynamic quantization** gave you a rapid path to smaller weights and faster CPU inference, and the **fusion plus quantization-aware training** workflow helped you fine-tune under simulated INT8 effects so the final converted model holds onto accuracy while gaining speed.\n",
        "\n",
        "**Impact:**\n",
        "\n",
        "The optimized StreetClassifier is now:\n",
        "- Lighter and faster for edge deployment\n",
        "- Easier to package and distribute across Metro City's fleet\n",
        "- Ready for the city council demo next week!\n",
        "\n",
        "Thanks to your work, CleanVision AI is positioned to secure the nationwide expansion contract. The same workflow you mastered scales to larger architectures and different tasks: prune with intent, quantify the accuracy and latency impact, and then use quantization-aware training when you want the best balance.\n",
        "\n",
        "**Next Steps for Your Career:**\n",
        "\n",
        "Now that you've mastered deployment optimization, consider:\n",
        "- Automating experiment tracking for reproducible results\n",
        "- Exploring structured pruning with channel removal to alter layer shapes\n",
        "- Exporting to formats like ONNX or TorchScript for even broader hardware compatibility\n",
        "- Deploying models to real edge devices (Raspberry Pi, NVIDIA Jetson, mobile phones)\n",
        "\n",
        "Your models can now serve users wherever they live‚Äîfrom smart cities to IoT devices to mobile applications. Keep building! üöÄ"
      ]
    }
  ],
  "metadata": {
    "grader_version": "1",
    "kernelspec": {
      "display_name": ".venv (3.12.6)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
