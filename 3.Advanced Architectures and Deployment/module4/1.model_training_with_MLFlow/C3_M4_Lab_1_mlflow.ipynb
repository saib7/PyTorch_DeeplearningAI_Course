{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9aa361f-1e3f-49fd-b3de-ac27ac1ed683",
   "metadata": {},
   "source": [
    "# PyTorch Model Training with MLflow & Lightning: Tracking & Management\n",
    "\n",
    "Welcome to this hands-on lab! You've likely experienced the frustration of training a model for hours, achieving great accuracy, only to have it disappear when your notebook kernel restarts. This happens because a trained model exists only in memory; to preserve your work, you need a way to save it to disk, a process known as serialization.\n",
    "\n",
    "This notebook moves you beyond simple training and into the essential practice of robust model management. You'll apply these concepts by training a PyTorch CNN using **PyTorch Lightning** and integrating a powerful tool, [MLflow](https://mlflow.org), to systematically track and organize your work, ensuring no effort is ever lost.\n",
    "\n",
    "By the end of this lab, you'll have hands-on experience with:\n",
    "\n",
    "* Using **Lightning** to simplify and organize your training code with `LightningModule` and `LightningDataModule`.\n",
    "\n",
    "* Creating custom **Lightning Callbacks** to extend functionality during training.\n",
    "\n",
    "* Setting up an **MLflow experiment** to serve as a dedicated container for your project's training runs.\n",
    "\n",
    "* Logging **hyperparameters** and tracking metrics so you always have a record of the exact settings and performance of each run.\n",
    "\n",
    "* Saving **model checkpoints** during training, creating snapshots of your best-performing models that you can reload instantly.\n",
    "\n",
    "* Logging **artifacts**, such as your saved model files and performance plots, directly to MLflow for easy access and comparison.\n",
    "\n",
    "* Accessing your results both through the interactive **MLflow UI** and programmatically with the **Python client**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71d72f8-f0e1-47c8-8984-db414e3ee4da",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436aafb4-86c2-40dd-9c86-f0d4740b141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Redirect stderr to a black hole to catch other potential messages\n",
    "class BlackHole:\n",
    "    def write(self, message):\n",
    "        pass\n",
    "    def flush(self):\n",
    "        pass\n",
    "sys.stderr = BlackHole()\n",
    "\n",
    "# Ignore Python-level UserWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e3acda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from lightning.pytorch.callbacks import Callback\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchmetrics.classification import Accuracy, ConfusionMatrix\n",
    "\n",
    "import helper_utils\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "logging.getLogger(\"mlflow\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c21ae59-e22d-425f-afaf-6e2db009b369",
   "metadata": {},
   "source": [
    "## Random Seed Initialization\n",
    "\n",
    "* Set a global random seed for PyTorch CPU and GPU operations.\n",
    "    * This is a crucial step for ensuring reproducibility of training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e583c15-cef0-42e7-940b-70b8ccb21773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the global random seed value\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Set seed for PyTorch CPU operations\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    # Set seed for PyTorch GPU operations on all available GPUs\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c430103-bf7f-4f5b-a6b3-d8fa3f7a2b1e",
   "metadata": {},
   "source": [
    "## CIFAR-10 DataModule\n",
    "\n",
    "* Create a `LightningDataModule` to encapsulate all data loading logic including transformations, dataset downloads, and dataloader creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0445a015-f9fe-4d74-9750-b0289b3cdc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10DataModule(pl.LightningDataModule):\n",
    "    \"\"\"A LightningDataModule for the CIFAR10 dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir='./CIFAR10_data', batch_size=128, num_workers=2):\n",
    "        \"\"\"\n",
    "        Initializes the DataModule.\n",
    "\n",
    "        Args:\n",
    "            data_dir (str): Directory to store the data.\n",
    "            batch_size (int): Number of samples per batch.\n",
    "            num_workers (int): Number of subprocesses for data loading.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        # Define transformations for training data\n",
    "        self.transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), \n",
    "                               std=(0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "        \n",
    "        # Define transformations for validation data\n",
    "        self.transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), \n",
    "                               std=(0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "        \n",
    "        # CIFAR-10 class labels\n",
    "        self.classes = ('plane', 'car', 'bird', 'cat', 'deer', \n",
    "                       'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"Downloads the CIFAR10 dataset if not already present.\"\"\"\n",
    "        # Check if data already exists\n",
    "        if os.path.exists(self.data_dir) and os.path.isdir(self.data_dir):\n",
    "            print(\"CIFAR10 Data folder found locally. Loading from local.\\n\")\n",
    "        else:\n",
    "            print(\"CIFAR10 Data folder not found locally. Downloading data.\\n\")\n",
    "            \n",
    "        # Download the dataset (will skip if already exists)\n",
    "        torchvision.datasets.CIFAR10(root=self.data_dir, train=True, download=True)\n",
    "        torchvision.datasets.CIFAR10(root=self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"\n",
    "        Assigns train/val datasets for use in dataloaders.\n",
    "\n",
    "        Args:\n",
    "            stage (str, optional): The stage of training (e.g., 'fit', 'test').\n",
    "        \"\"\"\n",
    "        # Create the training dataset\n",
    "        self.cifar_train = torchvision.datasets.CIFAR10(\n",
    "            root=self.data_dir, train=True, transform=self.transform_train\n",
    "        )\n",
    "        \n",
    "        # Create the validation dataset\n",
    "        self.cifar_val = torchvision.datasets.CIFAR10(\n",
    "            root=self.data_dir, train=False, transform=self.transform_test\n",
    "        )\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        \"\"\"Returns the DataLoader for the training set.\"\"\"\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.cifar_train, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=True, \n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \"\"\"Returns the DataLoader for the validation set.\"\"\"\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.cifar_val, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "\n",
    "# Instantiate the data module\n",
    "data_module = CIFAR10DataModule()\n",
    "\n",
    "# Get class names for later use\n",
    "classes = data_module.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebd62ce-eae3-492c-bfc1-f349a1869fde",
   "metadata": {},
   "source": [
    "## CNN Model Definition as LightningModule\n",
    "\n",
    "* Define a `SimpleCNNLightning` class, which extends `pl.LightningModule` and encapsulates the model architecture, training logic, and optimizer configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3324052c-b758-42dc-ad93-e41309786095",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(pl.LightningModule):\n",
    "    \"\"\"A Lightning-wrapped CNN for CIFAR10 image classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001):\n",
    "        \"\"\"\n",
    "        Initializes the LightningModule.\n",
    "        \n",
    "        Args:\n",
    "            learning_rate (float): The learning rate for the optimizer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Save hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Define the model architecture\n",
    "        self.model = nn.Sequential(\n",
    "            # Convolutional layers\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # Fully connected layers\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 4 * 4, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "        \n",
    "        # Loss function\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Metrics\n",
    "        self.train_accuracy = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            x: The input tensor containing a batch of images.\n",
    "            \n",
    "        Returns:\n",
    "            The output tensor (logits) from the model.\n",
    "        \"\"\"\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Performs a single training step.\n",
    "        \n",
    "        Args:\n",
    "            batch: The data batch from the dataloader.\n",
    "            batch_idx: The index of the current batch.\n",
    "            \n",
    "        Returns:\n",
    "            The loss value for backpropagation.\n",
    "        \"\"\"\n",
    "        inputs, labels = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = self.loss_fn(outputs, labels)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        self.train_accuracy(preds, labels)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_acc\", self.train_accuracy, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Performs a single validation step.\n",
    "        \n",
    "        Args:\n",
    "            batch: The data batch from the dataloader.\n",
    "            batch_idx: The index of the current batch.\n",
    "        \"\"\"\n",
    "        inputs, labels = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = self.loss_fn(outputs, labels)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        self.val_accuracy(preds, labels)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", self.val_accuracy, prog_bar=True)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configures and returns the optimizer and learning rate scheduler.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary containing optimizer and scheduler configuration.\n",
    "        \"\"\"\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "        \n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce66734-6d57-4930-9104-6f8d3219d161",
   "metadata": {},
   "source": [
    "## Confusion Matrix Plotting Function\n",
    "\n",
    "* Define `plot_confusion_matrix` to generate and save a visual confusion matrix from a pre-computed matrix tensor.\n",
    "    * MLflow can log various artifacts, including graphical plots like confusion matrices, to provide deeper insights into model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b982de3-851c-499b-849d-a7b5b75437d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, class_names):\n",
    "    # Plotting the Confusion Matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    # Define a threshold for text color (white on dark, black on light)\n",
    "    thresh = cm.max() / 2.\n",
    "    # Iterate over each cell in the confusion matrix\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            # Add the count as text to the cell\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     # Set text color based on cell value and threshold\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    # Adjust plot layout to prevent labels from overlapping\n",
    "    plt.tight_layout()\n",
    "    # Set the y-axis label\n",
    "    plt.ylabel('True label')\n",
    "    # Set the x-axis label\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    # Define the filename for the saved plot\n",
    "    output_filename = 'confusion_matrix.png'\n",
    "    # Save the plot to a file\n",
    "    plt.savefig(output_filename)\n",
    "    # Close the plot figure to free up memory\n",
    "    plt.close()\n",
    "\n",
    "    return output_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a6ba33-9ded-451f-8457-82d917b682f0",
   "metadata": {},
   "source": [
    "## MLflow Logging Callback\n",
    "\n",
    "* While Lightning provides a built-in [MLFlowLogger](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.loggers.mlflow.html) for standard experiment tracking, this lab uses a custom `Callback`. This approach allows for more advanced control, such as logging custom-generated plots and implementing unique checkpointing logic that the built-in logger does not support.\n",
    "* Define a custom Lightning `Callback` to automatically log metrics, hyperparameters, and artifacts to MLflow during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35b102e-343b-4c41-b210-86bb78ae1f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLflowLoggingCallback(Callback):\n",
    "    \"\"\"\n",
    "    A Lightning Callback to log metrics, hyperparameters, and artifacts to MLflow.\n",
    "    \n",
    "    This callback handles:\n",
    "    - Logging hyperparameters at the start of training\n",
    "    - Logging metrics after each epoch\n",
    "    - Saving and logging model checkpoints for best models\n",
    "    - Logging the confusion matrix at the end of training\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, classes):\n",
    "        \"\"\"\n",
    "        Initialize the MLflow logging callback.\n",
    "        \n",
    "        Args:\n",
    "            classes: Tuple of class names for the confusion matrix.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.classes = classes\n",
    "        self.best_accuracy = 0\n",
    "        self.model_save_dir = \"./best_model\"\n",
    "        os.makedirs(self.model_save_dir, exist_ok=True)\n",
    "    \n",
    "    def on_train_start(self, trainer, pl_module):\n",
    "        \"\"\"\n",
    "        Log hyperparameters at the start of training.\n",
    "        \n",
    "        Args:\n",
    "            trainer: The Lightning Trainer instance.\n",
    "            pl_module: The LightningModule being trained.\n",
    "        \"\"\"\n",
    "        # Log model hyperparameters\n",
    "        mlflow.log_param(\"model_type\", \"SimpleCNN\")\n",
    "        mlflow.log_param(\"optimizer\", \"Adam\")\n",
    "        mlflow.log_param(\"initial_lr\", pl_module.hparams.learning_rate)\n",
    "        mlflow.log_param(\"scheduler\", \"ReduceLROnPlateau\")\n",
    "        mlflow.log_param(\"batch_size\", trainer.datamodule.batch_size)\n",
    "        mlflow.log_param(\"random_seed\", RANDOM_SEED)\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        \"\"\"\n",
    "        Log metrics and save checkpoints after each validation epoch.\n",
    "        \n",
    "        Args:\n",
    "            trainer: The Lightning Trainer instance.\n",
    "            pl_module: The LightningModule being validated.\n",
    "        \"\"\"\n",
    "        # Skip if in sanity checking mode\n",
    "        if trainer.sanity_checking:\n",
    "            return\n",
    "        \n",
    "        # Get current metrics\n",
    "        metrics = trainer.callback_metrics\n",
    "        current_epoch = trainer.current_epoch\n",
    "        \n",
    "        # Log metrics to MLflow\n",
    "        if \"train_loss\" in metrics:\n",
    "            mlflow.log_metric(\"train_loss\", metrics[\"train_loss\"].item(), step=current_epoch)\n",
    "        if \"val_loss\" in metrics:\n",
    "            mlflow.log_metric(\"val_loss\", metrics[\"val_loss\"].item(), step=current_epoch)\n",
    "        if \"val_acc\" in metrics:\n",
    "            accuracy = metrics[\"val_acc\"].item() * 100\n",
    "            mlflow.log_metric(\"accuracy\", accuracy, step=current_epoch)\n",
    "            \n",
    "            # Log learning rate\n",
    "            current_lr = trainer.optimizers[0].param_groups[0]['lr']\n",
    "            mlflow.log_metric(\"learning_rate\", current_lr, step=current_epoch)\n",
    "            \n",
    "            # Save checkpoint if this is the best model so far\n",
    "            if accuracy > self.best_accuracy:\n",
    "                self.best_accuracy = accuracy\n",
    "                \n",
    "                # Create checkpoint\n",
    "                checkpoint = {\n",
    "                    'epoch': current_epoch + 1,\n",
    "                    'model_state_dict': pl_module.state_dict(),\n",
    "                    'optimizer_state_dict': trainer.optimizers[0].state_dict(),\n",
    "                    'val_loss': metrics[\"val_loss\"].item(),\n",
    "                    'accuracy': accuracy,\n",
    "                    'random_seed': RANDOM_SEED\n",
    "                }\n",
    "                \n",
    "                # Save checkpoint file\n",
    "                checkpoint_filename = f'best_model_checkpoint_epoch_{current_epoch + 1}.pt'\n",
    "                checkpoint_path = os.path.join(self.model_save_dir, checkpoint_filename)\n",
    "                torch.save(checkpoint, checkpoint_path)\n",
    "                \n",
    "                # Log checkpoint to MLflow\n",
    "                mlflow.log_artifact(checkpoint_path)\n",
    "    \n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        \"\"\"\n",
    "        Log final metrics and artifacts at the end of training.\n",
    "        \n",
    "        Args:\n",
    "            trainer: The Lightning Trainer instance.\n",
    "            pl_module: The LightningModule that was trained.\n",
    "        \"\"\"\n",
    "        # Log best accuracy\n",
    "        mlflow.log_metric(\"best_accuracy\", self.best_accuracy)\n",
    "        \n",
    "        # Generate and log confusion matrix\n",
    "        print(\"\\nCalculating final confusion matrix for artifact logging...\")\n",
    "        confmat_metric = ConfusionMatrix(task=\"multiclass\", num_classes=10).to(pl_module.device)\n",
    "        \n",
    "        pl_module.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in trainer.val_dataloaders:\n",
    "                images, labels = batch\n",
    "                images, labels = images.to(pl_module.device), labels.to(pl_module.device)\n",
    "                outputs = pl_module(images)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                confmat_metric.update(preds, labels)\n",
    "        \n",
    "        final_cm = confmat_metric.compute().cpu().numpy()\n",
    "        cm_path = plot_confusion_matrix(final_cm, self.classes)\n",
    "        mlflow.log_artifact(cm_path)\n",
    "        \n",
    "        # Log the trained model\n",
    "        input_example_tensor, _ = next(iter(trainer.val_dataloaders))\n",
    "        input_example_numpy = input_example_tensor.cpu().numpy()\n",
    "        \n",
    "        # Move model to CPU for serialization\n",
    "        pl_module.to(\"cpu\")\n",
    "        \n",
    "        # Log the PyTorch model to MLflow\n",
    "        mlflow.pytorch.log_model(\n",
    "            pytorch_model=pl_module,\n",
    "            artifact_path=\"cifar10_cnn_model_final\",\n",
    "            input_example=input_example_numpy\n",
    "        )\n",
    "        \n",
    "        print(f'\\nFinished Training. Best accuracy: {self.best_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ad6811-142b-4f1a-89eb-8cfa0fc44091",
   "metadata": {},
   "source": [
    "## MLflow Experiment Setup\n",
    "\n",
    "* Set the active MLflow experiment to \"CIFAR10_CNN_Lightning\" using [mlflow.set_experiment()](https://mlflow.org/docs/latest/api_reference/python_api/mlflow.html?highlight=set_experiment#mlflow.set_experiment).\n",
    "    * By default, the name of the experiment is set to `CIFAR10_CNN_Lightning`. You can rename it to anything you want.\n",
    "* Organizing runs within named experiments is a fundamental aspect of using MLflow for structured experiment tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218637c9-92ae-4008-b96b-811dec578771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set or create an MLflow experiment named \"CIFAR10_CNN_Lightning\"\n",
    "mlflow.set_experiment(\"CIFAR10_CNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2297b72-6246-4281-a7cb-462d4e5893da",
   "metadata": {},
   "source": [
    "## MLflow Run and Training with Lightning Trainer\n",
    "\n",
    "* Begin an MLflow run context using `mlflow.start_run()`, which groups all subsequent logging under a single run ID.\n",
    "* **Lightning Trainer Setup**:\n",
    "    * Configure the Lightning `Trainer` with essential parameters:\n",
    "        * `max_epochs`: Number of training epochs\n",
    "        * `accelerator=\"auto\"`: Automatically detect and use available hardware (GPU/CPU)\n",
    "        * `devices=1`: Use a single device\n",
    "        * `logger=False`: Disable default Lightning logging (we use MLflow instead)\n",
    "        * `callbacks`: Pass the `MLflowLoggingCallback` to handle all MLflow logging\n",
    "        * `enable_progress_bar=True`: Show training progress\n",
    "* **Training Execution**:\n",
    "    * Call `trainer.fit(model, data_module)` to run the entire training loop automatically\n",
    "    * Lightning handles all the training/validation loops, gradient updates, and device management\n",
    "    * The callback handles all MLflow logging throughout the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964fbc5a-e5f8-4471-ad30-29626c6e1042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start an MLflow run context\n",
    "with mlflow.start_run() as run:\n",
    "    \n",
    "    # Define the total number of training epochs\n",
    "    num_epochs = 10\n",
    "    \n",
    "    # Create the model\n",
    "    model = SimpleCNN(learning_rate=0.001)\n",
    "    \n",
    "    # Create the MLflow logging callback\n",
    "    mlflow_callback = MLflowLoggingCallback(classes=classes)\n",
    "    \n",
    "    # Configure the Lightning Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=num_epochs,\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        logger=False,  # Disable default Lightning logging\n",
    "        callbacks=[mlflow_callback],\n",
    "        enable_progress_bar=True,\n",
    "        enable_model_summary=True,\n",
    "        enable_checkpointing=False  # We handle checkpointing in the callback\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.fit(model, data_module)\n",
    "    \n",
    "    print(f'\\nMLflow run id: {run.info.run_id}')\n",
    "    print('To view results run mlflow ui')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfaf494-511d-4ca0-baf2-6515aa2535aa",
   "metadata": {},
   "source": [
    "## MLflow UI Setup\n",
    "\n",
    "### Navigating the UI\n",
    "\n",
    "* The `helper_utils.show_ui_navigation_instructions()` function will render a step-by-step guide on how to use the MLflow UI.\n",
    "    * To show the instructions, pass `display_instructions=True`.\n",
    "    * To hide the instructions, you can omit the argument, as it defaults to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c92dbd-a06b-4f23-8579-bb01228d05a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameter to True to see the instructions\n",
    "helper_utils.show_ui_navigation_instructions(display_instructions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f9b3bb-3341-4552-9047-0d76174aeac8",
   "metadata": {},
   "source": [
    "### Running the MLflow UI\n",
    "\n",
    "* Use `start_mlflow_ui()` to launch the MLflow UI server with nginx reverse proxy support.\n",
    "* The server will run in the background, allowing you to continue with the notebook without interruption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecb4c22-6e63-4831-aedb-a856b4ada5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the MLflow UI server (runs in background)\n",
    "mlflow_process = helper_utils.start_mlflow_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7606b62-3776-4341-a557-0b7dc2137c6e",
   "metadata": {},
   "source": [
    "## Alternative to UI: Viewing MLflow Logs via Python Client in Notebook\n",
    "\n",
    "While the MLflow UI provides a comprehensive graphical interface for exploring your experiment runs, there are times when accessing and reviewing your MLflow logs directly within your Jupyter Notebook can be more convenient or suitable for specific workflows. This section demonstrates how you can use MLflow's Python client to programmatically retrieve and display information about your experiment runs without leaving the notebook environment.\n",
    "\n",
    "The approach shown here offers a minimalistic view, primarily focusing on fetching details for specific runs based on their Run IDs. However, this is just a starting point. The MLflow Python client is quite powerful, and you can easily adapt and expand upon these methods to customize the information you want to extract and the way it's presented, tailoring it to your specific analytical or reporting needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eab4a61-a1d8-4852-8f7a-02eca761c05b",
   "metadata": {},
   "source": [
    "* Initialize [MLflow Client](https://mlflow.org/docs/1.25.1/python_api/mlflow.tracking.html) to interact with MLflow.\n",
    "* [Search](https://mlflow.org/docs/latest/search-experiments/) and display all experiments and, for each, list their runs with start times and names, _prompting you to copy a run_id_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5002922f-9bc2-429a-be8a-089c0c6fe47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "print(\"MLflow Client Initialized.\\n\")\n",
    "print(\"Listing all Run IDs from all experiments (descending by start time)...\\n\")\n",
    "\n",
    "experiments = client.search_experiments()\n",
    "\n",
    "if not experiments:\n",
    "    print(\"No experiments found.\")\n",
    "else:\n",
    "    all_runs_listed = False\n",
    "    for exp in experiments:\n",
    "        print(f\"Experiment: {exp.name} (ID: {exp.experiment_id})\")\n",
    "        runs = client.search_runs(exp.experiment_id, order_by=[\"attributes.start_time DESC\"])\n",
    "        \n",
    "        if not runs:\n",
    "            print(\"  No runs found in this experiment.\")\n",
    "        else:\n",
    "            all_runs_listed = True\n",
    "            for run_info in runs:\n",
    "                start_time_seconds = run_info.info.start_time / 1000.0\n",
    "                start_time_formatted = datetime.datetime.fromtimestamp(start_time_seconds).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                \n",
    "                run_name_tag = run_info.data.tags.get('mlflow.runName')\n",
    "                display_name = f\" (Name: {run_name_tag})\" if run_name_tag else \"\"\n",
    "                print(f\"  Run ID: {run_info.info.run_id}{display_name} - Started: {start_time_formatted}\")\n",
    "        print(\"-\" * 30)\n",
    "    if not all_runs_listed:\n",
    "        print(\"\\nNo runs found across any experiment.\")\n",
    "\n",
    "print(\"\\nCopy the Run ID you want to inspect from the list above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499daaae-5a50-406c-a1f9-c41951eb1ddd",
   "metadata": {},
   "source": [
    "* Copy a specific Run ID from the displayed list above, which you want to inspect.\n",
    "    * Set (paste) this as a `str` in `run_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82a15d3-fa06-4476-be69-821fc1a5110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste the Run ID you copied into the run_id variable below.\n",
    "run_id = \"2162895c442a4a619230f2b94d93c195\"  # <-- PASTE YOUR RUN ID BETWEEN THE QUOTES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d5dccc-863f-4330-bbc6-d4d07d81db24",
   "metadata": {},
   "source": [
    "* Use `display_mlflow_run_details` function to fetch and display a formatted summary of the run, which includes:\n",
    "    * The selected **Run ID** and the name/ID of the experiment it belongs to.\n",
    "    * The values of key **hyperparameters** that were logged for the run.\n",
    "    * The final values of all tracked **metrics**.\n",
    "    * A list of all **artifacts** associated with the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbc2508-3add-45bf-9f20-aa574d035b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Validate the run_id and print a confirmation.\n",
    "    if not run_id or not run_id.strip():\n",
    "        print(\"Error: Please paste a valid Run ID. It cannot be empty or just whitespace.\")\n",
    "    else:\n",
    "        print(f\"Run ID set to: {run_id}\")\n",
    "        # Call the helper function to display the run details.\n",
    "        helper_utils.display_mlflow_run_details(run_id)\n",
    "\n",
    "except NameError:\n",
    "    print(\"Error: The 'run_id' variable is not defined. Please make sure it's set in the try block.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe7c723-8e5a-4a17-9771-aeaf03ed9ee4",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations on completing this lab! You have successfully trained a CNN using **PyTorch Lightning** and, more importantly, implemented a systematic workflow to manage and preserve your results.\n",
    "\n",
    "You've put foundational MLOps principles into practice with modern deep learning tools. By using **PyTorch Lightning**, you've seen how to organize your code into clean, reusable components (`LightningModule` and `LightningDataModule`) and how to extend functionality with custom **Callbacks**. This approach dramatically simplifies your training code while maintaining full flexibility.\n",
    "\n",
    "You implemented **checkpointing** to not only save your final model but also the best-performing version during training, complete with its optimizer state. By logging this checkpoint and other **artifacts** like the confusion matrix through your custom MLflow callback, you've moved past the chaos of managing countless model files with confusing names.\n",
    "\n",
    "By combining **Lightning** with **MLflow**, you transformed your training process from a temporary session into a series of well-documented, reproducible experiments. Every hyperparameter, metric, and model file is now neatly organized and ready for comparison. The skills you've developed here in modern deep learning frameworks, serialization, and experiment tracking are fundamental for preparing any model for real-world deployment. You are now well-equipped to build more reliable and structured machine learning systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
