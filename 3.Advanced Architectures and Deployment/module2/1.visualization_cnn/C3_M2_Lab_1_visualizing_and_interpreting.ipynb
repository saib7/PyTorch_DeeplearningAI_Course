{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "l0gq8moplxx4",
    "outputId": "8305bbb0-c700-4e3d-825f-71702e89ee67"
   },
   "source": [
    "# Visualizing and Interpreting Convolutional Neural Networks\n",
    "\n",
    "In this notebook, we will explore how Convolutional Neural Networks (CNNs) work under the hood by visualizing their internal layers and feature maps. Understanding the inner workings of CNNs is crucial for developing intuition about how these models interpret images. Throughout this session, you will learn how to develop custom functions and hooks to peek inside the network and see what features it detects at different layers.\n",
    "\n",
    "**What you'll do in this notebook:**\n",
    "- Develop functions to hook into the CNN layers and extract intermediate outputs.\n",
    "- Visualize feature maps and understand how the network processes images step-by-step.\n",
    "- Compute and interpret the receptive field of neurons in various layers.\n",
    "- Gain practical insights into how CNNs capture local and global image features.\n",
    "- Experiment with different layers to see the progression from simple edges to complex objects.\n",
    "\n",
    "Let's get started and deepen our understanding of CNNs through visualization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Introduction \n",
    "\n",
    "### 1.1 Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as tv_models\n",
    "from torchvision import transforms\n",
    "\n",
    "import helper_utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 How Different Layers Change Images\n",
    "\n",
    "In this section, you will observe how each layer in a CNN transforms and interprets images. By visualizing feature maps at various depths, you can see how the network first detects basic features like edges and textures. As you delve deeper, it recognizes more complex patterns, textures, and object parts. This process demonstrates how CNNs learn and progressively build a detailed understanding of the image, step-by-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a ball image to illustrate\n",
    "ball_image = helper_utils.get_ball()\n",
    "ball_tensor = torch.tensor(ball_image, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.plot_image(ball_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Visualizing a Convolutional Layer\n",
    "\n",
    "To understand what a convolutional layer detects, you should visualize its feature maps or activation outputs. When an image is processed through this layer, it is transformed into a set of feature maps that highlight various features such as edges, textures, or shapes in the input. By visualizing these feature maps, you can see which regions of the input activate specific filters, providing insight into how the network interprets and extracts crucial information from the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_channels=16\n",
    "kernel_size=3 \n",
    "stride=1\n",
    "padding=1\n",
    "\n",
    "# Define a 2D convolutional layer with specified parameters.\n",
    "conv_layer = nn.Conv2d(in_channels=3, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "# Apply the convolutional layer to the input tensor.\n",
    "output_conv_layer = conv_layer(ball_tensor)\n",
    "\n",
    "# Print the shape of the tensor before and after convolution for comparison.\n",
    "print(f\"Input tensor shape: {ball_tensor.shape}\")\n",
    "print(f\"Output tensor shape after convolution: {output_conv_layer.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deeper network layers have larger receptive fields and smaller feature maps, balancing local details with global context for effective pattern interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each output channel (filter)\n",
    "for i in range(out_channels):\n",
    "    # Determine the grid size based on total number of filters\n",
    "    grid_size = int(np.ceil(np.sqrt(out_channels)))\n",
    "    # Add a subplot in a grid layout for each filter\n",
    "    plt.subplot(grid_size, grid_size, i + 1)\n",
    "    # Detach the tensor from the computation graph, convert to numpy array for visualization\n",
    "    plt.imshow(output_conv_layer[i].detach().numpy(), cmap='gray')\n",
    "    # Remove axis for a cleaner look\n",
    "    plt.axis('off')\n",
    "    # Set the title for each filter with proper formatting\n",
    "    plt.title(f'Filter {i+1}', fontsize=10, pad=10)  \n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()  \n",
    "# Display the plot with all filters\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Pooling Layer\n",
    "In this section, we will explore how a pooling layer behaves on different layers of the network, focusing on its effect in reducing the spatial dimensions of feature maps.\n",
    "\n",
    "#### **Mathematical Definition of Pooling**\n",
    "Pooling is a downsampling operation that reduces the spatial dimensions of feature maps. For a 2D pooling operation with kernel size $k \\times k$ and stride $s$:\n",
    "\n",
    "**Max Pooling:**\n",
    "$$y_{i,j} = \\max_{m,n \\in R_{i,j}} x_{m,n}$$\n",
    "\n",
    "**Average Pooling:**\n",
    "$$y_{i,j} = \\frac{1}{k^2} \\sum_{m,n \\in R_{i,j}} x_{m,n}$$\n",
    "\n",
    "where $R_{i,j}$ is the pooling region (receptive field) for output position $(i,j)$, defined as:\n",
    "$$R_{i,j} = \\{(m,n) : i \\cdot s \\leq m < i \\cdot s + k, \\quad j \\cdot s \\leq n < j \\cdot s + k\\}$$\n",
    "\n",
    "**Output dimensions** after pooling:\n",
    "$$H_{out} = \\left\\lfloor \\frac{H_{in} - k}{s} \\right\\rfloor + 1$$\n",
    "$$W_{out} = \\left\\lfloor \\frac{W_{in} - k}{s} \\right\\rfloor + 1$$\n",
    "\n",
    "where $H_{in}, W_{in}$ are input height and width, and $\\lfloor \\cdot \\rfloor$ denotes the floor function.\n",
    "\n",
    "#### **Example: 2×2 Pooling on a 4×4 Matrix**\n",
    "Let's apply 2×2 pooling with stride=2 to a 4×4 input matrix.\n",
    "\n",
    "**Input Matrix (4×4):**\n",
    "$$X = \\left[\\begin{array}{cccc}\n",
    "1 & 3 & 2 & 4 \\\\\n",
    "5 & 6 & 7 & 8 \\\\\n",
    "9 & 2 & 3 & 1 \\\\\n",
    "4 & 5 & 6 & 2\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "**Output Dimensions:**\n",
    "- $H_{out} = \\lfloor \\frac{4 - 2}{2} \\rfloor + 1 = 1 + 1 = 2$\n",
    "- $W_{out} = \\lfloor \\frac{4 - 2}{2} \\rfloor + 1 = 1 + 1 = 2$\n",
    "- Output will be 2×2\n",
    "\n",
    "**Max Pooling Computations:**\n",
    "\n",
    "Position (0,0): Pool region = rows [0:2], cols [0:2]\n",
    "$$\\left[\\begin{array}{cc}\n",
    "1 & 3 \\\\\n",
    "5 & 6\n",
    "\\end{array}\\right] \\rightarrow \\max(1, 3, 5, 6) = 6$$\n",
    "\n",
    "Position (0,1): Pool region = rows [0:2], cols [2:4]\n",
    "$$\\left[\\begin{array}{cc}\n",
    "2 & 4 \\\\\n",
    "7 & 8\n",
    "\\end{array}\\right] \\rightarrow \\max(2, 4, 7, 8) = 8$$\n",
    "\n",
    "Position (1,0): Pool region = rows [2:4], cols [0:2]\n",
    "$$\\left[\\begin{array}{cc}\n",
    "9 & 2 \\\\\n",
    "4 & 5\n",
    "\\end{array}\\right] \\rightarrow \\max(9, 2, 4, 5) = 9$$\n",
    "\n",
    "Position (1,1): Pool region = rows [2:4], cols [2:4]\n",
    "$$\\left[\\begin{array}{cc}\n",
    "3 & 1 \\\\\n",
    "6 & 2\n",
    "\\end{array}\\right] \\rightarrow \\max(3, 1, 6, 2) = 6$$\n",
    "\n",
    "**Max Pooling Output (2×2):**\n",
    "$$Y_{max} = \\left[\\begin{array}{cc}\n",
    "6 & 8 \\\\\n",
    "9 & 6\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "**Average Pooling Computations:**\n",
    "\n",
    "Position (0,0): Pool region = rows [0:2], cols [0:2]\n",
    "$$\\left[\\begin{array}{cc}\n",
    "1 & 3 \\\\\n",
    "5 & 6\n",
    "\\end{array}\\right] \\rightarrow \\text{avg}(1, 3, 5, 6) = \\frac{15}{4} = 3.75$$\n",
    "\n",
    "Position (0,1): Pool region = rows [0:2], cols [2:4]\n",
    "$$\\left[\\begin{array}{cc}\n",
    "2 & 4 \\\\\n",
    "7 & 8\n",
    "\\end{array}\\right] \\rightarrow \\text{avg}(2, 4, 7, 8) = \\frac{21}{4} = 5.25$$\n",
    "\n",
    "Position (1,0): Pool region = rows [2:4], cols [0:2]\n",
    "$$\\left[\\begin{array}{cc}\n",
    "9 & 2 \\\\\n",
    "4 & 5\n",
    "\\end{array}\\right] \\rightarrow \\text{avg}(9, 2, 4, 5) = \\frac{20}{4} = 5.00$$\n",
    "\n",
    "Position (1,1): Pool region = rows [2:4], cols [2:4]\n",
    "$$\\left[\\begin{array}{cc}\n",
    "3 & 1 \\\\\n",
    "6 & 2\n",
    "\\end{array}\\right] \\rightarrow \\text{avg}(3, 1, 6, 2) = \\frac{12}{4} = 3.00$$\n",
    "\n",
    "**Average Pooling Output (2×2):**\n",
    "$$Y_{avg} = \\left[\\begin{array}{cc}\n",
    "3.75 & 5.25 \\\\\n",
    "5.00 & 3.00\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "Notice how pooling reduces the spatial dimensions from 4×4 to 2×2, while preserving important features (maximum values) or computing representative values (averages) from each region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters for the max pooling layer\n",
    "pool_kernel_size = 2  # Size of the pooling window\n",
    "pool_stride = 2       # Stride of the pooling window\n",
    "\n",
    "# Create a MaxPool2d layer with specified kernel size and stride\n",
    "max_pool_layer = nn.MaxPool2d(kernel_size=pool_kernel_size, stride=pool_stride)\n",
    "\n",
    "# Apply the max pooling layer to the input tensor\n",
    "pooled_tensor = max_pool_layer(ball_tensor)\n",
    "\n",
    "# Print the tensor shape before and after pooling to observe the dimensionality reduction\n",
    "print(f\"Shape of input tensor before pooling: {ball_tensor.shape}\")\n",
    "print(f\"Shape of tensor after pooling: {pooled_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the original and pooled images side by side\n",
    "plt.figure(figsize=(12, 6))  # Set figure size for clarity\n",
    "\n",
    "# Plot original image\n",
    "plt.subplot(1, 2, 1)\n",
    "helper_utils.plot_image(ball_tensor, title='Original', aspect='equal')\n",
    "\n",
    "# Plot pooled image\n",
    "plt.subplot(1, 2, 2)\n",
    "helper_utils.plot_image(pooled_tensor, title='Pooled', aspect='equal')\n",
    "\n",
    "# Adjust layout to prevent overlap and display the figure\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Building and Understanding a Basic 3-Layer CNN\n",
    "In this section, we will construct a simple convolutional neural network consisting of three layers. This foundational architecture provides a clear view of fundamental deep learning operations such as convolution, activation functions, and pooling.\n",
    "\n",
    "#### **Mathematical Definition of 2D Convolution**\n",
    "The 2D convolution operation is the core building block of CNNs. For an input feature map $X$ and a kernel (filter) $K$ of size $k \\times k$, the convolution operation is defined as:\n",
    "\n",
    "$$Y_{i,j} = \\sum_{m=0}^{k-1} \\sum_{n=0}^{k-1} K_{m,n} \\cdot X_{i+m, j+n} + b$$\n",
    "\n",
    "where:\n",
    "- $Y_{i,j}$ is the output at position $(i,j)$\n",
    "- $K_{m,n}$ is the kernel weight at position $(m,n)$\n",
    "- $X_{i+m, j+n}$ is the input value at position $(i+m, j+n)$\n",
    "- $b$ is the bias term\n",
    "\n",
    "**With stride $s$ and padding $p$**, the output dimensions are:\n",
    "$$H_{out} = \\left\\lfloor \\frac{H_{in} + 2p - k}{s} \\right\\rfloor + 1$$\n",
    "$$W_{out} = \\left\\lfloor \\frac{W_{in} + 2p - k}{s} \\right\\rfloor + 1$$\n",
    "\n",
    "#### **Example: 3×3 Convolution with 2×2 Kernel**\n",
    "Let's apply a 2×2 kernel to a 3×3 input matrix (stride=1, no padding, bias=0).\n",
    "\n",
    "**Input Matrix X (3×3):**\n",
    "$$X = \\left[\\begin{array}{ccc}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "**Kernel K (2×2):**\n",
    "$$K = \\left[\\begin{array}{cc}\n",
    "1 & 0 \\\\\n",
    "0 & -1\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "This kernel acts as an edge detector, computing the difference between diagonal elements.\n",
    "\n",
    "**Output Dimensions:**\n",
    "- $H_{out} = \\lfloor \\frac{3 + 2(0) - 2}{1} \\rfloor + 1 = 1 + 1 = 2$\n",
    "- $W_{out} = \\lfloor \\frac{3 + 2(0) - 2}{1} \\rfloor + 1 = 1 + 1 = 2$\n",
    "- Output will be 2×2\n",
    "\n",
    "**Convolution Computations:**\n",
    "\n",
    "Position (0,0): Apply kernel to top-left 2×2 region\n",
    "$$\\left[\\begin{array}{cc}\n",
    "1 & 2 \\\\\n",
    "4 & 5\n",
    "\\end{array}\\right] \\odot \\left[\\begin{array}{cc}\n",
    "1 & 0 \\\\\n",
    "0 & -1\n",
    "\\end{array}\\right] = (1 \\times 1) + (2 \\times 0) + (4 \\times 0) + (5 \\times -1) = 1 - 5 = -4$$\n",
    "\n",
    "Position (0,1): Apply kernel to top-right 2×2 region\n",
    "$$\\left[\\begin{array}{cc}\n",
    "2 & 3 \\\\\n",
    "5 & 6\n",
    "\\end{array}\\right] \\odot \\left[\\begin{array}{cc}\n",
    "1 & 0 \\\\\n",
    "0 & -1\n",
    "\\end{array}\\right] = (2 \\times 1) + (3 \\times 0) + (5 \\times 0) + (6 \\times -1) = 2 - 6 = -4$$\n",
    "\n",
    "Position (1,0): Apply kernel to bottom-left 2×2 region\n",
    "$$\\left[\\begin{array}{cc}\n",
    "4 & 5 \\\\\n",
    "7 & 8\n",
    "\\end{array}\\right] \\odot \\left[\\begin{array}{cc}\n",
    "1 & 0 \\\\\n",
    "0 & -1\n",
    "\\end{array}\\right] = (4 \\times 1) + (5 \\times 0) + (7 \\times 0) + (8 \\times -1) = 4 - 8 = -4$$\n",
    "\n",
    "Position (1,1): Apply kernel to bottom-right 2×2 region\n",
    "$$\\left[\\begin{array}{cc}\n",
    "5 & 6 \\\\\n",
    "8 & 9\n",
    "\\end{array}\\right] \\odot \\left[\\begin{array}{cc}\n",
    "1 & 0 \\\\\n",
    "0 & -1\n",
    "\\end{array}\\right] = (5 \\times 1) + (6 \\times 0) + (8 \\times 0) + (9 \\times -1) = 5 - 9 = -4$$\n",
    "\n",
    "**Convolution Output Y (2×2):**\n",
    "$$Y = \\left[\\begin{array}{cc}\n",
    "-4 & -4 \\\\\n",
    "-4 & -4\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "The uniform output of -4 indicates that the difference between the top-left and bottom-right corners is constant across all 2×2 regions in our input, which makes sense given the regular pattern in our input matrix (consecutive integers).\n",
    "\n",
    "#### **Example with Different Kernel: Vertical Edge Detection**\n",
    "Let's try a different kernel that detects vertical edges:\n",
    "\n",
    "**Kernel K (2×2):**\n",
    "$$K = \\left[\\begin{array}{cc}\n",
    "1 & -1 \\\\\n",
    "1 & -1\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "**Convolution Computations:**\n",
    "\n",
    "Position (0,0):\n",
    "$$\\left[\\begin{array}{cc}\n",
    "1 & 2 \\\\\n",
    "4 & 5\n",
    "\\end{array}\\right] \\odot \\left[\\begin{array}{cc}\n",
    "1 & -1 \\\\\n",
    "1 & -1\n",
    "\\end{array}\\right] = (1 \\times 1) + (2 \\times -1) + (4 \\times 1) + (5 \\times -1) = 1 - 2 + 4 - 5 = -2$$\n",
    "\n",
    "Position (0,1):\n",
    "$$\\left[\\begin{array}{cc}\n",
    "2 & 3 \\\\\n",
    "5 & 6\n",
    "\\end{array}\\right] \\odot \\left[\\begin{array}{cc}\n",
    "1 & -1 \\\\\n",
    "1 & -1\n",
    "\\end{array}\\right] = (2 \\times 1) + (3 \\times -1) + (5 \\times 1) + (6 \\times -1) = 2 - 3 + 5 - 6 = -2$$\n",
    "\n",
    "Position (1,0):\n",
    "$$\\left[\\begin{array}{cc}\n",
    "4 & 5 \\\\\n",
    "7 & 8\n",
    "\\end{array}\\right] \\odot \\left[\\begin{array}{cc}\n",
    "1 & -1 \\\\\n",
    "1 & -1\n",
    "\\end{array}\\right] = (4 \\times 1) + (5 \\times -1) + (7 \\times 1) + (8 \\times -1) = 4 - 5 + 7 - 8 = -2$$\n",
    "\n",
    "Position (1,1):\n",
    "$$\\left[\\begin{array}{cc}\n",
    "5 & 6 \\\\\n",
    "8 & 9\n",
    "\\end{array}\\right] \\odot \\left[\\begin{array}{cc}\n",
    "1 & -1 \\\\\n",
    "1 & -1\n",
    "\\end{array}\\right] = (5 \\times 1) + (6 \\times -1) + (8 \\times 1) + (9 \\times -1) = 5 - 6 + 8 - 9 = -2$$\n",
    "\n",
    "**Output Y (2×2):**\n",
    "$$Y = \\left[\\begin{array}{cc}\n",
    "-2 & -2 \\\\\n",
    "-2 & -2\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "This kernel detects the consistent horizontal gradient in our input matrix.\n",
    "\n",
    "**For multiple channels**, if the input has $C_{in}$ channels and we want $C_{out}$ output channels:\n",
    "$$Y_{c_{out}, i, j} = \\sum_{c_{in}=0}^{C_{in}-1} \\sum_{m=0}^{k-1} \\sum_{n=0}^{k-1} K_{c_{out}, c_{in}, m, n} \\cdot X_{c_{in}, i+m, j+n} + b_{c_{out}}$$\n",
    "\n",
    "This shows how each output channel is computed by convolving across all input channels with different kernels and summing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),  # 3 input channels, 16 filters\n",
    "                nn.ReLU(),  # ReLU activation adds non-linearity\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)  # Reduces spatial dimensions by 2\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),  # 16 input channels, 32 filters\n",
    "                nn.ReLU(),  # ReLU activation\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)  # Pooling reduces size by half\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),  # 32 input channels, 64 filters\n",
    "                nn.ReLU(),  # Activation\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)  # Further size reduction\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Hook Function\n",
    "\n",
    "In this section, we introduce the concept of hook functions in PyTorch. Hooks are functions that you can register to a layer in a neural network to extract, modify, or monitor its outputs and internal states during the forward and backward passes. They are powerful tools for visualizing what the network learns and for debugging. Here, we focus on forward hooks, which allow us to capture intermediate outputs at desired layers of the network, making it possible to visualize feature maps and understand how data flows through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the captured feature maps\n",
    "activations = {}\n",
    "\n",
    "# Hook function to intercept and store intermediate activations\n",
    "# 'name' is used as the key in the 'activations' dictionary\n",
    "# This function creates a closure capturing 'name' for later use\n",
    "\n",
    "def grab(name):\n",
    "    def hook(model, input, output):\n",
    "        # Detach the output tensor from the computation graph\n",
    "        # This prevents gradients from accumulating and saves memory\n",
    "        # Store this tensor in the 'activations' dictionary with 'name' as the key\n",
    "        activations[name] = output.detach()\n",
    "    return hook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ThreeLayerCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1 Registering Forward Hooks\n",
    "\n",
    "In this step, you'll register forward hooks on the layers of your neural network that you're interested in examining. Forward hooks allow you to capture the output of a layer during the forward pass, which is essential for visualizing feature maps or collecting intermediate activations.\n",
    "\n",
    "To do this, you call the `register_forward_hook()` method on a specific layer, passing a hook function that will be executed every time the layer performs a forward pass. This hook function receives the layer, its input, and its output, enabling you to save or analyze the output for visualization or debugging purposes.\n",
    "\n",
    "Make sure to register these hooks **before running your data through the model** to capture the desired intermediate representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register forward hooks on specific layers to capture activations during a forward pass\n",
    "# This helps us visualize what features each layer detects\n",
    "model.layers[0].register_forward_hook(grab('layer1'))  # Hook for layer1\n",
    "model.layers[1].register_forward_hook(grab('layer2'))  # Hook for layer2\n",
    "model.layers[2].register_forward_hook(grab('layer3'))  # Hook for layer3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing the ball tensor to the network\n",
    "output = model(ball_tensor.unsqueeze(0))\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the shapes after each layer\n",
    "for layer, output_tensor in activations.items():\n",
    "    print(f\"Output shape after {layer}: {output_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.visualize_all_layers_grids(activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Receptive Field in Convolutional Neural Networks\n",
    "### What is the *Receptive Field*?\n",
    "In a convolutional neural network (CNN), the **receptive field** of a particular neuron (unit) in a feature map is the specific region of the input image that influences the value of that neuron. It tells us how much \"context\" (how large an area) from the input a unit at a certain layer can \"see\" or is affected by.\n",
    "- **Small receptive field:** The unit \"sees\" a small part of the input.\n",
    "- **Large receptive field:** The unit \"sees\" a larger part of the input and so can incorporate more global image information.\n",
    "### Why is the receptive field important?\n",
    "- It helps us understand **how much of the input image** each feature at a given layer is using.\n",
    "- Larger receptive fields allow the network to capture more complex, global patterns.\n",
    "- If the receptive field is too small, the network may miss important global information.\n",
    "### How is the receptive field computed?\n",
    "We can **recursively calculate** the receptive field as we stack layers in a CNN:\n",
    "#### For a single convolution or pooling layer:\n",
    "- The **receptive field increases** with the kernel size — each neuron looks at a window of the previous layer.\n",
    "- If a layer uses *stride* or *dilation*, these also affect how much the receptive field grows.\n",
    "\n",
    "#### **Key Terms:**\n",
    "- **Kernel:** The filter window (e.g., 3×3) that slides over the input to extract features.\n",
    "- **Stride:** The step size by which the kernel moves across the input (stride=2 means skip every other position).\n",
    "- **Dilation:** The spacing between kernel elements (dilation=2 means insert gaps between kernel values).\n",
    "\n",
    "#### **Recursive Calculation Rule:**\n",
    "Suppose you have a layer with:\n",
    "- kernel size $ k $\n",
    "- stride $ s $\n",
    "- (optional) dilation $ d $ (usually 1 if not specified)\n",
    "- previous layer's receptive field $ r_{prev} $\n",
    "- previous layer's jump/stride $ j_{prev} $\n",
    "\n",
    "Then:\n",
    "$$\n",
    "r_{new} = r_{prev} + (k - 1) \\times j_{prev} \\times d\n",
    "$$\n",
    "$$\n",
    "j_{new} = j_{prev} \\times s\n",
    "$$\n",
    "- **Receptive field** increases each time by the size and arrangement of the kernel.\n",
    "- **Jump** is the effective stride between two neighboring units in the current feature map, measured in the original input image.\n",
    "\n",
    "#### **Example Calculation**\n",
    "Imagine an input image with size 224x224. \n",
    "Suppose our network looks like this:\n",
    "| Layer type        | Kernel | Stride | Padding |\n",
    "|-------------------|--------|--------|---------|\n",
    "| Input             | N/A    | N/A    | N/A     |\n",
    "| Conv2d            | 3      | 1      | 1       |\n",
    "| MaxPool2d         | 2      | 2      | 0       |\n",
    "- Start with \\( r = 1 \\), \\( j = 1 \\) (every input pixel \"sees\" itself).\n",
    "- After **conv layer** (\\( k=3, s=1 \\)):  \n",
    "  \\( r = 1 + (3-1)*1 = 3 \\), \\( j = 1*1 = 1 \\)\n",
    "- After **pool layer** (\\( k=2, s=2 \\)):  \n",
    "  \\( r = 3 + (2-1)*1 = 4 \\), \\( j = 1*2 = 2 \\)\n",
    "\n",
    "So after these two layers, each neuron in the feature map \"sees\" a 4x4 patch of the original input image, and neighboring neurons correspond to positions 2 pixels apart in the input.\n",
    "### **In summary:**\n",
    "- The **receptive field** measures how much of the input contributes to each feature at each layer.\n",
    "- It **grows gradually** as you stack more layers, especially if you use larger kernels, more pooling, or bigger strides.\n",
    "- Understanding and calculating the receptive field lets you design neural networks that can see the right amount of context in your images.\n",
    "Let's plot it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfinfo = helper_utils.calculate_receptive_field(model, input_size=224)\n",
    "helper_utils.plot_receptive_field_summary(rfinfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - A Practical Example with ResNet\n",
    "\n",
    "This section demonstrates how ResNet processes an image internally. We'll explore how an image is transformed as it passes through various layers of the network and examine the Receptive Field graph to understand the context each neuron considers. This hands-on example provides insight into ResNet's architecture and operation, highlighting the flow of information from input to output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load pre-trained ResNet50 from local cache\nimport os\ntorch.hub.set_dir(os.path.join(os.getcwd(), 'pretrained_model'))\nmodel = tv_models.resnet50(weights=tv_models.ResNet50_Weights.IMAGENET1K_V2)\nmodel.eval()\nprint(\"\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(tv_models.resnet50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Plotting the Receptive Field Map\n",
    "\n",
    "This section visualizes the receptive field of different neurons across the network layers, demonstrating how much of the input image each neuron can 'see'. Plotting this map helps in understanding the spatial extent of the features captured by the model at various depths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfinfo = helper_utils.calculate_receptive_field(model, input_size=224)\n",
    "helper_utils.plot_receptive_field_summary(rfinfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Analyzing Selected Layers of ResNet\n",
    "\n",
    "In this section, you will examine how an image is progressively transformed as it passes through the layers of the ResNet model. We'll focus on understanding how the features evolve from the initial input up to the final prediction. Given that ResNet consists of many layers, we will select a few representative ones for detailed analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Applying Forward Hooks to ResNet Layers\n",
    "\n",
    "In this section, you will apply forward hooks to specific layers within the ResNet architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset activations dictionary\n",
    "activations = {}\n",
    "# To register a forward hook, you need to call the following method on each layer you want to register\n",
    "model.conv1.register_forward_hook(grab('conv1'))           # First layer\n",
    "model.layer1[0].conv1.register_forward_hook(grab('layer1'))  # Early block\n",
    "model.layer2[0].conv1.register_forward_hook(grab('layer2'))  # Middle block\n",
    "model.layer3[0].conv1.register_forward_hook(grab('layer3'))  # Later block\n",
    "model.layer4[0].conv1.register_forward_hook(grab('layer4'))  # Deep block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Preprocessing Pipeline\n",
    "\n",
    "Before feeding images into ResNet, it is essential to preprocess them to match the training conditions. Typically, this involves resizing, cropping, normalization, and tensor conversion. The standard steps are:\n",
    "\n",
    "1. **Resize:** Adjust the image so that the shorter side is 256 pixels.\n",
    "2. **Center Crop:** Crop a 224x224 pixel region from the center of the resized image.\n",
    "3. **To Tensor:** Convert the PIL image into a PyTorch tensor.\n",
    "4. **Normalize:** Adjust the pixel values using mean and standard deviation values that match the ImageNet dataset (mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225]) to ensure consistency with the model's training conditions.\n",
    "\n",
    "This pipeline ensures that the input images are properly scaled and normalized, which is crucial for the model to perform accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess an image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load a sample image (let's say we have a cat image)\n",
    "image = Image.open('images/cat.jpg')\n",
    "input_tensor = transform(image).unsqueeze(0)  # Add batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass through the model\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "\n",
    "# Visualize the input image and feature maps\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Display original image\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.imshow(image)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Exploring Feature Maps and Receptive Fields in ResNet\n",
    "\n",
    "In this part, you will look inside ResNet by seeing its feature maps and understanding how neurons respond in different layers. Looking at these helps you understand how ResNet processes and makes sense of complicated pictures. It shows what kinds of features the network detects at each level. By studying feature maps and receptive fields, you can learn how ResNet builds up a detailed understanding of visual information in steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.visualize_all_layers_grids(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.plot_rfinfo_over_image(rfinfo, 'images/cat.jpg', input_size=224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Interactive Visualization of Image Transformations in ResNet\n",
    "\n",
    "This widget enables you to see how an input image is transformed at each layer of the ResNet model. You can navigate through the layers to observe feature extraction and representation refinement, culminating in the final classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.plot_widget(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Conclusion\n",
    "\n",
    "In this notebook, you explored the inner workings of CNNs using various visualization techniques. You learned how to examine what each layer detects, discovering that early layers focus on simple features like edges and textures, while deeper layers recognize complex objects and patterns.\n",
    "\n",
    "Understanding the receptive field helped you grasp how much of the input image each neuron can 'see' at different stages, revealing the hierarchical process of feature extraction.\n",
    "\n",
    "Analyzing advanced models like ResNet showed how deep neural networks build a comprehensive understanding of visual data layer by layer.\n",
    "\n",
    "These visualization tools are crucial for debugging, interpreting, and improving neural networks. They assist you in developing more accurate and explainable models that better understand the visual world."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}