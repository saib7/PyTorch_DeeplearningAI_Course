{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40cfd8a8-a0ce-4bc6-8351-40762703307a",
   "metadata": {},
   "source": "# Saliency and Class Activation Map Visualization using PyTorch & ResNet\n\nDeep neural networks are powerful, but they can often behave like \"black boxes\":  \nYou input an image and get a prediction—but how does the model decide what it sees?\n\n**Saliency maps** and **Grad-CAM (Gradient-weighted Class Activation Mapping)** are two powerful visualization techniques that help you \"peek inside\" a model's decision process. \nThey allow you to see which areas or pixels in an image matter most to the network for a particular prediction.\n\nIn this notebook, you will:\n\n- Load and preprocess images for a pre-trained ResNet model.\n- Perform predictions and retrieve class probabilities.\n- Compute **saliency maps** by backpropagating gradients from the target class all the way to the input pixels.\n- Compute **class activation (\"attention\") maps** (GradCAM) to see what spatial regions influenced the model's choice.\n- Overlay these visualizations on top of the original images to interpret the model's focus.\n- Understand the intuition and math behind each visualization, with step-by-step code explanations.\n\nLet's get started!"
  },
  {
   "cell_type": "markdown",
   "id": "acd14611-1fc5-45df-bbb2-bbebf7289d66",
   "metadata": {},
   "source": "## 1 - Importing the libraries"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cfd4cd-dea1-4953-bfe7-5477848aad20",
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nimport cv2\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\n\nfrom skimage.transform import resize\nimport torch\nimport torch.nn as nn\nimport torchvision.models as tv_models\nimport torchvision.transforms as transforms\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)"
  },
  {
   "cell_type": "markdown",
   "id": "226f845e-13cf-4c2b-94d5-7b5e3f453726",
   "metadata": {},
   "source": "## 2 - Loading the Pretrained ResNet50 and Class Labels\n\n### 2.1 Load the Pretrained ResNet50 Model\n\nYou will use ResNet50, a powerful convolutional neural network trained on the [ImageNet](https://www.image-net.org/) dataset (1000 classes).\n\nSaliency maps can be computed for any image classifier, but ImageNet models are popular for demos.\n\n*You should also fetch the class label names for making your outputs human-readable.*"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af47c254-2fd3-457a-a326-6c98bda8d644",
   "metadata": {},
   "outputs": [],
   "source": "# Load pretrained ResNet50 model and class labels from local cache\nimport os\ntorch.hub.set_dir(os.path.join(os.getcwd(), 'pretrained_model'))\nmodel = tv_models.resnet50(weights=tv_models.ResNet50_Weights.IMAGENET1K_V1)\nmodel.eval()\nmodel.to(device)\n\n# Class names\nimagenet_class_mapping = tv_models.ResNet50_Weights.IMAGENET1K_V1.meta[\"categories\"]\nprint(f\"Loaded {len(imagenet_class_mapping)} classes.\")"
  },
  {
   "cell_type": "markdown",
   "id": "c5827bc0-cf86-4782-ae0c-288dacdb3ffd",
   "metadata": {},
   "source": "### 2.2 Image Preprocessing\n\nResNet50 expects:\n\n- Images as RGB, shape (224, 224)\n- Pixel normalization using ImageNet statistics:  \n  Mean = [0.485, 0.456, 0.406], Std = [0.229, 0.224, 0.225]\n\nHere's how you process an image from file to tensor:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99804793-1040-46aa-8e65-ded83bac9765",
   "metadata": {},
   "outputs": [],
   "source": "# Image preprocessing transform\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\ndef load_image(img_path):\n    img = Image.open(img_path)\n    if img.mode != 'RGB':\n        img = img.convert('RGB')\n    return img"
  },
  {
   "cell_type": "markdown",
   "id": "9fca413d-ac49-4a66-849d-d4a40b8062da",
   "metadata": {},
   "source": "### 2.3 Prediction and Visualizing Inputs\n\n### Load an Image and Make a Prediction\n\nLet's load an image, transform it, and get the model's top prediction.\n\nYou can use your own images or try the examples below (place them in an `images/` folder):"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c775bfc-b601-4cec-8a45-85ad07a56e07",
   "metadata": {},
   "outputs": [],
   "source": "img_path = 'images/dog.jpg'  # Change to your own image path\n\nif not os.path.exists(img_path):\n    raise FileNotFoundError(\"Please provide a valid path to an image.\")\n\nimg_pil = load_image(img_path)\nimg_tensor = transform(img_pil).unsqueeze(0).to(device)\n\n# Forward pass\nwith torch.no_grad():\n    logits = model(img_tensor)\n    probs = torch.softmax(logits, dim=1)\n    pred_prob, pred_class = torch.max(probs, dim=1)\n    pred_label = imagenet_class_mapping[pred_class]\nprint(f\"Predicted class: {pred_label} (prob: {pred_prob.item():.3f})\")\n\n# Show the image\nplt.imshow(np.array(img_pil))\nplt.title(f\"Original Image\\nPredicted: {pred_label}\")\nplt.axis('off')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "76bb6c03-f0cc-43bd-a940-87a31c46a7d8",
   "metadata": {},
   "source": "## 3 - Saliency Maps \n\n### 3.1 A bit of theory\n\nWhen you use a deep neural network to classify images, it is often helpful to \"see what the model is looking at.\"  \nA **saliency map** helps you visualize which parts of an image are most important for your model's decision.  \n- Think of it as a heatmap, where \"hot\" pixels strongly influence the prediction.\n\nHow does this work?\n- Think about how your model would \"feel\" a change in each pixel: \"If I change this pixel, does my prediction go up or down?\"\n- The **saliency map** answers this: it measures, for each input pixel, how much changing that pixel would change the score for the target class.\n\n<details>\n<summary><strong>Show mathematical details</strong></summary>\n<br>\n\nFor an input image $x$ and model output $f(x)$,  \nif you are interested in class $c$, you compute:\n\\[\nS_{i, j} = \\left| \\frac{\\partial f_c(x)}{\\partial x_{i, j}} \\right|\n\\]\nWhere:\n- $f_c(x)$: The model's score (before softmax) for class $c$.\n- $x_{i, j}$: Pixel value at position $(i, j)$ of the input image.\n\n**In practice**:  \n- You compute the gradient of the output for class $c$ with respect to each pixel in the input image.\n- You take the absolute value, and (usually) sum across color channels.\n- This gives you a single heatmap that shows which pixels most affect the prediction for class $c$.\n\n</details>"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5162945-8ab9-4b6b-aa67-4d12a4d96ba6",
   "metadata": {},
   "outputs": [],
   "source": "def compute_saliency_map(model, input_image, target_class=None):\n    \"\"\"\n    Compute a saliency map for the input image using gradients of the model's output\n    with respect to the input pixels.\n\n    Args:\n        model: PyTorch model (image classifier)\n        input_image: Input tensor with shape [1, 3, H, W]. This will be cloned for gradient computation.\n        target_class: Target class index. If None, uses the model's top predicted class.\n\n    Returns:\n        saliency_map: Numpy array (H, W). Each value represents the importance of that pixel.\n        pred_class: Integer index of the class used for saliency (either given or predicted).\n        pred_prob: Probability (float) assigned to that class.\n    \"\"\"\n    # Ensure input_image is a leaf tensor (not from any computation graph) and enable gradient tracking\n    input_image = input_image.clone().detach()\n    input_image.requires_grad_()\n\n    # Forward pass: get output logits from model\n    output = model(input_image)  # shape: [1, num_classes]\n\n    # Determine the class to use for saliency: use target_class if given, else model's prediction\n    probs = torch.softmax(output, dim=1)\n    if target_class is None:\n        pred_prob, pred_class = torch.max(probs, dim=1)\n        target_class = pred_class.item()\n        pred_prob = pred_prob.item()\n    else:\n        pred_prob = probs[0, target_class].item()\n        pred_class = target_class\n\n    # Set all model gradients to zero for safety\n    model.zero_grad()\n\n    # Backward pass: compute gradient of output for target class w.r.t. input pixels\n    # output is [1, num_classes], take value for [0, target_class]\n    output[0, target_class].backward()\n\n    # input_image.grad now contains the gradient of the target class score with respect to every input pixel\n    gradients = input_image.grad.data[0]  # gradients shape: [3, H, W]\n\n    # Common practice: take the sum of absolute gradients across the color channels to form a 2D map\n    saliency_map = torch.abs(gradients).sum(dim=0).cpu().numpy()\n\n    # Normalize to [0, 1] for visualization\n    saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min() + 1e-8)\n\n    return saliency_map, pred_class, pred_prob"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0523286d-737b-4196-a267-e67fd7dc665d",
   "metadata": {},
   "outputs": [],
   "source": "saliency_map, pred_class, pred_prob = compute_saliency_map(model, img_tensor)\nprint(\"saliency_map shape:\", saliency_map.shape)\nprint(\"pred_class:\", pred_class)\nprint(\"pred_prob:\", pred_prob)\nprint(\"Class label:\", imagenet_class_mapping[pred_class])"
  },
  {
   "cell_type": "markdown",
   "id": "a25034d8-9103-4fc4-a828-0c2e97868bd7",
   "metadata": {},
   "source": "### 3.2 Compute and Visualize the Saliency Map\n\nYou will now compute the map and overlay it on the original image."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79ddd8a-6d4d-4f13-917c-662a256d28a8",
   "metadata": {},
   "outputs": [],
   "source": "def visualize_saliency(img_display, saliency_map, pred_class, pred_score, title):\n    \"\"\"\n    Display the original image, the (optionally enhanced) saliency map, and an overlay of both.\n    Args:\n        img_display: Original image as a numpy array (H, W, 3), not normalized (uint8, 0-255).\n        saliency_map: Computed saliency array (usually 224x224, floats in [0,1]).\n        pred_class: String or int giving the predicted class or label.\n        pred_score: Float, the prediction confidence.\n        title:   String, title for saving/display.\n    Shows three side-by-side images:\n        1. The original image,\n        2. The stand-alone enhanced saliency heatmap,\n        3. A faded grayscale overlay with the saliency heatmap for interpretability.\n    \"\"\"\n    # --------- 1. Set up a 3-panel matplotlib figure ---------\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n    \n    # --------- 2. Show original image ---------\n    ax1.imshow(img_display)\n    ax1.set_title(f'Original Image: {title}', fontsize=14)\n    ax1.axis('off')\n    \n    # --------- 3. Enhance and resize saliency map ---------\n    # Enhance contrast of saliency map (gamma-correction helps make faint regions more visible)\n    gamma = 0.7\n    saliency_map_enhanced = np.power(saliency_map, gamma)\n    \n    # Resize saliency map to match the display image size\n    h, w = img_display.shape[:2]\n    saliency_map_resized = resize(\n        saliency_map_enhanced, (h, w),\n        order=1, mode='reflect', anti_aliasing=True\n    )\n    \n    # --------- 4. Show stand-alone saliency map ---------\n    saliency_heatmap = ax2.imshow(saliency_map_resized, cmap='inferno')\n    ax2.set_title('Enhanced Saliency Map', fontsize=14)\n    ax2.axis('off')\n    fig.colorbar(saliency_heatmap, ax=ax2, fraction=0.046, pad=0.04)\n    \n    # --------- 5. Prepare overlay visualization ---------\n    # Generate colored heatmap with inferno colormap (RGB only)\n    heatmap = cm.inferno(saliency_map_resized)[..., :3]\n    img_normalized = img_display / 255.0\n    fade_factor = 0.3  # Faded original image\n    img_faded = img_normalized * fade_factor\n    \n    # Convert to grayscale to make the overlay focus on saliency\n    img_gray = np.mean(img_faded, axis=2, keepdims=True)\n    img_gray = np.repeat(img_gray, 3, axis=2)\n    \n    # Alpha channel: strength of the saliency overlay at each pixel\n    alpha = saliency_map_resized[:, :, np.newaxis]\n    saliency_weight = 0.9\n    \n    # Overlay: combine grayscale base with colored heatmap in salient areas\n    overlay = (1 - alpha * saliency_weight) * img_gray + (alpha * saliency_weight) * heatmap\n    overlay = np.clip(overlay, 0, 1)\n    \n    # --------- 6. Show overlay visualization ---------\n    ax3.imshow(overlay)\n    ax3.set_title(f'Saliency Overlay\\nPrediction: {pred_class}\\nConfidence: {pred_score:.2f}', fontsize=14)\n    ax3.axis('off')\n    \n    # --------- 7. Finalize and display ---------\n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "d825de63-ea08-4d82-9ee6-3b3afab9d2cf",
   "metadata": {},
   "source": "Let's put it together:  \n- Load the image\n- Get a prediction\n- Generate the saliency map\n- Visualize everything!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01761cf3-df71-41e0-9b06-65e38a6566f3",
   "metadata": {},
   "outputs": [],
   "source": "# Reload your original (untransformed) image for display\nimg_display = np.array(img_pil)\n\n# Compute saliency map\nsal_map, pred_class_index, pred_prob = compute_saliency_map(model, img_tensor)\n\n# Show results!\nvisualize_saliency(\n    img_display,\n    sal_map,\n    imagenet_class_mapping[pred_class_index],\n    pred_prob,\n    title=os.path.splitext(os.path.basename(img_path))[0]\n)"
  },
  {
   "cell_type": "markdown",
   "id": "cafcb59c-24b4-40d2-a557-5dc79339d96d",
   "metadata": {},
   "source": "## 4 - What is GradCAM?\n\n### 4.1 A bit of theory\n\nGradCAM works by combining your model's **\"attention\" map** (from its deepest convolutional layer) with the **gradient of the output for the target class**.  \n- In other words: You see *where* the model looked when deciding \"this is a cat!\" or \"this is a dog!\".\n\nHere's how the algorithm works, at a high-level:\n1. The image is passed through the CNN; activations (feature maps) from the last convolutional layer are saved.\n2. The gradient of the class score (for the class you care about) is computed with respect to these feature maps.\n3. These gradients are averaged (global average pooling) to get \"importance weights\" for each channel.\n4. The feature maps are weighted by their importance and summed.\n5. The map is passed through a ReLU (so only positive influences remain).\n6. The heatmap is upsampled and overlaid on the input image for interpretation.\n\n<details>\n<summary><strong>Details & Formula (click to expand)</strong></summary>\n\nIf $A^k$ is the $k$-th feature map in the last conv layer, and $y^c$ is the output for class $c$,\n\n- Importance weight:\n  $$\\quad \\alpha_k^c = \\frac{1}{Z} \\sum_i\\sum_j \\frac{\\partial y^c}{\\partial A^k_{ij}}$$\n- GradCAM:\n $$\\quad L^{c}_{\\text{Grad-CAM}} = \\mathrm{ReLU}\\left(\\sum_k \\alpha_k^c A^k\\right)$$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c431acd6-8fbf-4aee-a412-68d5dd956268",
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\n\nclass GradCAM:\n    \"\"\"\n    Minimal, robust Grad-CAM.\n    - target_layer: a conv module whose activations you want (e.g., resnet.layer4[-1].conv3)\n    - Call the instance with an input tensor [1,3,H,W]; optionally pass class_idx.\n    - Use as a context manager to auto-remove hooks.\n    \"\"\"\n    def __init__(self, model: nn.Module, target_layer: nn.Module):\n        self.model = model\n        self.target_layer = target_layer\n        self.activations = None   # [N,C,H',W']\n        self.gradients = None     # [N,C,H',W']\n        self.target_layer.register_forward_hook(self._on_forward)\n\n    def _on_forward(self, module, inputs, output):\n        # Save activations; attach a grad hook to this tensor to capture gradients on backward\n        self.activations = output.detach()\n        def _on_backward(grad):\n            self.gradients = grad.detach()\n        output.register_hook(_on_backward)\n\n    def __call__(self, x: torch.Tensor, class_idx: int | None = None):\n        \"\"\"\n        Returns:\n          cam_np: np.ndarray of shape [H', W'] in [0,1]\n          class_idx: int used for Grad-CAM\n        \"\"\"\n        self.model.zero_grad(set_to_none=True)\n        output = self.model(x)  # logits [1, num_classes]\n        if class_idx is None:\n            class_idx = int(output.argmax(dim=1).item())\n\n        # Backprop only for the chosen class (no retain_graph needed)\n        score = output[:, class_idx].sum()\n        score.backward()\n\n        # weights: channel-wise mean of gradients\n        # activations/gradients are [1, C, H', W']; collapse batch dimension\n        weights = self.gradients.mean(dim=(2, 3), keepdim=True)      # [1,C,1,1]\n        cam = (weights * self.activations).sum(dim=1, keepdim=False) # [1,H',W']\n        cam = cam.relu()[0]  # keep positive influence\n\n        # normalize to [0,1]\n        cam -= cam.min()\n        cam /= cam.max().clamp_min(1e-8)\n        return cam.detach().cpu().numpy(), class_idx"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456a2d94-dc09-463c-a69b-bb1956e98ad7",
   "metadata": {},
   "outputs": [],
   "source": "def compute_gradcam(img_path, model, transform, device):\n    \"\"\"\n    Given an image file, compute the GradCAM heatmap for the most likely class.\n\n    Args:\n        img_path (str): Path to the input image.\n        model (torch.nn.Module): Pretrained PyTorch model (e.g., ResNet50).\n        transform (callable): Preprocessing function for model input.\n        device (torch.device): Device to run computation on (cpu/cuda).\n\n    Returns:\n        img_display (np.ndarray): The preprocessed image for display (H, W, 3).\n        heatmap (np.ndarray): The GradCAM heatmap (h', w') with values in [0, 1].\n        pred_class_name (str): String with the ImageNet class label.\n        pred_score (float): Class confidence/probability.\n    \"\"\"\n    try:\n        # -------- 1. Load and prepare the image --------\n        img = load_image(img_path)  # Load the image, make sure it's RGB\n        img_display = np.array(img.resize((224, 224)))   # Resize for consistent display (uint8)\n        img_tensor = transform(img).unsqueeze(0).to(device)   # Transform: resize, normalize, (1,3,224,224)\n\n        # -------- 2. Forward pass: Model prediction --------\n        output = model(img_tensor)  # Output logits from the model\n        pred_class_idx = torch.argmax(output, dim=1).item()   # Index of highest scoring class\n        # Get predicted class probability (softmax output, as float)\n        pred_score = torch.softmax(output, dim=1)[0, pred_class_idx].item()\n        # Map class index to human-readable label\n        pred_class_name = imagenet_class_mapping[pred_class_idx]\n\n        # -------- 3. GradCAM calculation (for model explanation) --------\n        # Pick the last convolutional layer in ResNet50 for GradCAM (recommended practice)\n        grad_cam = GradCAM(model, model.layer4[-1].conv3)\n        # Generate GradCAM heatmap for the predicted class\n        heatmap, _= grad_cam(img_tensor, pred_class_idx)  # heatmap shape: (activation_h, activation_w)\n\n        # -------- 4. Return results for visualization --------\n        return img_display, heatmap, pred_class_name, pred_score\n\n    except Exception as e:\n        # Handles any error during loading or computation gracefully\n        print(f\"Error processing image {img_path}: {e}\")\n        return None, None, \"Error\", 0"
  },
  {
   "cell_type": "markdown",
   "id": "af94092c-0b19-4e6e-b90d-2848f7375eb6",
   "metadata": {},
   "source": "Summary of the steps:\n\n1. Loads and preprocesses an image for ResNet input.\n2. Forwards through the model to get a class prediction, probability, and label.\n3. Computes GradCAM for that class using the last convolutional layer.\n4. Returns everything needed for visualization: display image, heatmap, label, and score."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29394378-f469-416e-a9e7-48ce5ae83696",
   "metadata": {},
   "outputs": [],
   "source": "def visualize_gradcam(img_display, heatmap, pred_class, pred_score, title):\n    \"\"\"\n    Visualizes GradCAM results using Matplotlib.\n\n    This function creates a figure with 3 side-by-side panels:\n      1. The original image.\n      2. The standalone GradCAM heatmap, showing the most important \"regions\" for prediction.\n      3. An overlay of the heatmap on the original image, so you can see how the model's \"attention\"\n         aligns with visual features.\n\n    The last panel is saved as a PNG with the filename: {title}_gradcam.png\n\n    Args:\n        img_display (np.ndarray): The image to display (height x width x 3, uint8).\n        heatmap (np.ndarray): The GradCAM activation map (values in [0,1]).\n        pred_class (str or int): The predicted class label or index.\n        pred_score (float): Model confidence/probability for predicted class.\n        title (str): Output filename prefix and figure titles.\n    \"\"\"\n    # --------- 1. Set up a 3-panel matplotlib figure ---------\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n\n    # --------- 2. Show original image ---------\n    ax1.imshow(img_display)\n    ax1.set_title(f'Original Image: {title}', fontsize=14)\n    ax1.axis('off')\n\n    # --------- 3. Show GradCAM heatmap alone ---------\n    # The heatmap is already normalized to [0,1]; use 'jet' colormap for colorful regions\n    ax2.imshow(heatmap, cmap='jet')\n    ax2.set_title('GradCAM Heatmap', fontsize=14)\n    ax2.axis('off')\n\n    # --------- 4. Overlay heatmap on original image ---------\n    # a. Resize the heatmap to match the original image size (if necessary)\n    heatmap_resized = cv2.resize(heatmap, (img_display.shape[1], img_display.shape[0]))\n    # b. Convert the heatmap to a color image using OpenCV's colormap\n    heatmap_color = cv2.applyColorMap(np.uint8(255 * heatmap_resized), cv2.COLORMAP_JET)\n    # c. Convert color from BGR (OpenCV default) to RGB (matplotlib expects RGB)\n    heatmap_color = cv2.cvtColor(heatmap_color, cv2.COLOR_BGR2RGB)\n    # d. Overlay: combine original image and heatmap color with chosen transparency\n    # 0.6 for original image, 0.4 for heatmap = visible blend of both\n    superimposed = cv2.addWeighted(img_display, 0.6, heatmap_color, 0.4, 0)\n    ax3.imshow(superimposed)\n    # Show predicted class and confidence in the overlay title\n    ax3.set_title(f'GradCAM Overlay\\nPrediction: {pred_class}\\nConfidence: {pred_score:.2f}', fontsize=14)\n    ax3.axis('off')\n\n    # --------- 5. Finalize and save ---------\n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b614dbf9-d763-4e10-8943-5e769cd0ce91",
   "metadata": {},
   "outputs": [],
   "source": "image_files = ['images/cat.jpg']\n# Check that files exist\nexisting_files = [f for f in image_files if os.path.exists(f)]\nfor img_path in existing_files:\n    # Use only base filename for saving overlays!\n    title = os.path.splitext(os.path.basename(img_path))[0]\n    print(f\"Processing {title}...\")\n    img_display, heatmap, pred_class, pred_score = compute_gradcam(\n        img_path, model, transform, device\n    )\n    if img_display is not None:\n        visualize_gradcam(img_display, heatmap, pred_class, pred_score, title)\n    else:\n        print(f\"Skipping visualization for {img_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b56d2b-26e2-4cc7-9154-3942f1dea34e",
   "metadata": {},
   "outputs": [],
   "source": "import helper_utils"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92671eb1-6134-497b-84e6-764ce2ba1195",
   "metadata": {},
   "outputs": [],
   "source": "helper_utils.plot_widget(compute_gradcam, visualize_gradcam, model, transform, device, folder=\"images\")"
  },
  {
   "cell_type": "markdown",
   "id": "0228dde9-c908-49ca-a65b-2f46de6f26cb",
   "metadata": {},
   "source": "## 5 - Conclusion\n\nIn this lab, you explored some of the most widely-used techniques for interpreting deep convolutional neural networks: **Saliency Maps** and **Class Activation Maps (GradCAM)**.  \nBy visualizing model “attention” on your own images, you gained practical insights into how a state-of-the-art classifier like ResNet50 reasons about visual information.\n\n**What you accomplished:**\n- Loaded and preprocessed images for a pretrained model.\n- Calculated and displayed saliency maps, highlighting pixels that most strongly influenced the model's decision for a target class.\n- Used GradCAM to generate and overlay spatial heatmaps, revealing which regions of an input image drove the final prediction.\n- Built reusable and interactive tools for explaining model behavior, empowering you to easily run these analyses on new images.\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}