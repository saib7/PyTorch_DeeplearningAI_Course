{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Programming Assignment: Fruit Guard Pro - Visualizing AI Food Quality Inspection\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this notebook, you'll explore a series of hands-on steps that build your understanding of how vision models process, interpret, and even create images. You'll start by examining how convolutional layers capture local features and progressively integrate them into more abstract representations as the receptive field expands. From there, you'll investigate ways to uncover what parts of an image most influence a model's predictions, and finally, you'll move into methods that generate entirely new images from scratch.\n",
        "\n",
        "By working through the exercises, you will:\n",
        "\n",
        "- Analyze how feature maps evolve through deeper convolutional layers and how receptive fields grow.\n",
        "- Apply pixel-level interpretability with saliency maps to highlight influential image areas.\n",
        "- Use region-focused techniques like Class Activation Mapping (CAM) to visualize high-level feature importance.\n",
        "- Experiment with generative approaches such as diffusion models to transform noise into coherent images.\n",
        "\n",
        "This progression will give you both an internal view of a model's decision process and an external ability to guide or control the outputs it produces.\n",
        "\n",
        "-------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Table of Contents\n",
        "- [ 1 - Imports](#1)\n",
        "- [ 2 - Fruits Dataset](#2)\n",
        "- [ 3 - Fruit Guard Pro Pre Trained Model](#3)\n",
        "- [ 4 - Feature Hierarchy Visualization](#4)\n",
        "  - [ Exercise 1](#ex01)\n",
        "- [ 5 - Feature Map Strip](#5)\n",
        "  - [ Exercise 2](#ex02)\n",
        "- [ 6 - Saliency Map](#6)\n",
        "  - [ Exercise 3](#ex03)\n",
        "- [ 7 - Simplified Class Activation Map (CAM)](#7)\n",
        "  - [ Exercise 4](#ex04)\n",
        "- [ 8 - Comparison of Interpretability Techniques](#8)\n",
        "- [ 9 - Optional: Text-to-Image Generation](#9)\n",
        "  - [ 9.1 - Optional Exercise: Text-to-Image Generation](#9-1)\n",
        "  - [ 9.2 - Denoising Timelapse](#9-2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<a name='submission'></a>\n",
        "\n",
        "<h4 style=\"color:green; font-weight:bold;\">TIPS FOR SUCCESSFUL GRADING OF YOUR ASSIGNMENT:</h4>\n",
        "\n",
        "* All cells are frozen except for the ones where you need to submit your solutions or when explicitly mentioned you can interact with it.\n",
        "\n",
        "* In each exercise cell, look for comments `### START CODE HERE ###` and `### END CODE HERE ###`. These show you where to write the solution code. **Do not add or change any code that is outside these comments**.\n",
        "\n",
        "* You can add new cells to experiment but these will be omitted by the grader, so don't rely on newly created cells to host your solution code, use the provided places for this.\n",
        "\n",
        "* Avoid using global variables unless you absolutely have to. The grader tests your code in an isolated environment without running all cells from the top. As a result, global variables may be unavailable when scoring your submission. Global variables that are meant to be used will be defined in UPPERCASE.\n",
        "\n",
        "* To submit your notebook for grading, first save it by clicking the ðŸ’¾ icon on the top left of the page and then click on the `Submit assignment` button on the top right of the page.\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='1'></a>\n",
        "## 1 - Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import helper_utils\n",
        "import unittests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "cmC6itUkagNJ",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='2'></a>\n",
        "## 2 - Fruits Dataset\n",
        "\n",
        "To test and study the techniques you will implement, you will need image data. For this assignment, you are provided with a subset of a fruit dataset containing examples of different fruits in both fresh and rotten conditions. These images are similar to those used by a pre-trained model, the **Fruit Guard Pro** (based on ResNet-50), which classifies fruits as fresh or rotten.\n",
        "\n",
        "**Dataset Overview:**\n",
        "- **Total images**: 234 images across train/dev/test splits\n",
        "- **Classes**: 2 (Fresh = 0, Rotten = 1)\n",
        "- **Image size**: Variable (will be resized to 224Ã—224 for the model)\n",
        "- **Fruit types**: Apples, Mangoes, Tomatoes in various conditions\n",
        "- **Use case**: Binary classification for quality control in food processing\n",
        "\n",
        "During the lab, you will implement tools to better understand how the network processes and evaluates these images.\n",
        "\n",
        "First let's see how the data is structured by displaying some (k=3) that you might want to use as test examples later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "L6V3CrS1c-8c"
      },
      "outputs": [],
      "source": [
        "# Checking some files to test our functions\n",
        "\n",
        "dataset_path = \"./data/fruit_subset/\"\n",
        "\n",
        "helper_utils.print_samples_from_dataset(dataset_path, k = 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use the `plot_samples_from_dataset()` function from `helper_utils` with `dataset_path` as the argument to visualize a few example images from the dataset. This will help you get familiar with the type of data youâ€™ll be working with before moving on to the implementation steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "helper_utils.plot_samples_from_dataset(dataset_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='3'></a>\n",
        "## 3 - Fruit Guard Pro Pre Trained Model\n",
        "\n",
        "Here, you load a pre-trained Fruit Guard Pro model and inspect its structure. The model is built on ResNet50 but has been modified for binary classification â€” fresh vs. rotten fruit â€” by replacing its final layer with one that outputs two values. The saved weights are then loaded so the model is ready to make predictions without training from scratch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "# Loading the FruitGuard Pro weights in a model based on ResNet-50\n",
        "\n",
        "net = helper_utils.load_model(\"./data/fruit_guard_pro.pth\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once loaded, you print the architecture layer by layer, showing each layerâ€™s name, type, and number of parameters. For sequential blocks, you also drill down into their sub-layers. This helps you see how the network processes images, from early convolutional layers that capture small details to deeper layers that combine those into larger patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "# Print the model architecture\n",
        "\n",
        "helper_utils.print_model_architecture(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Understanding this structure is key for the next steps â€” activation visualization, saliency maps, and class activation maps â€” because youâ€™ll know exactly which layers to monitor. With a clear map of the modelâ€™s internals, you can choose the most informative points to analyze and interpret how the network reaches its decisions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='4'></a>\n",
        "## 4 - Feature Hierarchy Visualization\n",
        "\n",
        "In this task, you will create a function that helps visualize how different layers of the Fruit Guard Pro model transform an image as it moves deeper into the network. You'll load an image, prepare it in the right format for the model, and capture the feature maps (activations) from key layers using forward hooks.  \n",
        "\n",
        "Your job will be to:  \n",
        "- Preprocess the image so it matches the model's expected size and normalization.  \n",
        "- Select important convolutional layers and attach hooks to record their outputs during a forward pass.  \n",
        "- Run the image through the model to collect these activations.  \n",
        "- Return the captured feature maps so they can be used for visualization and to understand the model's feature hierarchy and receptive fields.  \n",
        "\n",
        "By completing this, you'll see how the network's early layers capture fine details, while deeper layers respond to more complex and abstract patterns.\n",
        "\n",
        "Let's start.\n",
        "\n",
        "-------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following helper function creates a forward hook that stores the output (activation) of a specific layer during a modelâ€™s forward pass. When you call `grab(activations, name)`, it returns a hook function that, once attached to a layer, will save that layerâ€™s output tensor into the `activations` dictionary under the given `name`. The `.detach()` method is used to remove the tensor from the computation graph so it wonâ€™t track gradients, which saves memory and makes it easier to work with later.  \n",
        "\n",
        "In short, this function lets you capture and label the feature maps from chosen layers, making it possible to inspect how the network processes an image at different stages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "\n",
        "def grab(activations, name):\n",
        "    def _hook(_, __, out): \n",
        "        activations[name] = out.detach()\n",
        "    return _hook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='ex01'></a>\n",
        "### Exercise 1\n",
        "\n",
        "Implement ```cnn_feature_hierarchy_demo(img, model)```.\n",
        "\n",
        "Goal: Using a pretrained FruitGuard Pro (ResNet-50 backbone) and a preprocessed input tensor, return a dictionary mapping layer names â†’ captured feature-map tensors.\n",
        "\n",
        "The function has the following stages:\n",
        "1. Assume Preprocessed Input\n",
        "   - img is already:\n",
        "      + RGB\n",
        "      + center-cropped to 224Ã—224\n",
        "      + normalized with ImageNet statistics (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "      + shaped (1, 3, 224, 224) and placed on the same device as model.\n",
        "\n",
        "2. Register Forward Hooks\n",
        "   - Create a helper ```grab(name)``` that returns a hook storing a layer's output into ```activations[name]```.\n",
        "   - Select five sampling points:\n",
        "      + \"conv1\" â†’ model.conv1\n",
        "      + \"layer1\" â†’ model.layer1[0].conv1\n",
        "      + \"layer2\" â†’ model.layer2[0].conv1\n",
        "      + \"layer3\" â†’ model.layer3[0].conv1\n",
        "      + \"layer4\" â†’ model.layer4[0].conv1\n",
        "   - Attach hooks so that, during the forward pass, raw feature maps are recorded under those keys.\n",
        "\n",
        "3. Forward Pass\n",
        "   - Run the model under torch.no_grad():\n",
        "   ```\n",
        "      with torch.no_grad():\n",
        "         _ = model(img)\n",
        "   ```\n",
        "   - Afterward, activations should look like:\n",
        "   ```\n",
        "      {\n",
        "      \"conv1\":  Tensor of shape (1, C1, H1, W1),\n",
        "      \"layer1\": Tensor of shape (1, C2, H2, W2),\n",
        "      ...,\n",
        "      \"layer4\": Tensor of shape (1, C5, H5, W5),\n",
        "      }\n",
        "   ```\n",
        "4. Clean Up Hooks\n",
        "   - Remove all registered hooks to avoid memory leaks.\n",
        "5. Return\n",
        "   - Return the activations dictionary.\n",
        "---\n",
        "\n",
        "The output of this functionâ€”a dictionary of feature mapsâ€”will then be used by provided code to identify the channels with the highest average activation. These selected channels will be upsampled to 224Ã—224 and visualized side by side to reveal how the network's internal representations evolve across layers.\n",
        "\n",
        "<details>\n",
        "<summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
        "\n",
        "### 1) Hook & registration patern\n",
        "```python\n",
        "   def grab(store, name):\n",
        "      def _hook(module, inputs, output):\n",
        "         store[name] = output.detach()\n",
        "      return _hook\n",
        "\n",
        "   activations = {}\n",
        "   layers = {\n",
        "      \"conv1\":  model.conv1,\n",
        "      \"layer1\": model.layer1[0].conv1,\n",
        "      \"layer2\": model.layer2[0].conv1,\n",
        "      \"layer3\": model.layer3[0].conv1,\n",
        "      \"layer4\": model.layer4[0].conv1,\n",
        "   }\n",
        "\n",
        "   hooks = [layer.register_forward_hook(grab(activations, n))\n",
        "            for n, layer in layers.items()]\n",
        "```\n",
        "\n",
        "### 2) Forwar + cleanup\n",
        "```python\n",
        "   with torch.no_grad():\n",
        "      _ = model(img)\n",
        "\n",
        "   for hook in hook_array:\n",
        "      hook.remove()\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "id": "TqbSppasdFyw",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: cnn_feature_hierarchy_demo\n",
        "\n",
        "def cnn_feature_hierarchy_demo(img, model):\n",
        "    \"\"\"\n",
        "    Visualise FruitGuard Pro feature hierarchy & print receptive-field stats for key layers.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    img : torch.Tensor\n",
        "        Tensor of an RGB image (will be centre-cropped to 224Ã—224).\n",
        "    model : torch.nn.Module\n",
        "        Pretrained model to use for feature extraction.\n",
        "    save_path : str | None, default = None\n",
        "        If given, save the matplotlib figure to this path.\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, torch.Tensor]\n",
        "        A dictionary mapping layer names â†’ captured feature-map tensors.\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    activations = {} # Initialize an empty dictionary to store the activations \n",
        "    layers = { # Dictionary of layers to register forward hooks for \n",
        "        \"conv1\": model.conv1, # Register forward hook for the first convolution layer\n",
        "        \"layer1\": model.layer1, # Register forward hook for the first convolution layer in the first layer of the model\n",
        "        \"layer2\": model.layer2, # Register forward hook for the first convolution layer in the second layer of the model\n",
        "        \"layer3\": model.layer3, # Register forward hook for the first convolution layer in the third layer of the model\n",
        "        \"layer4\": model.layer4, # Register forward hook for the first convolution layer in the fourth layer of the model\n",
        "    }\n",
        "    hooks = [] # List to store the hook handles\n",
        "    for name, layer in layers.items(): # Loop over the layers dictionary\n",
        "        hook = layer.register_forward_hook(grab(activations, name)) # Register the forward hook\n",
        "        hooks.append(hook) # Append the hook handle to the hooks list   \n",
        "    \n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        _ = model(img) \n",
        "\n",
        "    # Remove hooks\n",
        "    for h in hooks:  \n",
        "        h.remove() \n",
        "\n",
        "    return activations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To verify your code, run the following cells!\n",
        "\n",
        "You can test your code with different images changing the `image_path` variable. Some values that can be used are:\n",
        "\n",
        "- ./data/fruit_subset/Apple__Rotten/rottenApple (482).jpg\n",
        "- ./data/fruit_subset/Apple__Rotten/rottenApple (522).jpg\n",
        "- ./data/fruit_subset/Mango__Healthy/35.jpg\n",
        "- ./data/fruit_subset/Mango__Rotten/163.jpg\n",
        "- ./data/fruit_subset/Tomato__Healthy/freshTomato (12).jpg\n",
        "- ./data/fruit_subset/Tomato__Rotten/rottenTomato (9).jpg\n",
        "\n",
        "You can find more images to test on in the folder ./data/fruit_subset/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "# Set the path to the image to test your function\n",
        "image_path = \"./data/fruit_subset/Apple__Healthy/FreshApple (2).jpg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "# Verify your implementation\n",
        "\n",
        "img = helper_utils.preprocess_image(image_path, device)\n",
        "net = net.to(device)\n",
        "\n",
        "# Compute the activations\n",
        "activations = cnn_feature_hierarchy_demo(\n",
        "    img=img,\n",
        "    model=net\n",
        ")\n",
        "\n",
        "# Check all keys and shapes\n",
        "print(\"Activations keys and shapes:\")\n",
        "print(\"-\"*100)\n",
        "print(activations.keys())\n",
        "print(activations[\"conv1\"].shape)\n",
        "print(activations[\"layer1\"].shape)\n",
        "print(activations[\"layer2\"].shape)\n",
        "print(activations[\"layer3\"].shape)\n",
        "print(activations[\"layer4\"].shape)\n",
        "print(\"-\"*100)\n",
        "\n",
        "# Display the feature hierarchy for the given image\n",
        "helper_utils.display_feature_hierarchy(activations, image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Expected Output:\n",
        "\n",
        "```\n",
        "Activations keys and shapes:\n",
        "----------------------------------------------------------------------------------------------------\n",
        "dict_keys(['conv1', 'layer1', 'layer2', 'layer3', 'layer4'])\n",
        "torch.Size([1, 64, 112, 112])\n",
        "torch.Size([1, 64, 56, 56])\n",
        "torch.Size([1, 128, 56, 56])\n",
        "torch.Size([1, 256, 28, 28])\n",
        "torch.Size([1, 512, 14, 14])\n",
        "----------------------------------------------------------------------------------------------------\n",
        "```\n",
        "The displayed image should look something like the following, notice that it will change if you use a different sample image:\n",
        "# ![Expected Output](./images/exercise1_expected.jpg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "# Test your code!\n",
        "\n",
        "unittests.exercise_1(cnn_feature_hierarchy_demo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='5'></a>\n",
        "## 5 - Feature Map Strip\n",
        "\n",
        "In this task, you will build a compact visual summary that shows the strongest responding feature at five depths of the model. You will pass forward an already-preprocessed image tensor through the model, capture feature maps from selected layers, pick the most active channel at each depth, upsample it to 224Ã—224, and return a list of five single-channel tensors suitable for side-by-side visualization.\n",
        "\n",
        "Your job will be to:\n",
        "- Attach forward hooks on `conv1` and the first convolution of `layer1`, `layer2`, `layer3`, and `layer4` to capture their feature maps during a forward pass.\n",
        "- Run the forward pass to populate the captured maps and then remove the hooks.\n",
        "- For each captured map, identify the channel with the highest mean activation, select it, upsample it to 224Ã—224 with bilinear interpolation, and normalize the values to a 0â€“1 range.\n",
        "- Return the ordered list of the five upsampled tensors so they can be visualized as a feature map strip.\n",
        "\n",
        "By completing this, you will get a quick, interpretable snapshot of how the model's focus shifts from simple edge and texture responses toward more abstract patterns as depth increases.\n",
        "\n",
        "Let's start.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='ex02'></a>\n",
        "### Exercise 2\n",
        "\n",
        "Implement a function that, given an RGB image and the pretrained Fruig Guard Pro model, returns a **list** of five 4D tensorsâ€”one per sampled layerâ€”each upsampled to 224Ã—224 and normalized to [0, 1].\n",
        "\n",
        "The function follow the followin stages:\n",
        "\n",
        "1. Capture feature maps with forward hooks\n",
        "   - Register hooks on:\n",
        "      + model.conv1\n",
        "      + model.layer1[0].conv1\n",
        "      + model.layer2[0].conv1\n",
        "      + model.layer3[0].conv1\n",
        "      + model.layer4[0].conv1\n",
        "   - Each hook should store its output (detached to CPU) in a dict (e.g., feats) under the keys: \"conv1\", \"layer1\", \"layer2\", \"layer3\", \"layer4\".\n",
        "   - Use a grab function to register forward hooks\n",
        "\n",
        "2. Forward pass (no grads)\n",
        "   - Run with torch.no_grad(): _ = model(img) to populate feats, then remove all hooks to avoid leaks.\n",
        "\n",
        "3. Select, upsample, and normalize the top channel per layer\n",
        "   - For each of the five keys in order:\n",
        "      + Compute the per-channel mean over spatial dims: fm.mean((2, 3)).\n",
        "      + Take the index of the highest mean activation.\n",
        "      + Select that channel while keeping the channel dimension: fm[:, idx:idx+1].\n",
        "      + Upsample to (224, 224) using bilinear interpolation with align_corners=False.\n",
        "      + Minâ€“max normalize to [0, 1] with a small epsilon (1e-8) to avoid divide-by-zero.\n",
        "4. Return value\n",
        "   - Return an ordered list of five tensors (one per depth), each shaped (1, 1, 224, 224), corresponding to:\n",
        "   [\"conv1\", \"layer1\", \"layer2\", \"layer3\", \"layer4\"].\n",
        "\n",
        "<details>\n",
        "<summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
        "\n",
        "#### **Define Grab Function**\n",
        "```python\n",
        "   def grab(name):\n",
        "      # Add key name to feats and store the output, detach and move to cpu.\n",
        "      return lambda _m, _i, out: feats.setdefault(name, out.detach().cpu())\n",
        "```\n",
        "\n",
        "#### **Register forward hooks**\n",
        "```python\n",
        "   hooks = [\n",
        "         model.conv1.register_forward_hook(grab(\"conv1\")), \n",
        "         model.layer1[0].conv1.register_forward_hook(grab(\"layer1\")), \n",
        "         .\n",
        "         .\n",
        "         .\n",
        "   ]\n",
        "```\n",
        "\n",
        "#### Forward Pass and Remove Hooks\n",
        "```python\n",
        "# Forward pass\n",
        "   with torch.no_grad():\n",
        "      _ = model(img)\n",
        "   \n",
        "   # Remove hooks\n",
        "   for h in hooks: \n",
        "      h.remove()\n",
        "```\n",
        "\n",
        "#### Select, upsample and normalize\n",
        "```python\n",
        "   # Capture channel with highest mean activation and upsample feature maps\n",
        "    upsampled = []\n",
        "    \n",
        "    for name in [\"conv1\", \"layer1\", \"layer2\", \"layer3\", \"layer4\"]: \n",
        "        fm = feats[name]\n",
        "        # Get the index of the max channel\n",
        "        sel = fm[:, idx:idx+1]\n",
        "        .\n",
        "        .\n",
        "        .\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "id": "8JrZ2iaEgOL2",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: feature_map_strip\n",
        "\n",
        "def feature_map_strip(img, model):\n",
        "    \"\"\"\n",
        "    Forward-pass an image through pretrained ResNet-50, capture the feature map\n",
        "    with the highest mean activation at five depths, upsample them and returns a\n",
        "    list of five upsampled tensors.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    img : torch.Tensor\n",
        "        Tensor of an RGB image (will be centre-cropped to 224Ã—224).\n",
        "    model : torch.nn.Module\n",
        "        Pretrained model to use for feature extraction.\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, torch.Tensor]\n",
        "        Dictionary of raw feature maps for keys\n",
        "        {\"conv1\", \"layer1\", \"layer2\", \"layer3\", \"layer4\"}.\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    \n",
        "    # Set model to evaluation mode\n",
        "    model.eval() \n",
        "    \n",
        "    # Get feature maps\n",
        "    feats = {} # Initialize an empty dictionary to store the activations\n",
        "\n",
        "    # Register forward hooks\n",
        "    def grab(name):\n",
        "        # Add key name to feats and store the output, detach and move to cpu.\n",
        "        return lambda _m, _i, out: feats.__setitem__(name, out.detach().cpu())\n",
        "\n",
        "    hooks = [ # Register forward hooks for each layer\n",
        "        model.conv1.register_forward_hook(grab(\"conv1\")), # Register forward hook for the first convolution layer\n",
        "        model.layer1.register_forward_hook(grab(\"layer1\")), # Register forward hook for the first convolution layer in the first layer of the model\n",
        "        model.layer2.register_forward_hook(grab(\"layer2\")), # Register forward hook for the first convolution layer in the second layer of the model\n",
        "        model.layer3.register_forward_hook(grab(\"layer3\")), # Register forward hook for the first convolution layer in the third layer of the model\n",
        "        model.layer4.register_forward_hook(grab(\"layer4\")), # Register forward hook for the first convolution layer in the fourth layer of the model\n",
        "    ]\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        # Forward pass the image through the model\n",
        "        _ = model(img)\n",
        "    \n",
        "    # Remove hooks\n",
        "    for h in hooks: \n",
        "        h.remove()\n",
        "\n",
        "    # Capture channel with highest mean activation and upsample feature maps\n",
        "    upsampled = [] # Initialize an empty list to store the upsampled feature maps\n",
        "    \n",
        "    for name in [\"conv1\", \"layer1\", \"layer2\", \"layer3\", \"layer4\"]:\n",
        "        fm = feats[name] # Get the feature map for the <<name>> layer\n",
        "        idx = fm.mean(dim=(2,3)).argmax().item() # Get the index of the channel with the highest mean activation\n",
        "        sel = fm[0, idx, :, :].unsqueeze(0).unsqueeze(0) # Select the channel with the highest mean activation\n",
        "        sel = F.interpolate(sel, size=(224,224), mode='bilinear', align_corners=False) # Upsample the feature map to 224x224\n",
        "        sel = (sel - sel.min()) / (sel.max() - sel.min()) # Normalize the feature map\n",
        "        upsampled.append(sel), # Append the upsampled feature map to the list\n",
        "        \n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return upsampled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To verify your code, run the following cells!\n",
        "\n",
        "You can test your code with different images changing the `image_path` variable. Some values that can be used are:\n",
        "\n",
        "- ./data/fruit_subset/Apple__Rotten/rottenApple (482).jpg\n",
        "- ./data/fruit_subset/Apple__Rotten/rottenApple (522).jpg\n",
        "- ./data/fruit_subset/Mango__Healthy/35.jpg\n",
        "- ./data/fruit_subset/Mango__Rotten/163.jpg\n",
        "- ./data/fruit_subset/Tomato__Healthy/freshTomato (12).jpg\n",
        "- ./data/fruit_subset/Tomato__Rotten/rottenTomato (9).jpg\n",
        "\n",
        "You can find more images to test on in the folder ./data/fruit_subset/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "# Set the path to the image to test the feature map strip function\n",
        "image_path = \"./data/fruit_subset/Apple__Healthy/FreshApple (2).jpg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "# Verify your implementation\n",
        "\n",
        "img = helper_utils.preprocess_image(image_path)\n",
        "net = net.to(\"cpu\")\n",
        "# Display the feature hierarchy for the given image\n",
        "upsampled = feature_map_strip(img, net)\n",
        "\n",
        "print(\"Shape of the upsampled feature maps:\")\n",
        "print(\"-\"*108)\n",
        "print(upsampled[0].shape)\n",
        "print(upsampled[1].shape)\n",
        "print(upsampled[2].shape)\n",
        "print(upsampled[3].shape)\n",
        "print(upsampled[4].shape)\n",
        "print(\"-\"*108)\n",
        "\n",
        "helper_utils.visual_strip(upsampled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Expected Output:\n",
        "```\n",
        "Shape of the upsampled feature maps:\n",
        "------------------------------------------------------------------------------------------------------------\n",
        "torch.Size([1, 1, 224, 224])\n",
        "torch.Size([1, 1, 224, 224])\n",
        "torch.Size([1, 1, 224, 224])\n",
        "torch.Size([1, 1, 224, 224])\n",
        "torch.Size([1, 1, 224, 224])\n",
        "------------------------------------------------------------------------------------------------------------\n",
        "```\n",
        "Notice that the displayed image might be different if you use a different input image.\n",
        "# ![Expected Output](./images/solution2.jpg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "# Test your code!\n",
        "\n",
        "unittests.exercise_2(feature_map_strip)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='6'></a>\n",
        "## 6 - Saliency Map\n",
        "\n",
        "In this exercise, you will implement a function that highlights which pixels in an input image most influence a model's score for a specific class. The result is a saliency heatmap: brighter values indicate pixels where small changes would most affect the chosen class score. This gives you a fine-grained view of what the network is sensitive to at the input level and helps you verify that attention is on the fruit rather than background clutter.\n",
        "\n",
        "<a id='ex03'></a>\n",
        "### Exercise 3\n",
        "\n",
        "Your job will be to implement the function ```saliency_map(model, image_tensor, class_idx)```.\n",
        "- Make sure gradients flow to the input by working with a leaf tensor that has `requires_grad=True`.\n",
        "- Run a forward pass to obtain the logits and select the target class score using `class_idx`.\n",
        "- Zero existing gradients and backpropagate from the selected logit to the input.\n",
        "- Take the absolute value of the input gradients and reduce over the color channels to get a single 2-D map.\n",
        "- Normalize the map to the range [0, 1] and return it for visualization or overlay later.\n",
        "\n",
        "By completing this, you'll produce a clear, pixel-level explanation of the model's sensitivity for a chosen class, a helpful complement to the deeper, region-level views you will build in later steps.\n",
        "\n",
        "\n",
        "<details>\n",
        "<summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
        "1. **Prepare the Input for Gradients**  \n",
        "   - Take the provided `image_tensor` (shape `(1, 3, H, W)`) and ensure it is a leaf tensor with `requires_grad=True`.  \n",
        "   - Hint: use `.clone().detach().requires_grad_(True)`.\n",
        "\n",
        "2. **Forward Pass & Select Target Logit**  \n",
        "   - Run the model on the input to get `output` (shape `(1, num_classes)`).  \n",
        "   - Extract the scalar logit at `[0, class_idx]` as `target_logit`.\n",
        "\n",
        "3. **Backward Pass**  \n",
        "   - Zero any existing gradients on the model via `model.zero_grad()`.  \n",
        "   - Call `target_logit.backward()` to compute gradients of the logit w.r.t. each input pixel.\n",
        "\n",
        "4. **Compute Saliency Map**  \n",
        "   - Access `image_tensor.grad` of shape `(1, 3, H, W)`.  \n",
        "   - Take the absolute value and sum over the color channel dimension â†’ results in a 2D tensor of shape `(H, W)`.\n",
        "\n",
        "5. **Normalize to [0, 1]**  \n",
        "   - Subtract the minimum, then divide by `(max + Îµ)` to scale all values into the [0,1] range.\n",
        "\n",
        "6. **Return Value**  \n",
        "   - Return the final 2D heatmap tensor of shape `(H, W)`, detached from the graph.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": true,
        "id": "q5E8Edh2d92c",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: saliency_map\n",
        "\n",
        "def saliency_map(model, image_tensor, class_idx):\n",
        "    \"\"\"\n",
        "    Generate a saliency map for a single image and class.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : torch.nn.Module\n",
        "        A *trained* CNN; should already be in `.eval()` mode.\n",
        "    image_tensor : torch.Tensor\n",
        "        Input tensor with shape (1, 3, H, W).  Must be a **leaf tensor** and\n",
        "        pre-processed the same way the model expects (e.g., ImageNet normalisation).\n",
        "    class_idx : int\n",
        "        Index of the target class logit to explain.\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "        A 2-D saliency heat-map normalised to the range [0, 1] with shape (H, W).\n",
        "    \"\"\" \n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # Ensure gradients flow to the input\n",
        "    image_tensor = image_tensor.clone() # Make a copy of the image tensor\n",
        "    image_tensor = image_tensor.detach() # detach the image tensor\n",
        "    image_tensor.requires_grad_(True) # Set requires grad to True\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(image_tensor) # Forward pass the image through the model\n",
        "    target_logit = output[0, class_idx] # Get the target logit assume class_idx is the index of the target class\n",
        "\n",
        "    # Backward pass â€“ gradient w.r.t. input pixels\n",
        "    model.zero_grad() # Zero gradients\n",
        "    target_logit.backward() # Backward pass to compute gradients\n",
        "\n",
        "    # Absolute gradient, sum over colour channels\n",
        "    grads = image_tensor.grad.abs().sum(dim=1)[0]  # Absolute gradient, sum over colour channels\n",
        "\n",
        "    # Normalise to [0, 1]\n",
        "    # grads = (grads - min) / (max + epsilon) \n",
        "    grads -= grads.min()\n",
        "    grads /= (grads.max() + 1e-8) # Use epslon as a small number (1e-8)to avoid division by zero\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    heatmap = grads  # alias for clarity\n",
        "\n",
        "    return heatmap.detach()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To verify your code, run the following cells!\n",
        "\n",
        "You can test your code with different images changing the `image_path` variable. Some values that can be used are:\n",
        "\n",
        "- ./data/fruit_subset/Apple__Rotten/rottenApple (482).jpg\n",
        "- ./data/fruit_subset/Apple__Rotten/rottenApple (522).jpg\n",
        "- ./data/fruit_subset/Mango__Healthy/35.jpg\n",
        "- ./data/fruit_subset/Mango__Rotten/163.jpg\n",
        "- ./data/fruit_subset/Tomato__Healthy/freshTomato (12).jpg\n",
        "- ./data/fruit_subset/Tomato__Rotten/rottenTomato (9).jpg\n",
        "\n",
        "You can find more images to test on in the folder ./data/fruit_subset/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "# Set the path to the image\n",
        "image_path = \"./data/fruit_subset/Apple__Rotten/rottenApple (1).jpg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "# Test saliency map\n",
        "\n",
        "# Open image and transform to processed tensor\n",
        "image_tensor = helper_utils.preprocess_image(image_path)\n",
        "net = helper_utils.load_model(\"./data/fruit_guard_pro.pth\")\n",
        "# Set class index to 1 (rotten)\n",
        "# See why the net thinks the image is from a rotten fruit!!!\n",
        "class_idx = 1\n",
        "\n",
        "heatmap = saliency_map(\n",
        "    model=net,\n",
        "    image_tensor=image_tensor,\n",
        "    class_idx=class_idx\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Shape and range of the heatmap:\")\n",
        "print(\"-\"*100)\n",
        "print(f\"shape = {heatmap.shape}\")\n",
        "print(f\"min = {heatmap.min()}, max = {heatmap.max()}\")\n",
        "print(\"-\"*100)\n",
        "\n",
        "helper_utils.display_saliency(image_tensor=image_tensor, heatmap=heatmap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Expected Output:\n",
        "```\n",
        "Shape and range of the heatmap:\n",
        "----------------------------------------------------------------------------------------------------\n",
        "shape = torch.Size([224, 224])\n",
        "min = 0.0, max = 1.0\n",
        "----------------------------------------------------------------------------------------------------\n",
        "```\n",
        "Notice that the displayed image might be different if you use a different input image.\n",
        "# ![Expected Output](./images/solution3.jpg)\n",
        "\n",
        "---\n",
        "\n",
        "**Interpreting Saliency Maps: What to Look For**\n",
        "\n",
        "When examining saliency maps, keep these interpretation guidelines in mind:\n",
        "\n",
        "1. **Focus of Attention**: Bright regions indicate pixels that strongly influence the model's prediction for the target class. For a \"rotten\" classification, you should see highlights on damaged or discolored areas of the fruit.\n",
        "\n",
        "2. **Expected Behavior**: A well-trained model should highlight relevant features (e.g., brown spots, mold, holes) rather than background or irrelevant texture. If the saliency focuses on the background, the model might be using spurious correlations.\n",
        "\n",
        "3. **Noise vs. Signal**: Saliency maps can be noisyâ€”you'll often see scattered bright pixels. The overall pattern matters more than individual pixels. Look for coherent regions rather than random noise.\n",
        "\n",
        "4. **Complementary to CAM**: Saliency maps show pixel-level sensitivity (fine-grained), while CAM shows region-level importance (coarse-grained). Use both together for complete understandingâ€”saliency tells you *exactly which pixels*, CAM tells you *which general areas*.\n",
        "\n",
        "5. **Limitations**: \n",
        "   - Saliency maps show sensitivity, not necessarily causation\n",
        "   - Different classes may produce different patterns for the same image\n",
        "   - Sharp edges often appear salient even if not semantically important\n",
        "\n",
        "**Try this:** Compare saliency maps for class 0 (fresh) vs class 1 (rotten) on the same image to see what distinguishes them!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# Test your code!\n",
        "\n",
        "unittests.exercise_3(saliency_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='7'></a>\n",
        "## 7 - Simplified Class Activation Map (CAM)\n",
        "\n",
        "In this exercise, you will implement a compact version of Class Activation Mapping to highlight which regions of an image most support a chosen class score. Instead of gradients, you will combine the final convolutional feature maps with the corresponding row of the fully connected layer to produce a class-specific heatmap that you can overlay on the input.\n",
        "\n",
        "**Why \"Simplified\" CAM?**  \n",
        "This implementation is called \"simplified\" because it assumes the model architecture uses Global Average Pooling (GAP) followed by a fully connected layer, which is the case for our ResNet-50-based FruitGuard Pro model. The original CAM technique relies on this specific architecture pattern. For models with different architectures, **Grad-CAM** (Gradient-weighted Class Activation Mapping) is a more general approach that uses gradients instead of direct weight mapping and works with any CNN architecture. Both techniques produce similar visualizations, but Grad-CAM is more flexible and doesn't require the GAPâ†’FC pattern.\n",
        "\n",
        "<a id='ex04'></a>\n",
        "### Exercise 4\n",
        "\n",
        "Implement the ```simplified_cam``` function:\n",
        "- Capture the final convolutional feature maps by attaching a forward hook to the last conv layer of the backbone.\n",
        "- Run a forward pass to fill a holder with those feature maps, then remove the hook.\n",
        "- Fetch the weight vector from the classification layer for `class_idx`.\n",
        "- Compute a weighted sum across channels (one weight per channel) to form the raw activation map.\n",
        "- Keep only positive evidence with a ReLU, then scale the map to the [0, 1] range.\n",
        "- Upsample the map to the input image size and return the 2-D heatmap tensor.\n",
        "\n",
        "By completing this, you will produce a clear, region-level view of the evidence the model uses for a specific class, complementing the pixel-level sensitivity you obtained with saliency maps.\n",
        "\n",
        "<details>\n",
        "<summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
        "\n",
        "#### 1) Implement the aux function save_fmap\n",
        "Detach the output and stor it in the feature map holder\n",
        "```python\n",
        "def save_fmap(_, _, output):\n",
        "    fmap_holder[\"feat\"] = output.detach()\n",
        "```\n",
        "\n",
        "#### 2) Register forward hooks and populate them wit a forward pass\n",
        "```python\n",
        "hook = model.layer4[-1].conv3.register_forward_hook(save_fmap)\n",
        "# Forward pass\n",
        "with torch.no_grad():\n",
        "    _ = model(image_tensor)\n",
        "```\n",
        "\n",
        "#### 3) Retrieve features and weighted sum over channels\n",
        "```python\n",
        "feats = fmap_holder[\"feat\"]              \n",
        "weight_vec = model.fc.weight[class_idx]  \n",
        "\n",
        "# Weighted sum over channels\n",
        "cam = torch.einsum(\"c,chw->hw\", weight_vec, feats.squeeze(0)) # Weighted sum over channels\n",
        "```\n",
        "\n",
        "#### 4) Normalize the positive evidence to [0, 1]\n",
        "```python\n",
        "cam = F.relu(cam) \n",
        "cam = (cam - cam.min()) / (cam.max() + 1e-8)\n",
        "```\n",
        "\n",
        "#### 5) Upsample cam\n",
        "```python\n",
        "H, W = image_tensor.shape[2:]\n",
        "cam_up = F.interpolate( \n",
        "    cam.unsqueeze(0).unsqueeze(0),\n",
        "    size=(H, W), \n",
        "    mode=\"bilinear\",\n",
        "    align_corners=False,\n",
        ")[0, 0]\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "id": "UrBkFmHsdq1I",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: simplified_cam\n",
        "\n",
        "def simplified_cam(model, image_tensor, class_idx):\n",
        "    \"\"\"\n",
        "    Generate a simplified Class Activation Map (CAM) for a single image.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : torch.nn.Module\n",
        "        A trained ResNet-50-style network (final conv features â†’ `model.fc`).\n",
        "    image_tensor : torch.Tensor\n",
        "        Input tensor of shape (1, 3, H, W) already normalised as the model expects.\n",
        "    class_idx : int\n",
        "        Index of the target class logit.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "        2-D CAM heat-map scaled to [0,â€‰1] with shape (H, W).\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # Capture final convolutional feature maps\n",
        "    fmap_holder = {} # Initialize an empty dictionary to store the feature maps\n",
        "\n",
        "    def save_fmap(_, __, output): # Aux function to save the feature maps\n",
        "        fmap_holder[\"feat\"] = output.detach()\n",
        "\n",
        "    hook = model.layer4.register_forward_hook(save_fmap) # Register forward hook for the last convolution layer\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        _ = model(image_tensor) # Forward pass the image through the model\n",
        "\n",
        "    hook.remove() # Remove the forward hook\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # Retrieve features and FC weights\n",
        "    feats = fmap_holder[\"feat\"] # Get the captured feature maps from the fmap_holder\n",
        "    weight_vec = model.fc.weight[class_idx] # Get the weights of the fully connected layer for the target class\n",
        "\n",
        "    # Weighted sum over channels\n",
        "    # 'c': weight per channel, 'chw': feature maps (c=channels, h=height, w=width)\n",
        "    # compute activation map summed over channels\n",
        "    cam = torch.einsum(\"c,chw->hw\", weight_vec, feats.squeeze(0)) # Weighted sum over channels\n",
        "\n",
        "    # Keep positive evidence, normalise to [0,1]\n",
        "    cam = torch.clamp(cam, min=0.0) # Keep positive evidence 0 if negative, same if positive\n",
        "    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8) # Normalize to [0,1] (cam - min) / (max - min + epsilon)\n",
        "\n",
        "    # Upsample to input size\n",
        "    H, W = image_tensor.shape[2], image_tensor.shape[3] # Get the height and width of the input image\n",
        "    cam_up = F.interpolate( # Upsample the activation map to the input size \n",
        "        cam.unsqueeze(0).unsqueeze(0), # Add batch and channel dimensions\n",
        "        size=(H, W), # Set the size of the output activation map\n",
        "        mode=\"bilinear\", # Use bilinear interpolation\n",
        "        align_corners=False, # Set align_corners to False\n",
        "    )[0, 0]\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return cam_up.cpu().detach()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To verify your code, run the following cells!\n",
        "\n",
        "You can test your code with different images changing the `image_path` variable. Some values that can be used are:\n",
        "\n",
        "- ./data/fruit_subset/Apple__Rotten/rottenApple (482).jpg\n",
        "- ./data/fruit_subset/Apple__Rotten/rottenApple (522).jpg\n",
        "- ./data/fruit_subset/Mango__Healthy/35.jpg\n",
        "- ./data/fruit_subset/Mango__Rotten/163.jpg\n",
        "- ./data/fruit_subset/Tomato__Healthy/freshTomato (12).jpg\n",
        "- ./data/fruit_subset/Tomato__Rotten/rottenTomato (9).jpg\n",
        "\n",
        "You can find more images to test on in the folder ./data/fruit_subset/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "image_path = \"./data/fruit_subset/Apple__Rotten/rottenApple (1).jpg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "# Verify the CAM\n",
        "\n",
        "# Load image tensor and preprocess\n",
        "image_tensor = helper_utils.preprocess_image(image_path)\n",
        "\n",
        "# Compute CAM\n",
        "cam_up = simplified_cam(net, image_tensor, class_idx)\n",
        "\n",
        "print(\"Shape and range of the CAM:\")\n",
        "print(\"-\"*100)\n",
        "print(f\"shape = {cam_up.shape}\")\n",
        "print(f\"min = {cam_up.min()}, max = {cam_up.max()}\")\n",
        "print(\"-\"*100)\n",
        "\n",
        "helper_utils.display_cam(image_tensor, cam_up)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Expected Output:\n",
        "```\n",
        "Shape and range of the CAM:\n",
        "----------------------------------------------------------------------------------------------------\n",
        "shape = torch.Size([224, 224])\n",
        "min = 0.0, max = 0.9915805459022522\n",
        "----------------------------------------------------------------------------------------------------\n",
        "```\n",
        "Notice that the displayed image might be different if you use a different input image.\n",
        "# ![Expected Output](./images/solution4.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "# Test your code!\n",
        "unittests.exercise_4(simplified_cam)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id='8'></a>\n",
        "## 8 - Comparison of Interpretability Techniques\n",
        "\n",
        "You've now implemented three powerful techniques for understanding model behavior. Here's how they compare:\n",
        "\n",
        "| **Aspect** | **Feature Hierarchy** | **Saliency Maps** | **Class Activation Maps (CAM)** |\n",
        "|------------|----------------------|-------------------|--------------------------------|\n",
        "| **What it shows** | Evolution of features across layers | Pixel-level gradient sensitivity | Region-level class evidence |\n",
        "| **Granularity** | Layer-by-layer channel responses | Individual pixels | Coarse spatial regions |\n",
        "| **Computation** | Forward hooks only | Backpropagation to input | Forward hooks + FC weights |\n",
        "| **Speed** | Fast âš¡ | Medium â±ï¸ | Fast âš¡ |\n",
        "| **Use case** | Understanding feature learning | Finding important pixels | Localizing class evidence |\n",
        "| **Output** | Multiple feature maps | Single heatmap per class | Single heatmap per class |\n",
        "| **Requires gradients?** | No | Yes | No |\n",
        "| **Architecture constraints** | None | None | Requires GAPâ†’FC (or use Grad-CAM) |\n",
        "| **Best for** | Model debugging, visualization | Adversarial analysis, fine details | Model trustworthiness, localization |\n",
        "\n",
        "**When to use each:**\n",
        "\n",
        "- **Feature Hierarchy**: Use when you want to understand *how* a model learns, what patterns emerge at different depths, or debug training issues.\n",
        "  \n",
        "- **Saliency Maps**: Use when you need *precise pixel-level explanations*, are concerned about adversarial vulnerabilities, or want to understand fine-grained sensitivities.\n",
        "  \n",
        "- **CAM/Grad-CAM**: Use when you need *human-interpretable region highlights*, want to verify the model is looking at the right object, or need coarse localization for weakly-supervised tasks.\n",
        "\n",
        "**Pro Tip**: Combine multiple techniques! For example:\n",
        "1. Use **CAM** to verify the model focuses on the fruit (not background)\n",
        "2. Use **Saliency** to see exactly which pixels (e.g., specific spots, edges) drive the decision\n",
        "3. Use **Feature Hierarchy** to understand what low/mid/high-level features the model learned\n",
        "\n",
        "This multi-method approach gives you comprehensive understanding of your model's behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Submission Note\n",
        "\n",
        "Congratulations! You've completed the final graded exercise of this assignment.\n",
        "\n",
        "If you've successfully passed all the unit tests above, you've completed the core requirements of this assignment. Feel free to [submit](#submission) your work now. The grading process runs in the background, so it will not disrupt your progress and you can continue on with the rest of the material.\n",
        "\n",
        "**ðŸš¨ IMPORTANT NOTE** If you have passed all tests within the notebook, but the autograder shows a system error after you submit your work:\n",
        "\n",
        "<div style=\"background-color: #1C1C1E; border: 1px solid #444444; color: #FFFFFF; padding: 15px; border-radius: 5px;\">\n",
        "    <p><strong>Grader Error: Grader feedback not found</strong></p>\n",
        "    <p>Autograder failed to produce the feedback...</p>\n",
        "</div>\n",
        "<br>\n",
        "\n",
        "This is typically a temporary system glitch. The most common solution is to resubmit your assignment, as this often resolves the problem. Occasionally, it may be necessary to resubmit more than once. \n",
        ">\n",
        "If the error persists, please reach out for support in the [DeepLearning.AI Community Forum](https://community.deeplearning.ai/c/course-q-a/pytorch-for-developers/pytorch-advanced-architectures-and-deployment/562).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='9'></a>\n",
        "## 9 - Optional: Text-to-Image Generation\n",
        "\n",
        "**From Interpretation to Creation**\n",
        "\n",
        "So far, you've been analyzing how models *interpret* existing imagesâ€”visualizing feature hierarchies, identifying important pixels with saliency maps, and highlighting class-specific regions with CAM. These techniques help you understand what a trained model \"sees\" and how it makes decisions.\n",
        "\n",
        "Now, we shift perspective: instead of analyzing how models process images, you'll explore how they can *create* new ones from scratch. This transition from **interpretation to generation** demonstrates the full power of deep learningâ€”models that not only understand visual patterns but can also synthesize new images based on learned representations.\n",
        "\n",
        "In this optional section, you'll work with **Stable Diffusion**, a state-of-the-art text-to-image model that uses diffusion processes to gradually refine random noise into coherent images guided by text prompts. This connects back to the interpretability work you've done: understanding how models process features helps you better control what they generate.\n",
        "\n",
        "---\n",
        "\n",
        "<a id='9-1'></a>\n",
        "### 9.1 - Optional Exercise: Text-to-Image Generation\n",
        "\n",
        "In this optional exercise, you will implement `generate_sd_image` to synthesize an image from a text prompt using a pre-trained Stable Diffusion pipeline. The model will be loaded from a **local `.models` folder** to avoid downloading from the internet each time. This gives you practice with prompt design, negative prompts, deterministic sampling with seeds, and saving outputs in a clear directory structure.\n",
        "\n",
        "### What you will do\n",
        "- Select the compute device and data type based on CUDA availability.\n",
        "- Load the Stable Diffusion pipeline using a local cache directory (`.models`).\n",
        "- Create a `torch.Generator` on the chosen device and set a manual seed for reproducible outputs.\n",
        "- Call the pipeline with `prompt`, `negative_prompt`, and `num_inference_steps` to generate one image.\n",
        "- Build a save path of the form `synthetic/<slugified_prompt>/img_<seed>.png`, create the folder if it does not exist, and save the PNG.\n",
        "- Return the generated `PIL.Image.Image` and print where it was saved.\n",
        "\n",
        "<details>\n",
        "<summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
        "\n",
        "#### 1) Set the appropriate device and data type\n",
        "Set `device = \"cuda\" if torch.cuda.is_available() else \"cpu\"` and choose `dtype = torch.float16` on CUDA, otherwise `torch.float32`.\n",
        "\n",
        "#### 2) Load the pipeline with cache_dir:\n",
        "```python\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-2-base\",  # Model ID\n",
        "    torch_dtype=dtype,\n",
        "    cache_dir=\".models\"  # Use local .models folder as cache\n",
        ").to(device)\n",
        "```\n",
        "\n",
        "#### 3) Create a seeded generator:\n",
        "```python\n",
        "generator = torch.Generator(device).manual_seed(seed)\n",
        "```\n",
        "\n",
        "#### 4) Generate one image:\n",
        "```python\n",
        "image = pipe(\n",
        "    prompt=prompt,\n",
        "    negative_prompt=negative_prompt,\n",
        "    num_inference_steps=steps,\n",
        "    generator=generator,\n",
        ").images[0]\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": true,
        "id": "SxWH7IkEdq7K"
      },
      "outputs": [],
      "source": [
        "# EDITABLE CELL\n",
        "\n",
        "def generate_sd_image(prompt, negative_prompt, seed, steps, model_id=\"stabilityai/stable-diffusion-2-base\", save_dir=\"synthetic\"):\n",
        "    \"\"\"\n",
        "    Generate a single image with Stable Diffusion using a local cache.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    prompt : str\n",
        "        Positive text prompt.\n",
        "    negative_prompt : str | None\n",
        "        Negative prompt; pass None for none.\n",
        "    seed : int\n",
        "        Random seed for deterministic output.\n",
        "    steps : int\n",
        "        Number of inference steps.\n",
        "    model_id : str\n",
        "        HuggingFace model ID (default: \"stabilityai/stable-diffusion-2-base\").\n",
        "    save_dir : str\n",
        "        Root directory to save the PNG.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    PIL.Image.Image\n",
        "        The generated image.\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ###\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "    # Load the pipeline with cache_dir to use local .models folder\n",
        "    pipe = StableDiffusionPipeline.from_pretrained( \n",
        "        model_id,\n",
        "        torch_dtype=dtype,\n",
        "        cache_dir=\"./.models\"\n",
        "    ).to(device)\n",
        "\n",
        "    generator = torch.Generator(device).manual_seed(seed) # Get the generator use manual seed and torch.Generator in the device\n",
        "    \n",
        "    # Generate the image use a pipeline\n",
        "    image = pipe( \n",
        "        prompt=prompt,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_inference_steps=steps,\n",
        "        generator=generator\n",
        "    ).images[0]\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Build path: synthetic/<slugified_prompt>/img_seed.png\n",
        "    slug = \"_\".join(prompt.lower().split()[:3]) # Slugify the prompt\n",
        "    out_dir = Path(save_dir) / slug # Get the output directory\n",
        "    out_dir.mkdir(parents=True, exist_ok=True) # Create the output directory\n",
        "    out_path = out_dir / f\"img_{seed}.png\" # Get the output path\n",
        "    image.save(out_path)# Save the image\n",
        "\n",
        "    print(f\"Image saved to {out_path}\") # Print the output path\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary><b><font color=\"green\">Solution to the exercise.</font></b></summary>\n",
        "\n",
        "In case you want to see the solution:\n",
        "\n",
        "```python\n",
        "\n",
        "def generate_sd_image(prompt, negative_prompt, seed, steps, model_id=\"stabilityai/stable-diffusion-2-base\", save_dir=\"synthetic\"):\n",
        "    ### START CODE HERE ###\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
        "\n",
        "    # Load the pipeline with cache_dir to use local .models folder\n",
        "    pipe = StableDiffusionPipeline.from_pretrained( \n",
        "        model_id, # Set the model ID\n",
        "        torch_dtype=dtype, # Set the data type\n",
        "        cache_dir=\".models\" # Use .models folder as cache directory\n",
        "    ).to(device)\n",
        "\n",
        "    generator = torch.Generator(device).manual_seed(seed) # Get the generator use manual seed and torch.Generator in the device\n",
        "\n",
        "    # Generate the image use a pipeline\n",
        "    image = pipe( \n",
        "        prompt=prompt, # Set the prompt\n",
        "        negative_prompt=negative_prompt, # Set the negative prompt\n",
        "        num_inference_steps=steps, # Set the number of inference steps\n",
        "        generator=generator, # Set the generator\n",
        "    ).images[0]\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Build path: synthetic/<slugified_prompt>/img_seed.png\n",
        "    slug = \"_\".join(prompt.lower().split()[:3]) # Slugify the prompt\n",
        "    out_dir = Path(save_dir) / slug # Get the output directory\n",
        "    out_dir.mkdir(parents=True, exist_ok=True) # Create the output directory\n",
        "    out_path = out_dir / f\"img_{seed}.png\" # Get the output path\n",
        "    image.save(out_path)# Save the image\n",
        "\n",
        "    print(f\"Image saved to {out_path}\") # Print the output path\n",
        "\n",
        "    return image\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "# Verify your code here!!\n",
        "\n",
        "img = generate_sd_image(\"A mango with a small hole made by a worm in the middle.\", \"Fresh, intact.\", 42, 50)\n",
        "\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Expected Output:\n",
        "\n",
        "Your generated image should look like this:\n",
        "\n",
        "![Expected output image](./images/img_42.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='9-2'></a>\n",
        "### 9.2 - Denoising Timelapse\n",
        "\n",
        "In this exercise, you will implement `denoising_movie` to capture snapshots of the denoising process at specific steps and assemble them into a 2Ã—2 grid. This helps you see how noisy latents gradually turn into a coherent image as the sampler progresses. You will intercept intermediate latents via a callback, decode them to RGB with the VAE, collect the frames in order, and save a simple timelapse grid.\n",
        "\n",
        "### What you will do\n",
        "- Choose device and data type based on CUDA availability.\n",
        "- Load a pre-trained Stable Diffusion pipeline using the `.models` cache directory.\n",
        "- Create a seeded `torch.Generator` for deterministic sampling.\n",
        "- Register a step-end callback that:\n",
        "  - Checks if the current `step_idx` is in `capture_steps`.\n",
        "  - Extracts `latents` from `callback_kwargs`.\n",
        "  - Decodes latents to an image with the pipeline VAE and postprocesses to PIL.\n",
        "  - Stores each captured frame in a dictionary keyed by `step_idx`.\n",
        "- Run the pipeline with the callback enabled and collect the frames.\n",
        "- Order frames to match `capture_steps`, compose a 2Ã—2 grid, save it, and return the frames.\n",
        "\n",
        "<details>\n",
        "<summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
        "\n",
        "#### 1) Device and dtype\n",
        "```python\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
        "```\n",
        "\n",
        "#### 2) Load the Pipeline with cache_dir\n",
        "```python\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    model_id, \n",
        "    torch_dtype=dtype,\n",
        "    cache_dir=\".models\"\n",
        ").to(device)\n",
        "```\n",
        "\n",
        "#### 3) Prepare a frames holder and callback\n",
        "```python\n",
        "frames: dict[int, Image.Image] = {}\n",
        "\n",
        "def grab_frame(pipeline, step_idx, timestep, callback_kwargs):\n",
        "    if step_idx in capture_steps:\n",
        "        latents = callback_kwargs[\"latents\"]\n",
        "        with torch.no_grad():\n",
        "            img = pipe.vae.decode(\n",
        "                latents / pipe.vae.config.scaling_factor,\n",
        "                return_dict=False\n",
        "            )[0]\n",
        "        pil = pipe.image_processor.postprocess(img, output_type=\"pil\")[0]\n",
        "        frames[step_idx] = pil\n",
        "    return callback_kwargs\n",
        "```\n",
        "\n",
        "#### 4) Torch generator and run the pipe\n",
        "```python\n",
        "generator = torch.Generator(device).manual_seed(seed)\n",
        "\n",
        "_ = pipe(\n",
        "    prompt=prompt,\n",
        "    num_inference_steps=steps,\n",
        "    generator=generator,\n",
        "    callback_on_step_end=grab_frame,\n",
        ")\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": true,
        "id": "QPeQL2WCh0So"
      },
      "outputs": [],
      "source": [
        "# EDITABLE CELL\n",
        "\n",
        "def denoising_movie(prompt, seed, steps, capture_steps, model_id=\"stabilityai/stable-diffusion-2-base\", save_grid_path=\"timelapse.png\"):\n",
        "    \"\"\"\n",
        "    Capture intermediate denoising frames and assemble a 2Ã—2 grid PNG.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    prompt : str\n",
        "        Positive text prompt.\n",
        "    seed : int\n",
        "        Random seed for deterministic sampling.\n",
        "    steps : int\n",
        "        Total number of inference steps.\n",
        "    capture_steps : list[int]\n",
        "        Which step indices to capture (e.g. [0,10,20,30]).\n",
        "    model_id : str\n",
        "        HuggingFace model ID (default: \"stabilityai/stable-diffusion-2-base\").\n",
        "    save_grid_path : str\n",
        "        Output path for the grid PNG.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list[PIL.Image.Image]\n",
        "        List of captured frames in the order of `capture_steps`.\n",
        "    \"\"\"\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    pipe = StableDiffusionPipeline.from_pretrained( \n",
        "        None, # Set the model ID\n",
        "        None # Set the data type\n",
        "        None # Set variant=\"fp16\"\n",
        "        None # Set cache_dir=\".models\"\n",
        "    ).to(device)\n",
        "\n",
        "    frames = {} # Initialize an empty dictionary to store the frames\n",
        "\n",
        "    def grab_frame(pipeline, step_idx, timestep, callback_kwargs): # Aux function to grab the frame and decode the latents\n",
        "        if None: # Check if the step index is in the capture steps\n",
        "            latents = None # Get the latents\n",
        "            with torch.no_grad():\n",
        "                img = None ( # Decode the latents space from vae in pipe\n",
        "                    None, # Divide the latents by the scaling factor\n",
        "                      None # Return the decoded image\n",
        "                )[0]\n",
        "            pil = None # Postprocess the image set the output type to pil\n",
        "            frames[step_idx] = None # Store the frame in the dictionary\n",
        "        return None # Return the callback kwargs\n",
        "\n",
        "    generator = None # Get the generator use manual seed and torch.Generator in the device\n",
        "\n",
        "    _ = pipe( \n",
        "        None, # Set the prompt\n",
        "        None, # Set the number of inference steps\n",
        "        None, # Set the generator\n",
        "        None, # Set the callback on step end\n",
        "    )\n",
        "\n",
        "    # Order frames according to capture_steps\n",
        "    ordered_frames = None # Order the frames according to the capture steps\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Build grid\n",
        "    w, h = ordered_frames[0].size # Get the width and height of the first frame\n",
        "    grid = Image.new(\"RGB\", (w * 2, h * 2)) # Create a new image with the width and height of the first frame\n",
        "    for idx, frame in enumerate(ordered_frames): \n",
        "        row, col = divmod(idx, 2) # Get the row and column of the frame\n",
        "        grid.paste(frame, (col * w, row * h)) # Paste the frame in the grid\n",
        "\n",
        "    grid.save(save_grid_path) # Save the grid\n",
        "    print(f\"Timelapse grid saved to {save_grid_path}\") # Print the output path\n",
        "\n",
        "    return ordered_frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary><b><font color=\"green\">Solution to the exercise</font></b></summary>\n",
        "\n",
        "```python\n",
        "def denoising_movie(prompt, seed, steps, capture_steps, model_id=\"stabilityai/stable-diffusion-2-base\", save_grid_path=\"timelapse.png\"):\n",
        "\n",
        "   ### START CODE HERE ###\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
        "\n",
        "    pipe = StableDiffusionPipeline.from_pretrained( \n",
        "        model_id, # Set the model ID\n",
        "        torch_dtype=dtype, # Set the data type\n",
        "        cache_dir=\".models\" # Use .models folder as cache directory\n",
        "    ).to(device)\n",
        "\n",
        "    frames: dict[int, Image.Image] = {} # Initialize an empty dictionary to store the frames\n",
        "\n",
        "    def grab_frame(pipeline, step_idx, timestep, callback_kwargs): # Aux function to grab the frame and decode the latents\n",
        "        if step_idx in capture_steps: # Check if the step index is in the capture steps\n",
        "            latents = callback_kwargs[\"latents\"] # Get the latents vector\n",
        "            with torch.no_grad():\n",
        "                img = pipe.vae.decode( # Decode the latents space from the vae in the pipeline\n",
        "                    latents / pipe.vae.config.scaling_factor, # Divide the latents by the scaling factor\n",
        "                      return_dict=False # Return the decoded image \n",
        "                )[0]\n",
        "            pil = pipe.image_processor.postprocess(img, output_type=\"pil\")[0] # Postprocess the image set the output type to pil\n",
        "            frames[step_idx] = pil # Store the frame in the dictionary  \n",
        "        return callback_kwargs # Return the callback kwargs\n",
        "\n",
        "    generator = torch.Generator(device).manual_seed(seed) # Get the generator use manual seed and torch.Generator in the device\n",
        "\n",
        "    _ = pipe(\n",
        "        prompt=prompt, # Set the prompt\n",
        "        num_inference_steps=steps,  # Set the number of inference steps\n",
        "        generator=generator,  # Set the generator\n",
        "        callback_on_step_end=grab_frame,  # Set the callback on step end\n",
        "    )\n",
        "\n",
        "    # Order frames according to capture_steps\n",
        "    ordered_frames = [frames[s] for s in capture_steps] # Order the frames according to the capture steps\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Build grid\n",
        "    w, h = ordered_frames[0].size # Get the width and height of the first frame\n",
        "    grid = Image.new(\"RGB\", (w * 2, h * 2)) # Create a new image with the width and height of the first frame\n",
        "    for idx, frame in enumerate(ordered_frames): \n",
        "        row, col = divmod(idx, 2) # Get the row and column of the frame\n",
        "        grid.paste(frame, (col * w, row * h)) # Paste the frame in the grid\n",
        "\n",
        "    grid.save(save_grid_path) # Save the grid\n",
        "    print(f\"Timelapse grid saved to {save_grid_path}\") # Print the output path\n",
        "\n",
        "    return ordered_frames\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "outputs": [],
      "source": [
        "# Verify your implementation\n",
        "\n",
        "ordered_frames = denoising_movie(\"A healthy mango.\", 42, 50, [0, 10, 20, 30, 50])\n",
        "\n",
        "grid_image = plt.imread(\"timelapse.png\")\n",
        "plt.imshow(grid_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Expected Output:\n",
        "\n",
        "Your generated image should look like this:\n",
        "\n",
        "![Expected output image](./images/timelapse.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "You have traced the journey of an image through different stages â€” from being transformed into layered feature representations, to revealing which parts of it shaped the modelâ€™s decision, and finally to generating entirely new visuals guided by patterns the model has learned. Along the way, you used tools that operate at different scales, from fine-grained pixel sensitivity to broader region-level focus, and eventually to structured creation of images from random noise. These skills give you a balanced toolkit: one that not only helps you interpret and refine models, but also lets you apply them in creative and purposeful ways.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "grader_version": "1",
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
