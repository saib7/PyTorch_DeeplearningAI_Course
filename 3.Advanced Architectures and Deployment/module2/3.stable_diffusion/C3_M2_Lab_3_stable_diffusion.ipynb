{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3a08f7d-2ac7-44f0-b2c2-87edb6ba2d61",
   "metadata": {},
   "source": [
    "# Stable Diffusion: From Image Classification to Generative Modeling\n",
    "\n",
    "In this lab, you'll explore diffusion models for image generation. Diffusion models learn how to turn random noise into images by gradually \"denoising\" it through a learned process. When trained with images and their associated text prompts, these models can generate new images simply by describing what you'd like to see.\n",
    "\n",
    "You'll use the Stable Diffusion 2 model and the Hugging Face diffusers library to:\n",
    "\n",
    "- Generate images from text prompts\n",
    "- Control generation with deterministic seeds\n",
    "- Experiment with \"negative prompts\"\n",
    "- Learn about properties and parameters in the pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7f2ffe-7d83-44c5-af12-d4202cd0f23f",
   "metadata": {},
   "source": [
    "## 1 - Introduction\n",
    "\n",
    "### 1.1 Loading the libraries\n",
    "\n",
    "Run the cell below to load the necessary libraries for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6de6b25-9372-432c-8e63-156e9859dec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53be9e07-b346-498f-9269-5eacbb4f9ca4",
   "metadata": {},
   "source": [
    "### 1.2 Discriminative vs. Generative Approaches in Deep Learning\n",
    "\n",
    "Up to this point in the course, you have engaged primarily with **discriminative models**—deep learning systems developed to recognize and categorize images (e.g., via convolutional neural networks). With tools such as **saliency maps** and **class activation maps**, you have explored how these models identify salient regions that inform their decisions.\n",
    "\n",
    "In this lab, you will shift focus to **generative models**: neural networks capable of synthesizing new, realistic images from scratch by learning the underlying distribution of the training data.\n",
    "\n",
    "For example, consider describing a novel scenario through a prompt such as:\n",
    "> \"A puppy riding a skateboard in Times Square.\"\n",
    "\n",
    "With recent advancements in generative AI, it is possible for a model to produce images matching this description, even though the process begins not with a recognizable picture, but with random noise.\n",
    "\n",
    "Let's load the model and check its output. You will go deeper into the noise details later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2820561d-480e-42fb-b8b7-d64a429274b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model and cache directory\n",
    "repo_id = \"stabilityai/stable-diffusion-2-base\"\n",
    "cache_dir = os.path.join(os.getcwd(), './models')\n",
    "model_cache_dir_name = f\"models--{repo_id.replace('/', '--')}\"\n",
    "model_cache_path = os.path.join(cache_dir, model_cache_dir_name)\n",
    "\n",
    "# Check if this directory exists. If it does, the model is cached.\n",
    "# Note: This cache detection logic is repeated for each model in this notebook.\n",
    "# In production code, you might refactor this into a reusable helper function!\n",
    "model_is_cached = os.path.isdir(model_cache_path)\n",
    "\n",
    "if model_is_cached:\n",
    "    print(\"Model cache found.\")\n",
    "    print(\"Setting local_files_only=True.\")\n",
    "else:\n",
    "    print(\"Model cache not found\")\n",
    "    print(\"Setting local_files_only=False (will attempt download).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55687105-884f-464c-b826-afaa3236307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the pipeline\n",
    "# Note: If you encounter CUDA out of memory errors, you can reduce memory usage by:\n",
    "# 1. Using torch.float32 instead of torch.float16 (though slower)\n",
    "# 2. Reducing num_inference_steps\n",
    "# 3. Setting enable_attention_slicing: pipe.enable_attention_slicing()\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    repo_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    variant=\"fp16\", \n",
    "    cache_dir=cache_dir,\n",
    "    local_files_only=model_is_cached\n",
    ")\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "generator = torch.Generator(device=device).manual_seed(42)\n",
    "\n",
    "pipe = pipe.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18134445-337c-493c-bcdb-d0ce30bcd4bc",
   "metadata": {},
   "source": [
    "> **Note on PyTorch Generators and Randomness**\n",
    ">\n",
    "> PyTorch `Generator` objects provide a way to produce sequences of random numbers in a **deterministic** (\"non-random\") order—if you set the same seed, you’ll always get the same sequence of numbers *from that generator*.  \n",
    ">\n",
    "> However, every time you generate an image, the generator “rolls” (uses up) some of its numbers depending on how many random draws the model needs. If you call the pipeline multiple times in a row using the **same generator**, each call will consume random numbers in sequence.  \n",
    ">\n",
    "> As a result, **even with the same prompt and generator/seed**, you may get different images on subsequent calls—unless you re-initialize or reset the generator’s seed before each run.  \n",
    ">\n",
    "> To consistently reproduce the same image for a prompt, always re-create or re-seed your generator in the exact same way before each generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc259476-5573-4081-86dd-bf52583bca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A puppy riding a skateboard in Times Square.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c307dd37-dc98-4fb4-9f42-f2c658d0a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 40 steps provides a good balance between quality and speed\n",
    "# (Typical range: 20-50 for fast generation, 50-150 for high quality)\n",
    "images = pipe(\n",
    "    prompt,                # What you want the model to create\n",
    "    num_inference_steps=40, # How many denoising steps to use (more steps = more detail/compute)\n",
    "    generator=generator    # Ensures reproducible noise/randomness\n",
    ").images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23736f7e-05ce-4307-9618-2def5944c0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdc1b38-a11b-415c-9cb7-e926116414c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = pipe(\n",
    "    prompt,                # What you want the model to create\n",
    "    num_inference_steps=40, # How many denoising steps to use (more steps = more detail/compute)\n",
    "    generator=generator    # Ensures reproducible noise/randomness\n",
    ").images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cd3e1a-5c0c-438e-9953-0f0ef0396c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c158858-58dc-45ad-9924-f9b1ca5f1069",
   "metadata": {},
   "source": [
    "Notice how the images are different, even with the same prompt! This is because the generator is \"using\" its pseudo-random numbers. However, if you re-define the generator every time prior to calling the pipeline, then the results will be **the same**. Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7776a84d-6928-41db-8f01-74eeab379917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run\n",
    "generator = torch.Generator(device=device).manual_seed(42)\n",
    "images = pipe(\n",
    "    prompt,                # What you want the model to create\n",
    "    num_inference_steps=40, # How many denoising steps to use (more steps = more detail/compute)\n",
    "    generator=generator    # Ensures reproducible noise/randomness\n",
    ").images\n",
    "images[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121d7d99-264c-4ace-a185-317da2707eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second run\n",
    "generator = torch.Generator(device=device).manual_seed(42)\n",
    "images = pipe(\n",
    "    prompt,                # What you want the model to create\n",
    "    num_inference_steps=40, # How many denoising steps to use (more steps = more detail/compute)\n",
    "    generator=generator    # Ensures reproducible noise/randomness\n",
    ").images\n",
    "images[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84b0ed1-77f6-4813-921d-8715abf8acb9",
   "metadata": {},
   "source": [
    "**Explanation of `.images[0]`:**\n",
    "\n",
    "Often, image generation pipelines can produce more than one image at a time (by passing, for example, num_images_per_prompt=3), so they always return a list for consistency, even if you only generate one image. Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafca5d7-beb6-4538-8ddf-58c2c6b92d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = pipe(\n",
    "    prompt,                # What you want the model to create\n",
    "    num_inference_steps=40, # How many denoising steps to use (more steps = more detail/compute)\n",
    "    generator=generator,   # Ensures reproducible noise/randomness\n",
    "    num_images_per_prompt = 3\n",
    ").images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dff4b6-3291-44d2-beda-859e9b208d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a row of 3 subplots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15,5))\n",
    "\n",
    "# Display each image\n",
    "for idx, img in enumerate(image_list):\n",
    "    axs[idx].imshow(img)\n",
    "    axs[idx].axis('off')  # Hide axis\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2837d07f-cfac-4468-a5db-f6414ee26f1e",
   "metadata": {},
   "source": [
    "### 1.3 Parameters in Diffusion Models\n",
    "\n",
    "\n",
    "#### 1.3.1 `num_inference_steps` \n",
    "This parameter controls how many **denoising steps** the model takes. More steps typically yield more detailed and higher-quality images, but can take longer to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03ff607-717a-4c30-8afe-4e5c9a836386",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a cute dog with a red bandana, sitting in a lush park\"\n",
    "# Redefining the generator\n",
    "generator = torch.Generator(device=device).manual_seed(42)\n",
    "\n",
    "# Fewer steps (fast, less detail)\n",
    "image_fast = pipe(prompt, num_inference_steps=10, generator=generator).images[0]\n",
    "# Default/standard (balanced quality)\n",
    "\n",
    "image_standard = pipe(prompt, num_inference_steps=50, generator=generator).images[0]\n",
    "# More steps (slower, more detail)\n",
    "\n",
    "image_high_quality = pipe(prompt, num_inference_steps=200, generator=generator).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261e3c15-e98d-4f8b-9404-7462033c3ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [image_fast, image_standard, image_high_quality]\n",
    "titles = [\"10 steps (fast)\", \"50 steps (standard)\", \"200 steps (high quality)\"]\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15,5))\n",
    "for idx, (img, title) in enumerate(zip(images, titles)):\n",
    "    axs[idx].imshow(img)\n",
    "    axs[idx].set_title(title)\n",
    "    axs[idx].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef5b442-0279-4a17-80e7-a706c64eb79f",
   "metadata": {},
   "source": [
    "#### 1.3.2 `guidance_scale` (a.k.a. \"Classifier-Free Guidance\")\n",
    "\n",
    "This parameter controls how closely the generated image should follow your prompt.\n",
    "\n",
    "- **Lower values** (e.g., 5-7): Allow more creativity and variation, but may stray from your prompt\n",
    "- **Medium values** (e.g., 7-10): Balanced adherence to prompt (default is typically 7.5)\n",
    "- **Higher values** (e.g., 10-15): Force the image to match your prompt more literally\n",
    "- **Too high** (e.g., >20): Can make images look unnatural or oversaturated\n",
    "\n",
    "**Tip**: Start with 7.5 and adjust based on whether you want more creative freedom or stricter prompt adherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd13193c-4371-47ed-b824-27eb99f774da",
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance_scales = [5, 7.5, 12]\n",
    "images = []\n",
    "\n",
    "prompt = \"A surreal landscape with floating clocks, melting trees, and a purple sky, in the style of Salvador Dalí\"\n",
    "# Generate all images first (so the generator seed doesn't advance unpredictably)\n",
    "for gs in guidance_scales:\n",
    "    # It's important to re-create the generator for reproducibility\n",
    "    generator = torch.Generator(device=device).manual_seed(42)\n",
    "    img = pipe(prompt, generator=generator, guidance_scale=gs).images[0]\n",
    "    images.append(img)\n",
    "\n",
    "# Plot images side by side\n",
    "fig, axs = plt.subplots(1, len(images), figsize=(5 * len(images), 5))\n",
    "for idx, (img, gs) in enumerate(zip(images, guidance_scales)):\n",
    "    axs[idx].imshow(img)\n",
    "    axs[idx].set_title(f\"guidance_scale = {gs}\")\n",
    "    axs[idx].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6903fb0b-0fcf-4c4f-938c-7f0f960113dd",
   "metadata": {},
   "source": [
    "#### 1.3.3 `negative_prompt`\n",
    "\n",
    "This optional parameter tells the model what you do **NOT** want to see in the image.\n",
    "For example, if image generators often have trouble drawing hands, you can instruct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd621005-8e22-4523-b232-af058645150b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A group of realistic teddy bears eating pizza at a birthday party\"\n",
    "negative_prompt = \"pepperoni, deformed hands, extra limbs, blurry, out of focus, text, watermark\"\n",
    "\n",
    "# Generate without negative prompt\n",
    "generator = torch.Generator(device=device).manual_seed(42)\n",
    "result_no_neg = pipe(prompt, generator=generator, num_inference_steps=30)\n",
    "\n",
    "# Re-seed to ensure the same starting noise for fair comparison\n",
    "generator = torch.Generator(device=device).manual_seed(42)\n",
    "result_with_neg = pipe(prompt, negative_prompt=negative_prompt, generator=generator, num_inference_steps=30)\n",
    "\n",
    "imgs = [result_no_neg.images[0], result_with_neg.images[0]]\n",
    "titles = ['Without negative_prompt', 'With negative_prompt']\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "for i, (img, title) in enumerate(zip(imgs, titles)):\n",
    "    axs[i].imshow(img)\n",
    "    axs[i].set_title(title)\n",
    "    axs[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2392d6-468c-42b5-ad63-26ecdf46aa92",
   "metadata": {},
   "source": [
    "### 1.4 Saving Intermediate Denoising Steps in the Stable Diffusion Pipeline\n",
    "\n",
    "In many research and creative workflows, it’s useful to inspect or save the intermediate images produced during the denoising process of a diffusion model.\n",
    "This lets you better understand, debug, or visualize how your prompt is transformed from noise into a coherent image.\n",
    "\n",
    "With the Hugging Face diffusers library, you can easily capture these steps using the pipeline’s callback functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a07123-79a7-49ac-b345-1b2ac6f1efae",
   "metadata": {},
   "source": [
    "#### 1.4.1 How the Callback Mechanism Works\n",
    "\n",
    "The diffusion pipeline allows you to supply a callback function, which is called after each denoising step.\n",
    "The callback receives:\n",
    "\n",
    "- The current step index\n",
    "- The current timestep\n",
    "- The current latent representation (not yet decoded)\n",
    "  \n",
    "You can use this callback to:\n",
    "\n",
    "- Save the latent at each step\n",
    "- Decode it into an image (using the VAE) if you wish\n",
    "- Visualize the denoising trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63932de8-05e4-4b39-9345-f24bcdea9c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store intermediate images\n",
    "all_steps = []\n",
    "\n",
    "def save_intermediate_steps(step_index: int, timestep: int, latents: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Callback for saving intermediate images during denoising in Stable Diffusion v2.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # 0.18215 is the VAE scaling factor used by Stable Diffusion 2\n",
    "        # This rescales latents back to the range expected by the VAE decoder\n",
    "        latents_input = latents / 0.18215\n",
    "        image = pipe.vae.decode(latents_input).sample\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)  # Normalize to [0,1]\n",
    "        image = image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "    pil_image = Image.fromarray((image * 255).astype(\"uint8\"))\n",
    "    # Save to directory (optional)\n",
    "    outdir = \"intermediate_steps\"\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    pil_image.save(f\"{outdir}/step_{step_index:02d}.png\")\n",
    "    # Also collect for grid plotting\n",
    "    all_steps.append((step_index, pil_image))\n",
    "\n",
    "generator = torch.Generator(device).manual_seed(42)\n",
    "\n",
    "prompt = \"A puppy dog riding a skateboard in Times Square\"\n",
    "negative_prompt = \"cars\"\n",
    "\n",
    "# Clear previous\n",
    "all_steps.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01af877c-7ccc-4189-ad1c-e52c80c81d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipe with the callback\n",
    "pipe(\n",
    "    prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_inference_steps=40,\n",
    "    generator=generator,\n",
    "    callback=save_intermediate_steps,\n",
    "    callback_steps=1  # Every step\n",
    ")\n",
    "# ----- Plot all 40 steps in an 8×5 grid -----\n",
    "n_rows, n_cols = 8, 5  # 8 rows, 5 columns\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 24))\n",
    "fig.suptitle('Stable Diffusion 2: Denoising Steps', fontsize=18)\n",
    "\n",
    "for idx, (step, img) in enumerate(all_steps):\n",
    "    row, col = divmod(idx, n_cols)\n",
    "    axes[row, col].imshow(img)\n",
    "    axes[row, col].set_title(f\"Step {step+1}\", fontsize=8)\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d680d7-6b3b-4786-86b9-9168d2734faa",
   "metadata": {},
   "source": [
    "### What is a DDPM?\n",
    "\n",
    "**DDPM (Denoising Diffusion Probabilistic Models)** is a type of generative model that creates images through a two-phase process:\n",
    "\n",
    "#### Forward Process (Diffusion)\n",
    "- Gradually adds Gaussian noise to an image over many steps (typically 1000)\n",
    "- Eventually transforms any image into pure random noise\n",
    "- This process is fixed and doesn't require learning\n",
    "\n",
    "#### Reverse Process (Denoising)\n",
    "- Learns to reverse the forward process step by step\n",
    "- A neural network is trained to predict and remove noise at each step\n",
    "- Starting from pure noise, it gradually denoises to generate a new image\n",
    "\n",
    "#### Key Characteristics:\n",
    "- **Probabilistic**: Each denoising step involves sampling from a probability distribution\n",
    "- **Iterative**: Requires many small denoising steps (unlike GANs which generate in one pass)\n",
    "- **Stable Training**: Easier to train than GANs, with fewer stability issues\n",
    "- **High Quality**: Produces very high-quality, diverse images\n",
    "\n",
    "The mathematical foundation involves learning to approximate the reverse of a Markov chain that gradually adds noise to data.\n",
    "\n",
    "---\n",
    "\n",
    "### 2 - Comparing Pixel-space DDPM and Latent-space Stable Diffusion: How Does Denoising Progress Look?\n",
    "\n",
    "Generative diffusion models can operate either on images directly (pixel space) or on compressed representations of images (latent space).\n",
    "Understanding the intermediate steps of each approach is highly instructive for anyone learning about generative models.\n",
    "\n",
    "#### Pixel-space DDPMs\n",
    "- Operate directly on RGB image pixels\n",
    "- The denoising process is intuitive—images become less noisy and more detailed step by step, and every intermediate step is a valid, viewable image\n",
    "- This makes pixel-space DDPMs ideal for educational demos about the core idea behind denoising diffusion\n",
    "- **Drawback**: Computationally expensive due to working with high-dimensional pixel data\n",
    "\n",
    "#### Latent Diffusion Models (like Stable Diffusion)\n",
    "- Operate on compressed codes (\"latents\") learned from image data\n",
    "- Intermediate outputs may not look like natural images—structure becomes visible only in late steps\n",
    "- This makes them **much more efficient and scalable**, but less intuitive to visualize at each step\n",
    "- Use an autoencoder to compress images to latent space and decode back to pixels\n",
    "\n",
    "In this section, you'll see both processes side by side, using classic DDPM (on pixels) and Stable Diffusion (on latents)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4221cd5f-a10d-4253-bde8-8793706c414d",
   "metadata": {},
   "source": [
    "**You may see a message about .safetensors not being found and the model using .bin weights instead.**\n",
    "\n",
    "This is a normal part of loading older Hugging Face models and does not affect their functionality.\n",
    "You can safely ignore this warning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf610af-531c-4fa0-947c-f40753be6fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMPipeline\n",
    "from helper_utils import visualize_ddpm_denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cbddfe-4454-413f-8f06-d4fb0d77d88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model and cache directory\n",
    "model_id = \"google/ddpm-ema-bedroom-256\"  # Bedroom images (256x256)\n",
    "cache_dir = os.path.join(os.getcwd(), './models')\n",
    "model_cache_dir_name = f\"models--{model_id.replace('/', '--')}\"\n",
    "model_cache_path = os.path.join(cache_dir, model_cache_dir_name)\n",
    "\n",
    "# Check if this directory exists. If it does, the model is cached.\n",
    "model_is_cached = os.path.isdir(model_cache_path)\n",
    "\n",
    "if model_is_cached:\n",
    "    print(\"Model cache found\")\n",
    "    print(\"Setting local_files_only=True.\")\n",
    "else:\n",
    "    print(\"Model cache not found\")\n",
    "    print(\"Setting local_files_only=False (will attempt download).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762936b1-40eb-47dc-b96a-96096ed12396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DDPM pipeline from cache\n",
    "\n",
    "ddpm_pipeline = DDPMPipeline.from_pretrained(\n",
    "    model_id, \n",
    "    cache_dir=cache_dir, \n",
    "    local_files_only=model_is_cached\n",
    ").to(device)\n",
    "\n",
    "# Using 1000 steps for highest quality DDPM generation\n",
    "# (DDPM typically requires more steps than Stable Diffusion for best results)\n",
    "num_inference_steps = 1000\n",
    "\n",
    "print(f\"Using model: {model_id}\")\n",
    "print(f\"Image resolution: 256x256 pixels\")\n",
    "print(f\"Steps: {num_inference_steps}\")\n",
    "\n",
    "# Access the model and scheduler\n",
    "model = ddpm_pipeline.unet\n",
    "scheduler = ddpm_pipeline.scheduler\n",
    "\n",
    "# Set up for visualization\n",
    "scheduler.set_timesteps(num_inference_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba6847e-d89b-428f-a1fd-14db83a26890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run pixel-space DDPM comparison ---\n",
    "num_inference_steps = 100\n",
    "print(\"Generating DDPM denoising comparison...\")\n",
    "gradual_images, full_removal_images = visualize_ddpm_denoising(ddpm_pipeline, num_inference_steps=num_inference_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cc4616-2a37-4c99-896f-ef848e38effb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splits = 5\n",
    "# Use the actual step IDs saved\n",
    "actual_step_ids = [gi[0] for gi in gradual_images]\n",
    "num_inference_steps = actual_step_ids[-1]  # last step\n",
    "step_indices = [int(np.round(i * num_inference_steps / (num_splits - 1))) for i in range(num_splits - 1)]\n",
    "step_indices.append(num_inference_steps)\n",
    "\n",
    "print(\"Plotted step indices:\", step_indices)\n",
    "\n",
    "# Speed up access: build {step: image} dictionaries\n",
    "grad_dict = {s: img for (s, img) in gradual_images}\n",
    "full_dict = {s: img for (s, img) in full_removal_images}\n",
    "\n",
    "# Use closest available step if step is not present\n",
    "def get_closest_img(dct, step):\n",
    "    # Find the closest available key in the dict\n",
    "    best = min(dct.keys(), key=lambda k: abs(k-step))\n",
    "    return dct[best]\n",
    "\n",
    "images_full = [get_closest_img(full_dict, s) for s in step_indices]\n",
    "images_grad = [get_closest_img(grad_dict, s) for s in step_indices]\n",
    "final_img = get_closest_img(grad_dict, num_inference_steps)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, len(step_indices), figsize=(3.5 * len(step_indices), 6))\n",
    "for i, (img, s) in enumerate(zip(images_full, step_indices)):\n",
    "    axes[0, i].imshow(img)\n",
    "    axes[0, i].set_title(f\"Step {s}: All noise removed\")\n",
    "    axes[0, i].axis('off')\n",
    "for i, (img, s) in enumerate(zip(images_grad, step_indices)):\n",
    "    axes[1, i].imshow(img)\n",
    "    axes[1, i].set_title(\"Final Result\" if i == len(step_indices)-1 else f\"Step {s}: Gradual\")\n",
    "    axes[1, i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94d0f39-f3ea-4133-ab4a-158465a63cc1",
   "metadata": {},
   "source": [
    "**Notice how:**\n",
    "- \"All noise removed\" at each step starts out blurry and imperfect, only approaching the true image by the very end.\n",
    "- \"Gradual\" denoising images sharpen step by step, with the final result much cleaner.\n",
    "- This demonstrates why diffusion models remove noise one step at a time, rather than all at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f3cc93-141e-49bb-983d-edff7477cd79",
   "metadata": {},
   "source": [
    "### 3 - Playing with the Diffusion Model\n",
    "\n",
    "Now it's your turn to experiment! The widget below allows you to easily generate images with different prompts and parameters.\n",
    "\n",
    "**Suggestions to try:**\n",
    "- **Different subjects**: animals, landscapes, objects, fantasy creatures\n",
    "- **Art styles**: \"in the style of Van Gogh\", \"pixel art\", \"watercolor painting\", \"3D render\"\n",
    "- **Lighting/mood**: \"sunset lighting\", \"dramatic shadows\", \"soft pastel colors\"\n",
    "- **Negative prompts**: Try adding \"blurry, distorted, low quality\" to improve results\n",
    "- **Parameter experimentation**: \n",
    "  - Low guidance_scale (3-5) for creative, artistic results\n",
    "  - High guidance_scale (12-15) for literal interpretation\n",
    "  - More steps (100+) for higher detail\n",
    "\n",
    "**Common issues:**\n",
    "- **Blurry images**: Try increasing `num_inference_steps` or adding negative prompts\n",
    "- **Doesn't match prompt**: Increase `guidance_scale`\n",
    "- **Unnatural colors/artifacts**: Lower `guidance_scale` or try different seeds\n",
    "- **Slow generation**: Reduce `num_inference_steps` (try 25-30 for faster results)\n",
    "\n",
    "Run the cell below to load the interactive widget!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97747f5c-b558-4635-9992-c57685764016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_utils import load_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a8fad4-a648-4898-b2ef-c836e9cebb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_widget(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665c7997-b478-4e76-bbe3-42dc704625b7",
   "metadata": {},
   "source": [
    "\n",
    "## 4 -  Conclusion\n",
    "\n",
    "In this lab, you:\n",
    "- Explored how diffusion models generate images step by step from noise.\n",
    "- Compared classic DDPMs (in pixel space) with modern Stable Diffusion (in latent space).\n",
    "- Used PyTorch and the Diffusers library to experiment with generation parameters and observe denoising in action.\n",
    "- Built and interacted with a widget to visualize your own prompts through the denoising process.\n",
    "\n",
    "Continue to try new prompts, tweak settings, and see what amazing images you can create!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
