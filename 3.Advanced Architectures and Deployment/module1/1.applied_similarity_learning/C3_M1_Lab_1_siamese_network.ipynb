{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-f6a1-b2c3-d4e5-f6a1b2c3d4e5",
   "metadata": {},
   "source": [
    "# Applied Similarity Learning: Signatures & Satellites\n",
    "\n",
    "Welcome! So far, your journey with deep learning has likely involved building models that follow a linear path: data goes in one end, flows through a series of layers, and a single prediction comes out the other. This is a powerful and straightforward approach for many classification and regression tasks.\n",
    "\n",
    "But what happens when the question is not \"What is this?\" but rather, \"Are these two things similar?\". This shift in perspective requires a different kind of architecture, one that excels at comparison rather than categorization.\n",
    "\n",
    "In this lab, you will build and train a **Siamese Network**, a specialized architecture designed to learn a *similarity* function. Instead of a traditional classifier that learns to assign a label to a single input, a Siamese network learns to measure the \"distance\" between two or more inputs in a learned feature space. It achieves this by processing multiple inputs through an identical encoding network. This single, reused network ensures that all inputs are mapped to a consistent and comparable representation, creating an embedding space where similar items are pulled close together and dissimilar items are pushed far apart.\n",
    "\n",
    "The flexibility of this approach makes it an invaluable tool for solving complex, real-world problems. To see this in action, you will apply this architecture to two completely different domains.\n",
    "\n",
    "#### Use Case 1: Signature Verification\n",
    "\n",
    "> You will first build a model to handle signature verification, a vital security task in settings like at a bank. The goal is to accurately determine if two signatures were written by the same person. A standard classifier struggles with this because it would need to be retrained for every new individual. Your Siamese network will instead learn the general characteristics of handwriting, enabling it to verify signatures even for people it has never seen during training. You will train this model using triplets of images: an anchor (the reference signature), a positive (another genuine one), and a negative (a forgery).\n",
    ">\n",
    "#### Use Case 2: Environmental Change Detection\n",
    ">\n",
    "> Next, you will showcase the model's versatility by adapting it for an environmental task. Instead of signatures, you will compare satellite images of the same location taken at different times. The goal is to detect significant changes in vegetation by analyzing \"Before\" and \"After\" image pairs. This application demonstrates how a model trained to understand similarity can be a powerful tool for monitoring our changing planet.\n",
    "\n",
    "In this lab, you will:\n",
    "* Build a custom `SiameseNetwork` class from the ground up.\n",
    "* Prepare specialized datasets that provide image triplets and pairs for training.\n",
    "* Train a model for signature verification using `TripletMarginLoss`.\n",
    "* Adapt the model to detect environmental changes using a custom `WeightedContrastiveLoss`.\n",
    "* Apply your trained models to perform predictions on new, unseen data.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a653bf-c97f-41b2-8606-8fec25f6beb7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380e8ea7-91b0-440f-8e98-bcaec4acfc77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import helper_utils\n",
    "import training_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f22db9a-5a44-434f-838c-ca28d513a568",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8673eb95-fb27-41c0-b8ca-588c0b5e7f75",
   "metadata": {},
   "source": [
    "## **Use Case 1**: Signature *Verification*\n",
    "\n",
    "Your first challenge is to tackle signature verification, an application with important security implications in settings such as banking. To do this, you will use a specially prepared version of the [Signature Verification_v5](https://universe.roboflow.com/signature-verification-online-and-offline/signature-verification_v5) dataset, which contains multiple genuine (**Real**) and forged (**Fake**) signatures for many individuals.\n",
    "\n",
    "**How the Dataset Was Prepared by Original Authors**\n",
    "\n",
    "The original authors performed extensive work to standardize and enrich the images by applying the significant modifications summarized below:\n",
    "\n",
    "* **Pre-processing:**\n",
    "    * **Auto-orientation** of pixel data (with EXIF-orientation stripping) to ensure all images have the correct upright orientation.\n",
    "    * **Standard Resize** to 224x224 pixels (Stretch) for uniform model input.\n",
    "    * **Conversion to Grayscale** (CRT phosphor) to help the model learn the fundamental shapes and strokes of the signatures.\n",
    "* **Augmentation**\n",
    "    * Random **rotation** between -15 and +15 degrees to mimic the natural tilt of a paper or scanner.\n",
    "    * Random **brightness** adjustment between -25% and +25% and random **exposure** adjustment between -15% and +15% to simulate various lighting conditions.\n",
    "    * Random Gaussian **blur** of up to 0.5 pixels to represent minor focus issues common with scanning hardware.\n",
    "    \n",
    "**How the Dataset Was Prepared for This Lab**\n",
    "\n",
    "While the original dataset was well prepared, it stored signatures in bulk 'real' and 'fake' folders. For this lab, the data was meticulously reorganized to create a more powerful learning experience. Using machine learning clustering and subsequent human review, signatures were grouped into **52 unique individual profiles**.\n",
    "\n",
    "You will train and validate your model on 51 of these profiles, reserving the final one to test the model's performance on a completely unseen individual. This \"per person\" organization provides the foundation for training an effective similarity based network.\n",
    "\n",
    "As you explore the data, you may find images that appear miscategorized. This is not an error but a realistic simulation of a common industry challenge. Building models that are robust enough to handle this \"data noise\" is a fundamental skill for any AI professional.\n",
    "\n",
    "* Run the next cells below to see a statistical overview of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d79938-82c0-4174-a2d1-458d7c53282d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the path to the root directory containing the signature dataset.\n",
    "signature_data_dir = './Signature_Verification_v5_v11/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a27a906-c5b0-462d-97d1-431e18294740",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scan the data directory and display the statistical summary.\n",
    "helper_utils.display_signature_dataset_summary(signature_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7017198d-366a-4a5d-a65f-48d353defd88",
   "metadata": {},
   "source": [
    "* Run the cell below to see a side by side comparison of a \"genuine\" signature and a corresponding \"forgery\" for a randomly chosen individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a4b884-8c87-4a89-8b6c-69c3f6639d86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display a side-by-side comparison of a random real and fake signature from the dataset.\n",
    "helper_utils.display_random_signature_pair(signature_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc1eee5-07cc-45e8-baea-eecfe201ac2d",
   "metadata": {},
   "source": [
    "### Building a Custom Triplet Dataset Class\n",
    "\n",
    "This next step is a cornerstone of your model's success. Unlike traditional classifiers, a Siamese Network learns from **comparisons**. You will teach it to distinguish genuine signatures from forgeries by feeding it specially structured data packets called **triplets**. Each triplet is the essential learning unit for your model and contains three images:\n",
    "\n",
    "* **Anchor**: A genuine signature from an individual. This is your baseline.\n",
    "* **Positive**: A *different* genuine signature from the **same** individual.\n",
    "* **Negative**: A forged signature of that **same** individual.\n",
    "\n",
    "By training on these triplets, your model learns a specific objective: to minimize the distance between the **Anchor** and **Positive** while maximizing the distance between the **Anchor** and **Negative**. This process is how the network develops a nuanced understanding of an individual's unique handwriting.\n",
    "\n",
    "To generate these triplets on the fly, the data must be organized predictably. The `SignatureTripletDataset` class is designed to work with the exact 'per person' directory structure shown below. This clean separation allows the class to efficiently sample genuine images for the **Anchor** and **Positive** roles and forged images for the **Negative**.\n",
    "\n",
    "```\n",
    "./Signature_Verification_v5_v11/\n",
    "│\n",
    "├── Real/\n",
    "│   ├── ID_01/\n",
    "│   │   ├── signature_real_1_1.jpg\n",
    "│   │   ├── signature_real_1_2.jpg\n",
    "│   │   └── ... (more real signatures)\n",
    "│   │\n",
    "│   └── ... (more individual folders)\n",
    "│\n",
    "└── Fake/\n",
    "    ├── ID_01/\n",
    "    │   ├── signature_fake_1_1.jpg\n",
    "    │   └── ... (more fake signatures)\n",
    "    │\n",
    "    └── ... (more individual folders)\n",
    "```\n",
    "\n",
    "This class is the engine that connects the organized directory structure to the triplet concept your model needs. It's a standard PyTorch `Dataset` with a few implementations designed specifically for this task. Here's how its main components empower your training process:\n",
    "\n",
    "* **`__init__(self, base_data_dir, ...)`**: \n",
    "> This is the constructor, which performs a **one-time** setup when you create the dataset object.\n",
    ">    * It efficiently scans the entire `base_data_dir`, validates each individual's folder, and builds a `signature_map` that stores all valid file paths.\n",
    ">    * The `triplets_per_user` argument you provide gives you direct control over the length of each training epoch.\n",
    "\n",
    "* **`__len__(self)`**: \n",
    "> This method returns the \"virtual\" size of your dataset for an epoch.\n",
    ">    * This size is calculated by multiplying the number of individuals by the `triplets_per_user` value. \n",
    ">        * The `triplets_per_user` acts like any other hyperparameter that you can tune to control the length and dynamics of each training epoch.\n",
    "\n",
    "* **`__getitem__(self, index)`**: \n",
    "> This is the core of the data loading process, generating a new training triplet every time one is requested. A key aspect of its design is that it **intentionally ignores the `index` argument**. Instead of fetching a pre-defined item, it randomly selects an individual and then samples a unique combination of **Anchor**, **Positive**, and **Negative** images.\n",
    ">    * This on-the-fly random sampling is a powerful feature: it ensures your model sees a massive variety of triplets, forcing it to learn the general characteristics of a person's handwriting rather than memorizing specific pairs.\n",
    "\n",
    "* **`_load_image(self, path)`**: \n",
    "> This is a private helper method that robustly loads each image.\n",
    ">    * It uses a context manager to ensure files are closed properly, preventing potential resource leaks during long training runs, and making the overall class more resilient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e468ca14-a8a9-4fc4-8fb3-ae8315571646",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SignatureTripletDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for creating signature triplets for verification.\n",
    "\n",
    "    This class scans a directory of real and fake signatures, organized by\n",
    "    user ID, and generates triplets (anchor, positive, negative) on the fly\n",
    "    for training a Siamese network with triplet loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_data_dir, triplets_per_user=100, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset by scanning the data directory and organizing file paths.\n",
    "        \n",
    "        Args:\n",
    "            base_data_dir (str): The root directory of the signature dataset.\n",
    "            triplets_per_user (int): The \"virtual\" number of triplets to generate\n",
    "                                     per individual for one epoch.\n",
    "            transform (callable, optional): PyTorch transforms to be applied to each image.\n",
    "        \"\"\"\n",
    "        self.base_data_dir = base_data_dir\n",
    "        self.triplets_per_user = triplets_per_user\n",
    "        self.transform = transform\n",
    "        # Build the map of all available image paths from the source directory.\n",
    "        self.signature_map = self._create_signature_map()\n",
    "        # Create the definitive list of individuals to be used in the dataset.\n",
    "        self.user_ids = list(self.signature_map.keys())\n",
    "        \n",
    "        # Raise an error if the dataset directory is empty or improperly structured.\n",
    "        if not self.user_ids:\n",
    "            raise RuntimeError(f\"No valid individuals found in {base_data_dir}. Check directory structure and image counts.\")\n",
    "\n",
    "    def _create_signature_map(self):\n",
    "        \"\"\"Scans the directory to build a map of individuals to their signature paths (one-time setup).\"\"\"\n",
    "        # Define paths for real and fake signature directories.\n",
    "        real_signatures_dir = os.path.join(self.base_data_dir, 'Real')\n",
    "        fake_signatures_dir = os.path.join(self.base_data_dir, 'Fake')\n",
    "        signature_map = defaultdict(lambda: {'real': [], 'fake': []})\n",
    "\n",
    "        # Validate that the 'Real' signatures directory exists.\n",
    "        if not os.path.isdir(real_signatures_dir):\n",
    "            raise FileNotFoundError(f\"Error: Directory not found at {real_signatures_dir}\")\n",
    "\n",
    "        # Validate that the 'Fake' signatures directory exists.\n",
    "        if not os.path.isdir(fake_signatures_dir):\n",
    "            raise FileNotFoundError(f\"Error: Directory not found at {fake_signatures_dir}\")\n",
    "\n",
    "        # Iterate through each user ID directory.\n",
    "        all_ids = sorted(os.listdir(real_signatures_dir))\n",
    "        for user_id in all_ids:\n",
    "            if user_id.startswith('ID_'):\n",
    "                # Find all real and fake signature images for the current user.\n",
    "                real_images = glob.glob(os.path.join(real_signatures_dir, user_id, '*.jpg'))\n",
    "                fake_images = glob.glob(os.path.join(fake_signatures_dir, user_id, '*.jpg'))\n",
    "                \n",
    "                # Only include individuals with enough images to create a valid triplet.\n",
    "                if len(real_images) >= 2 and len(fake_images) >= 1:\n",
    "                    signature_map[user_id]['real'] = real_images\n",
    "                    signature_map[user_id]['fake'] = fake_images\n",
    "                    \n",
    "        return signature_map\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the \"virtual\" length of the dataset for an epoch.\n",
    "        \n",
    "        This is not the total number of possible triplets, but a fixed number\n",
    "        to define the size of an epoch.\n",
    "        \"\"\"\n",
    "        return len(self.user_ids) * self.triplets_per_user\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Generates and returns one triplet of images on the fly.\n",
    "        \n",
    "        Args:\n",
    "            index (int): Required by PyTorch's Dataset API but not used here,\n",
    "                         as triplets are generated randomly.\n",
    "            \n",
    "        Returns:\n",
    "            tuple: A tuple containing the (anchor, positive, negative) image tensors.\n",
    "        \"\"\"\n",
    "        # Randomly select an individual to form the triplet.\n",
    "        person_id = random.choice(self.user_ids)\n",
    "        \n",
    "        # Sample two distinct real images for the anchor and positive samples.\n",
    "        anchor_path, positive_path = random.sample(self.signature_map[person_id]['real'], 2)\n",
    "        # Sample one fake image for the negative sample.\n",
    "        negative_path = random.choice(self.signature_map[person_id]['fake'])\n",
    "\n",
    "        # Load images from paths and apply any specified transformations.\n",
    "        anchor_img = self._load_image(anchor_path)\n",
    "        positive_img = self._load_image(positive_path)\n",
    "        negative_img = self._load_image(negative_path)\n",
    "        \n",
    "        return (anchor_img, positive_img, negative_img)\n",
    "\n",
    "    def _load_image(self, path):\n",
    "        \"\"\"\n",
    "        Helper function to robustly load a single image from a given path.\n",
    "\n",
    "        Args:\n",
    "            path (str): The file path of the image to load.\n",
    "\n",
    "        Returns:\n",
    "            The loaded and transformed image, typically a torch.Tensor.\n",
    "        \"\"\"\n",
    "        # Use a context manager to ensure the file is properly closed after loading.\n",
    "        with Image.open(path) as img:\n",
    "            # Ensure the image is in RGB format, as many networks expect 3 channels.\n",
    "            image = img.convert(\"RGB\")\n",
    "            # Apply any specified transformations (e.g., resizing, tensor conversion).\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5496ff-6265-450e-aa33-8f432590829f",
   "metadata": {},
   "source": [
    "* Now that your `SignatureTripletDataset` class is defined, create the main dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733476eb-7eca-42dd-9143-4a657a2658d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the full dataset object. \n",
    "full_signature_dataset = SignatureTripletDataset(signature_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c95251-23ac-43fe-a295-2acb3ac61e8b",
   "metadata": {},
   "source": [
    "### Preparing the Signature Data for Training\n",
    "\n",
    "* First, define your pipelines of transformations for training and validation data. Since the source dataset already includes common augmentations, your focus will be on transformations that specifically mimic variations in handwriting style.\n",
    "    * Use the pre-calculated `mean` and `std` of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcea700-3dfd-476e-b126-2a65f460e495",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pre-calculated mean and standard deviation for this dataset\n",
    "mean = [0.861, 0.861, 0.861]\n",
    "std = [0.274, 0.274, 0.274]\n",
    "\n",
    "# Transformations for the training set (with augmentation)\n",
    "train_transform = transforms.Compose([\n",
    "    # Randomly apply slight affine transformations (shear and translation)\n",
    "    # This mimics variations in writing slant and position\n",
    "    transforms.RandomAffine(degrees=0, shear=10, translate=(0.1, 0.1)),\n",
    "    # Randomly apply a slight perspective shift\n",
    "    # This can simulate viewing the signature from a different angle\n",
    "    transforms.RandomPerspective(distortion_scale=0.1, p=0.5),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std) \n",
    "])\n",
    "\n",
    "# Transformations for validation set (no augmentation)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std) \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d4e374-ead2-44fb-b5fb-37074997a1fc",
   "metadata": {},
   "source": [
    "* Now, you will split the main dataset and prepare the final `DataLoaders` for training.\n",
    "    * Divide the full dataset into an 80% training set and a 20% validation set.\n",
    "    * Create dataloaders for each set to handle batching and shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f9cfba-284b-4909-865f-0ea494659fd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the full dataset into training and validation sets.\n",
    "train_dataset, val_dataset = helper_utils.create_signature_datasets_splits(\n",
    "    full_dataset=full_signature_dataset,\n",
    "    train_split=0.8, \n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform\n",
    ")\n",
    "\n",
    "# Create a DataLoader for the training set.\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# Create a DataLoader for the validation set.\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Print the final \"virtual\" size of each dataset split.\n",
    "print(f\"Total training triplets:    {len(train_dataset)}\")\n",
    "print(f\"Total validation triplets:  {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ab686b-40cc-4f30-b5d9-56fbd386da05",
   "metadata": {},
   "source": [
    "### Visualizing a Training Sample\n",
    "\n",
    "* Visualize a random sample to confirm everything is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a81b052-c264-4852-9d29-9c504f048c14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize a random triplet from the training dataloader.\n",
    "helper_utils.show_random_triplet(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e560b67-dbc0-4201-b293-525d16680760",
   "metadata": {},
   "source": [
    "### Constructing the Siamese Network\n",
    "\n",
    "Now that your data is ready, it's time to build the model. Constructing a Siamese Network involves two key parts: first, you'll build a core \"backbone\" network to extract features from the signatures, and second, you'll create the *main* Siamese model that uses this backbone to compare the images in a triplet.\n",
    "\n",
    "#### Building the Backbone: The Embedding Network\n",
    "\n",
    "At first glance, the network you are about to define, `SimpleEmbeddingNetwork`, will look very familiar. It's a standard Convolutional Neural Network (CNN), much like others you may have built for image classification tasks.\n",
    "\n",
    "However, there is a pivotal difference in its final purpose. Instead of classifying an image into a predefined category (like 'cat' or 'dog'), this network's job is to distill a signature's unique characteristics into a dense numerical vector called an **embedding**.\n",
    "\n",
    "This changes how you think about the final layer. Instead of an output with a neuron for each class, you have an output that produces a vector of a specific size. This size, which you'll define as the **embedding dimension**, replaces the concept of the 'number of classes'.\n",
    "\n",
    "This `SimpleEmbeddingNetwork` is the most important piece of the puzzle. It acts as the backbone, or the core feature extractor. The full Siamese Network, which you'll define next, is a simple wrapper that uses this backbone to process all three images in a triplet, ensuring each signature is analyzed in the exact same way to produce a comparable embedding.\n",
    "\n",
    "* Define the `SimpleEmbeddingNetwork` model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e976ad-f058-4185-95c2-105e42582615",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleEmbeddingNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Convolutional Neural Network to generate a fixed-size embedding from an image.\n",
    "    This network is designed for 224x224 RGB input images.\n",
    "\n",
    "    Attributes:\n",
    "        conv (nn.Sequential): The convolutional layers for feature extraction.\n",
    "        fc (nn.Sequential): The fully connected layers for generating the embedding.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim=128):\n",
    "        # Initialize the parent nn.Module class.\n",
    "        super(SimpleEmbeddingNetwork, self).__init__()\n",
    "        \n",
    "        # Define the convolutional layers that act as a feature extractor.\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=5), nn.ReLU(), nn.MaxPool2d(2, stride=2),\n",
    "            # Add a dropout layer for regularization to prevent overfitting.\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Conv2d(32, 64, kernel_size=5), nn.ReLU(), nn.MaxPool2d(2, stride=2),\n",
    "            # Add another dropout layer.\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Conv2d(64, 128, kernel_size=3), nn.ReLU(), nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Define the fully connected layers that produce the final embedding vector.\n",
    "        self.fc = nn.Sequential(\n",
    "            # The input size is derived from the output of the final conv layer.\n",
    "            nn.Linear(128 * 25 * 25, 256), nn.ReLU(),\n",
    "            # Use a dropout layer with a higher rate for stronger regularization.\n",
    "            nn.Dropout(0.6),\n",
    "            # The final linear layer maps the features to the desired embedding dimension.\n",
    "            nn.Linear(256, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input batch of images.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output embedding vector for each image in the batch.\n",
    "        \"\"\"\n",
    "        # Pass the input through the convolutional feature extractor.\n",
    "        x = self.conv(x)\n",
    "        # Flatten the 3D feature map into a 1D vector for each item in the batch.\n",
    "        x = x.view(x.size(0), -1) \n",
    "        # Pass the flattened vector through the fully connected layers.\n",
    "        x = self.fc(x)\n",
    "        # Return the final embedding.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bf33f5-191a-4653-a5f8-40a1954f44f1",
   "metadata": {},
   "source": [
    "#### Assembling the Siamese Network\n",
    "\n",
    "With your `SimpleEmbeddingNetwork` backbone defined, you will now create the main `SiameseNetwork` class. This class acts as a high level wrapper, and its design is centered around **modularity**.\n",
    "\n",
    "* **Interchangeable Backbone**: The class is built to accept any embedding network you provide. This allows you to easily swap your `SimpleEmbeddingNetwork` for a different, more powerful backbone later without having to change this main wrapper class.\n",
    "* **Flexible Inputs**: This modular design also applies to the data it can process. The `forward` method is built to handle both:\n",
    "    * **Triplets**, by taking an Anchor, Positive, and Negative image and returning three embeddings for your signature verification task. By default, the network expects triplets.\n",
    "    * **Pairs**, by taking two images and returning two embeddings. You'll see an example of this pair based comparison in a later section within this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90231881-4825-4f1f-b0ef-56dc07c3664c",
   "metadata": {},
   "source": [
    "* Define the `SiameseNetwork` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5657fbed-cfc6-4334-be24-44ee18a7764c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A flexible Siamese Network that can process either image triplets or pairs.\n",
    "\n",
    "    This network uses a shared backbone (embedding network) to generate feature\n",
    "    vectors (embeddings) for multiple input images simultaneously. It can operate\n",
    "    in two modes: one for training with triplets (anchor, positive, negative)\n",
    "    and one for inference with pairs.\n",
    "\n",
    "    Attributes:\n",
    "        embedding_network (nn.Module): The shared backbone network.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_network):\n",
    "        \"\"\"\n",
    "        Initializes the Siamese Network.\n",
    "        \n",
    "        Args:\n",
    "            embedding_network (nn.Module): The backbone network that generates embeddings.\n",
    "        \"\"\"\n",
    "        # Initialize the parent nn.Module class.\n",
    "        super().__init__()\n",
    "        # Store the shared backbone model.\n",
    "        self.embedding_network = embedding_network\n",
    "        \n",
    "    def forward(self, *inputs, triplet_bool=True):\n",
    "        \"\"\"\n",
    "        Processes either a triplet or a pair of images through the embedding network.\n",
    "\n",
    "        Args:\n",
    "            *inputs: A sequence of input tensors.\n",
    "                     - If triplet_bool is True, expects (anchor, positive, negative).\n",
    "                     - If triplet_bool is False, expects (image1, image2).\n",
    "            triplet_bool (bool): If True, operates in triplet mode for training.\n",
    "                                 If False, operates in pair mode for inference.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: A tuple of output embedding tensors.\n",
    "        \"\"\"\n",
    "        if triplet_bool:\n",
    "            # Handle the case for training with triplets.\n",
    "            if len(inputs) != 3:\n",
    "                raise ValueError(\"In triplet mode, expected 3 inputs: anchor, positive, negative.\")\n",
    "            \n",
    "            # Unpack the triplet inputs.\n",
    "            anchor, positive, negative = inputs\n",
    "            \n",
    "            # Generate embeddings for each image using the shared backbone.\n",
    "            anchor_output = self.embedding_network(anchor)\n",
    "            positive_output = self.embedding_network(positive)\n",
    "            negative_output = self.embedding_network(negative)\n",
    "            \n",
    "            return anchor_output, positive_output, negative_output\n",
    "        \n",
    "        else:\n",
    "            # Handle the case for inference with image pairs.\n",
    "            if len(inputs) != 2:\n",
    "                raise ValueError(\"In pair mode, expected 2 inputs: before_img, after_img.\")\n",
    "            \n",
    "            # Unpack the pair inputs.\n",
    "            img1, img2 = inputs\n",
    "            \n",
    "            # Generate embeddings for both images using the shared backbone.\n",
    "            output1 = self.embedding_network(img1)\n",
    "            output2 = self.embedding_network(img2)\n",
    "            \n",
    "            return output1, output2\n",
    "    \n",
    "    def get_embedding(self, image):\n",
    "        \"\"\"\n",
    "        Generates a single embedding for a given image.\n",
    "        \n",
    "        Args:\n",
    "            image (torch.Tensor): A single image tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The resulting embedding vector.\n",
    "        \"\"\"\n",
    "        # Pass the single image through the backbone to get its embedding.\n",
    "        return self.embedding_network(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e93d36-469e-4270-85ba-d3bbf5e3e292",
   "metadata": {},
   "source": [
    "* Now, set the `embedding_dim`. This is a key hyperparameter that defines the size of the numerical vector the model will use to represent each signature.\n",
    "* Next, create an instance of your backbone, the `SimpleEmbeddingNetwork`, passing in the embedding dimension you just defined.\n",
    "* Finally, assemble the complete model by creating an instance of the `SiameseNetwork` and passing your `embedding_net` into it as its \"backbone\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71564de-00bc-4820-831f-30cc9fcee6ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the desired size for the final embedding vector\n",
    "embedding_dim = 128\n",
    "\n",
    "# Create an instance of the base model that generates embeddings\n",
    "embedding_net = SimpleEmbeddingNetwork(embedding_dim=embedding_dim)\n",
    "\n",
    "# Create the main Siamese network model, using the embedding network\n",
    "siamese_network = SiameseNetwork(embedding_network=embedding_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e90011-a058-4e18-8dda-9bfbd3d68f1e",
   "metadata": {},
   "source": [
    "### Teaching the Network to See Similarity\n",
    "\n",
    "With your model architecture and data loaders prepared, you are ready to define the core components for the training.\n",
    "\n",
    "#### Measuring Similarity: The Triplet Margin Loss\n",
    "\n",
    "A traditional classifier learns to assign a fixed label to an image. Your goal is different. You need to teach your model how to create an **embedding space**—a virtual map where similar signatures are grouped closely together and forgeries are pushed far away. To sculpt this space, you need a specialized tool, and that's where **Triplet Margin Loss** comes in.\n",
    "\n",
    "This loss function works by analyzing the three images in a triplet. Its objective is simple but powerful:\n",
    "\n",
    "> The distance between the **Anchor** and the **Positive** should be smaller than the distance between the **Anchor** and the **Negative**, by at least a certain **margin**.\n",
    "\n",
    "Think of the **margin** as a \"safety buffer.\" It's not enough for the forgery's distance to be slightly larger than the genuine one's. The loss function demands a clear separation, forcing the model to be decisive. This \"common sense\" rule can be written as:\n",
    "\n",
    "> `distance(Anchor, Positive) + margin < distance(Anchor, Negative)`\n",
    "\n",
    "When the model produces embeddings that satisfy this rule for a given triplet, the loss is zero, and no update is needed. If the rule is violated, the model is penalized, and its weights are adjusted to better separate the embeddings.\n",
    "\n",
    "#### The Mathematical Formula\n",
    "\n",
    "Mathematically, this process is captured in the following formula. Here, $f(A)$, $f(P)$, and $f(N)$ represent the embedding vectors your network creates. Let's define the squared Euclidean distance between the anchor and positive as $d(A, P)^2 = ||f(A) - f(P)||^2$.\n",
    "\n",
    "The loss function is then:\n",
    "\n",
    "$$L = \\max(0, d(A, P)^2 - d(A, N)^2 + \\text{margin})$$\n",
    "\n",
    "The $\\max(0, ...)$ function ensures that no loss is incurred if the distance to the negative, $d(A, N)$, is already greater than the distance to the positive, $d(A, P)$, by at least the margin. This allows the model to focus only on misclassified triplets.\n",
    "\n",
    "Fortunately, you don't need to implement this from scratch. PyTorch provides a highly optimized, pre-built version, <code>[nn.TripletMarginLoss](https://docs.pytorch.org/docs/stable/generated/torch.nn.TripletMarginLoss.html)</code>, that you can use directly.\n",
    "\n",
    "* Define the `TripletMarginLoss` loss function.\n",
    "    * `margin=1.0`: This sets the \"safety margin\" to 1.0. Your model will be penalized unless the distance to the negative example is at least 1.0 greater than the distance to the positive example.\n",
    "    * `p=2`: This specifies that the distance between signatures will be calculated using the L2 norm, which is the standard **Euclidean distance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b52d91-77aa-4445-96a2-8c14ae3bbdd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the Triplet Margin Loss function\n",
    "triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb3c841-5287-493b-b9d8-332bd590f4c8",
   "metadata": {},
   "source": [
    "#### Optimizer and Scheduler\n",
    "\n",
    "* Initialize the `AdamW` optimizer with an initial learning rate of `1e-3`.\n",
    "* Define a `StepLR` learning rate scheduler that will decrease the learning rate by a factor of 10 every 2 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a610206f-c24b-426e-b79b-1d23358d1b67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the AdamW optimizer to update the model's weights\n",
    "optimizer_siamese = optim.AdamW(siamese_network.parameters(), lr=1e-3)\n",
    "\n",
    "# Set step_size=2 and gamma=0.1 to decrease the LR by a factor of 10 every 2 epochs\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer_siamese, step_size=2, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e48e09-8a77-4174-857c-f17fb8aa3f12",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Orchestrating the Training Process\n",
    "\n",
    "With all the components prepared, you are ready to train your network. You will use the helper function, `training_loop_signature`, that orchestrates the entire process from start to finish. This function handles training on triplets, validates the model's performance after each epoch, and saves the best version of your model.\n",
    "\n",
    "Here are the key techniques the function uses:\n",
    "* **Training on Triplets**: For each step in a training epoch, the function feeds the model a batch of triplets. It unpacks the **Anchor**, **Positive**, and **Negative** images, generates three separate embeddings, and uses the `TripletMarginLoss` to calculate the loss and update the model's weights.\n",
    "\n",
    "* **Validating with a Threshold**: After training, the function switches to evaluation mode. Since the model outputs a distance, not a class label, accuracy is measured using a **threshold**. This value acts as your decision boundary:\n",
    "    * A genuine pair (Anchor-Positive) is correct if its distance is **less than** the threshold.\n",
    "    * A forgery pair (Anchor-Negative) is correct if its distance is **greater than or equal** to the threshold.\n",
    "    \n",
    "**A Note on Threshold vs. Margin**: It's important to distinguish between the `margin` (a hyperparameter of the `TripletMarginLoss` function) that teaches the model to create a clear separation during **training**, and the `threshold`, which is the decision boundary used during **evaluation/validation**. For best performance, the `margin` is set higher than the optimal `threshold` to ensure a clear gap is created for making accurate classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d876025a-940f-4752-8b2b-f6f036f9a3ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Uncomment and execute the line below if you wish to see the source code for the training function.\n",
    "\n",
    "# training_functions.display_code(training_functions.training_loop_signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391f5442-2cac-4850-96c0-360539afa417",
   "metadata": {},
   "source": [
    "* Run the `training_loop_signature` function to begin the process.\n",
    "\n",
    "You will train the model for 5 epochs. Feel free to experiment with this number, as a range of 5 to 10 epochs is recommended to achieve strong performance on this dataset.\n",
    "\n",
    "For this task, you will set the validation threshold to `0.8`. In a high-stakes scenario like signature verification, the cost of getting a prediction wrong (accepting a forgery) is very high. A reasonably strict threshold like `0.8` makes it more difficult for a forgery to be accepted, prioritizing security even if it means an occasional genuine signature might be flagged for a second look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746c22f1-eee5-42f8-914a-965d7fa227a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the distance threshold for validation accuracy calculation.\n",
    "threshold_dist = 0.8\n",
    "\n",
    "# Execute the main training and validation loop.\n",
    "trained_siamese = training_functions.training_loop_signature(\n",
    "    # The Siamese network model instance.\n",
    "    model=siamese_network,\n",
    "    # DataLoader for the training set.\n",
    "    train_loader=train_dataloader,\n",
    "    # DataLoader for the validation set.\n",
    "    val_loader=val_dataloader,\n",
    "    # The triplet margin loss function.\n",
    "    loss_fcn=triplet_loss,\n",
    "    # The optimizer for updating model weights.\n",
    "    optimizer=optimizer_siamese,\n",
    "    # The learning rate scheduler.\n",
    "    scheduler=scheduler,\n",
    "    # The distance threshold for validation accuracy.\n",
    "    threshold=threshold_dist,\n",
    "    # The compute device (e.g., 'cpu' or 'cuda').\n",
    "    device=device,\n",
    "    # File path to save the best performing model.\n",
    "    save_path='./saved_models/best_signature_siamese.pth',\n",
    "    # The total number of epochs for training.\n",
    "    n_epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5063a6-091f-4035-a486-fd97742dbe9b",
   "metadata": {},
   "source": [
    "### Visualizing Model Predictions\n",
    "\n",
    "An overall accuracy score is a great metric, but it is also vital to see your model in action on individual examples. This qualitative check helps you build intuition for where the model succeeds and where it might struggle.\n",
    "\n",
    "* Calculate the distance between the **Anchor-Positive** and **Anchor-Negative** pairs using your trained model.\n",
    "* Compare these distances against your `threshold_dist` to make a prediction (Genuine or Forgery)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e93af3e-71a4-4465-a678-bf1a3f77c62d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the model's performance on a few random triplets from the validation set.\n",
    "helper_utils.show_signature_val_predictions(\n",
    "    trained_siamese,\n",
    "    val_dataloader,\n",
    "    threshold=threshold_dist,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19fd4dd-3593-4907-9af0-eaa0bb7a6742",
   "metadata": {},
   "source": [
    "### Real-World Application: One-Shot Signature Verification\n",
    "\n",
    "This is where the true power of a Siamese Network becomes clear. The model has not learned to classify specific signatures; it has learned to create a meaningful *embedding space* where similar signatures are close together and different ones are far apart.\n",
    "\n",
    "This means you can now verify signatures for individuals the model has **never seen before** without retraining it. Imagine a new customer joins the bank. Instead of needing hundreds of their signatures to retrain the model, you only need **one** trusted, genuine signature on file. This is **one-shot learning**. Any new signature they provide can be compared against that single trusted instance to verify its authenticity. This makes the system incredibly scalable and practical for real-world use at the bank.\n",
    "\n",
    "To test this one-shot capability, you'll perform a verification for this new, unseen individual. The available sample images for this individual are:\n",
    "\n",
    "```\n",
    "./signature_samples/\n",
    "│\n",
    "├── Real/\n",
    "│   ├── real_6_1.jpg\n",
    "│   ├── real_6_2.jpg\n",
    "│   ├── real_6_3.jpg\n",
    "│   └── real_6_4.jpg\n",
    "│\n",
    "└── Fake/\n",
    "    ├── fake_6_1.jpg\n",
    "    ├── fake_6_2.jpg\n",
    "    ├── fake_6_3.jpg\n",
    "    ├── fake_6_4.jpg\n",
    "    └── fake_6_5.jpg\n",
    "```\n",
    "\n",
    "* First, you need to select a trusted reference signature that would be hypothetically kept \"on file\" at the bank. This will serve as your anchor image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42200bac-02c8-49d8-9e5a-ffbd6fb1ad3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the path to the real signature image\n",
    "signature_anchor = \"./signature_samples/Real/real_6_4.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9299ffb-8b1c-4de8-bed9-648940d21a5c",
   "metadata": {},
   "source": [
    "* Next, you will choose a second signature, either another genuine one or a forgery, to test against the reference image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaeff9e-9d87-400f-9d86-e5b43788afb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the path to the signature image to verify\n",
    "signature_to_verify = \"./signature_samples/Fake/fake_6_3.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c44b0b-6ef4-4b09-80a0-bab3cb0640c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "With both the trusted 'signature on file' and the signature to be verified now defined, you are ready to see the final result.\n",
    "\n",
    "* Compare the distance to your `threshold_dist` to make a final verdict: **Genuine** or **Forgery**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff63cb8-3b38-4149-b63c-6cec47374b75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform one-shot verification on the two images\n",
    "helper_utils.verify_signature(\n",
    "    model=trained_siamese, \n",
    "    genuine_path=signature_anchor, \n",
    "    test_path=signature_to_verify, \n",
    "    threshold=threshold_dist,\n",
    "    transform=val_transform, \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43477db-fa96-498e-9829-def1f5c6597f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Excellent work! You have successfully engineered a sophisticated verification system. By teaching a network to measure similarity, you have unlocked the power of one shot learning, creating a model that can validate a signature it has never encountered before. This is a powerful demonstration of what this architecture can achieve.\n",
    "\n",
    "Now, prepare to pivot. You will take the core concepts you have mastered and apply them to an entirely new challenge: detecting environmental change from satellite imagery. Let's see just how versatile this approach to similarity learning truly is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7840631-8607-4afc-8c19-fa24fae6ab87",
   "metadata": {},
   "source": [
    "## **Use Case 2:** Tracking Environmental *Change*\n",
    "\n",
    "You've seen how a Siamese Network can master a verification task. Now, you will apply the same core architecture to a powerful new application: **change detection** in satellite imagery. Your goal is to identify significant changes in vegetation by comparing \"Before\" and \"After\" images of a location.\n",
    "\n",
    "For this challenge, you will work with the [LEVIR-CD+](https://www.kaggle.com/datasets/mdrifaturrahman33/levir-cd-change-detection) dataset. While it was originally designed to track changes in buildings, you will creatively repurpose it to focus on a new metric: **the change in greenery**. This exercise mirrors a common real world scenario where you must adapt existing data to solve a novel problem. It's a perfect demonstration of how the fundamental steps for building a similarity model remain consistent across different domains.\n",
    "\n",
    "**How the Dataset Was Prepared for This Lab**\n",
    "\n",
    "To tailor the dataset for this task, a custom script automated the curation process. The key steps performed were:\n",
    "\n",
    "* **Data Augmentation**: The original dataset mostly showed greenery loss due to new construction. To create more balanced examples of Positive and Negative change, the \"Before\" and \"After\" images in each pair were randomly swapped 50% of the time.\n",
    "\n",
    "* **Greenery Calculation**: The script analyzed each image by converting it to the **HSV (Hue, Saturation, Value)** colour space. It then counted every pixel that fell within a specific color range for green to calculate the total percentage of vegetation.\n",
    "\n",
    "* **Change Categorization**: Based on this simple count of green pixels, each pair was sorted using a 5% change threshold: **Positive** (≥5% gain), **Negative** (≥5% loss), or **No Change** (any change within the -5% to +5% range).\n",
    "\n",
    "* **Image Resizing**: As a final step in the script, all images were resized to **300x300 pixels** to create a more memory efficient dataset.\n",
    "\n",
    "The 5% threshold was an arbitrary value chosen for this lab. In your own use cases, you could set this to any number that best defines a \"significant\" change for your specific problem.\n",
    "\n",
    "As a reminder, this automated sorting process is not perfect. You will likely encounter some noisy or miscategorized images. This presents a realistic industry challenge, and your goal remains the same: to build a robust model that can learn effectively from imperfect data.\n",
    "\n",
    "* Run the next cells below to see a statistical overview of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e693ce-1611-4cef-824d-77264bbdf1f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the path to the root directory containing the change dataset.\n",
    "change_data_dir = './levir_cd_plus_simulated/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ae5cbe-5e57-447b-bc30-9874f4d0cfe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scan the data directory and display the statistical summary.\n",
    "helper_utils.display_change_dataset_stats(change_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa87189-4709-4701-ac8e-41644aefd611",
   "metadata": {},
   "source": [
    "* Run the cell below to visualize a random \"Before\" and \"After\" pair from each of the three change categories (`Positive`, `Negative`, and `No_Change`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72afed03-894e-45fa-a53c-83667d1101f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display a side-by-side comparison of pairs of each class from the dataset.\n",
    "helper_utils.display_random_change_pairs(change_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb98da6-0160-43ad-b81d-3abb05ed92a9",
   "metadata": {},
   "source": [
    "### Building the Change Detection Dataset Class\n",
    "\n",
    "For this new use case, your approach must adapt. Instead of feeding the network **triplets**, you will now work with **pairs** of images. The essential learning unit for your model now becomes a \"Before\" image, an \"After\" image, and a label describing the change between them.\n",
    "\n",
    "To generate these pairs efficiently, the `ChangeDetectionDataset` class is designed for the following directory structure:\n",
    "\n",
    "```\n",
    "./levir_cd_plus_simulated/\n",
    "│\n",
    "├── Negative/\n",
    "│   ├── After/\n",
    "│   │   ├── train_1.png\n",
    "│   │   └── ... (more 'after' images)\n",
    "│   │\n",
    "│   └── Before/\n",
    "│       ├── train_1.png\n",
    "│       └── ... (more 'before' images)\n",
    "│\n",
    "├── No_Change/\n",
    "│   ├── After/\n",
    "│   │   ├── train_1.png\n",
    "│   │   └── ... (more 'after' images)\n",
    "│   │\n",
    "│   └── Before/\n",
    "│       ├── train_1.png\n",
    "│       └── ... (more 'before' images)\n",
    "│\n",
    "└── Positive/\n",
    "    ├── After/\n",
    "    │   ├── train_1.png\n",
    "    │   └── ... (more 'after' images)\n",
    "    │\n",
    "    └── Before/\n",
    "        ├── train_1.png\n",
    "        └── ... (more 'before' images)\n",
    "```\n",
    "\n",
    "This categorized structure allows the class to load a \"Before\" image, find its corresponding \"After\" image, and assign the correct label (`Positive`, `Negative`, or `No_Change`). While it's still a standard PyTorch `Dataset`, its internal logic differs significantly from the triplet class.\n",
    "\n",
    "Here’s how its main components work:\n",
    "\n",
    "* **`__init__(self, base_dir, ...)`**: \n",
    "> This constructor still performs a **one time** setup scan. However, instead of mapping signatures to individuals, it builds a single, complete list of all valid (`before_path`, `after_path`, `label`) image pairs found across all categories.\n",
    "\n",
    "* **`__len__(self)`**: \n",
    "> This method returns the **actual total number of pairs** in the dataset. Unlike the \"virtual\" length in the previous class, this is a fixed number determined by the files on disk.\n",
    "\n",
    "* **`__getitem__(self, index)`**: \n",
    "> This is a fundamental change from the triplet dataset. The `__getitem__` method now **uses the `idx` argument** to retrieve a specific image pair and its label from the list created during initialization. There is no on the fly random sampling; it simply fetches the item at the requested index.\n",
    "\n",
    "* **`_load_image(self, path)`**: \n",
    "> This is the same robust, private helper method used before. It ensures each image is loaded correctly and efficiently, using a context manager to prevent resource leaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cfc359-f8e6-46cf-9dce-2480cd9c021a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ChangeDetectionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for loading 'Before' and 'After' image pairs\n",
    "    for a change detection task.\n",
    "\n",
    "    This class scans a directory where subdirectories are named\n",
    "    'Positive', 'Negative', and 'No_Change', each containing 'Before' and\n",
    "    'After' subfolders with corresponding image pairs.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset by scanning the data directory and organizing file paths.\n",
    "        \n",
    "        Args:\n",
    "            base_dir (str): Path to the root directory which contains the\n",
    "                            'Positive', 'Negative', and 'No_Change' folders.\n",
    "            transform (callable, optional): PyTorch transforms to be applied to each image.\n",
    "        \"\"\"\n",
    "        self.base_dir = base_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Define a mapping from class names to integer labels for convenience.\n",
    "        self.class_to_label = {'Positive': 0, 'Negative': 1, 'No_Change': 2}\n",
    "        \n",
    "        # Build the complete list of all available image pairs from the source directory.\n",
    "        self.image_pairs = self._create_image_pairs()\n",
    "        \n",
    "        # Raise an error if the dataset directory is empty or improperly structured.\n",
    "        if not self.image_pairs:\n",
    "            raise RuntimeError(f\"No valid image pairs found in {base_dir}. Check directory structure.\")\n",
    "\n",
    "    def _create_image_pairs(self):\n",
    "        \"\"\"Scans the directory to build a list of (before_path, after_path, label) tuples (one-time setup).\"\"\"\n",
    "        image_pairs = []\n",
    "        # Iterate through each change category ('Positive', 'Negative', 'No_Change').\n",
    "        for class_name, label in self.class_to_label.items():\n",
    "            class_dir = os.path.join(self.base_dir, class_name)\n",
    "            before_dir = os.path.join(class_dir, 'Before')\n",
    "            after_dir = os.path.join(class_dir, 'After')\n",
    "            \n",
    "            # Skip this category if its 'Before' directory does not exist.\n",
    "            if not os.path.isdir(before_dir):\n",
    "                continue\n",
    "\n",
    "            # Iterate through all files in the 'Before' directory.\n",
    "            for filename in os.listdir(before_dir):\n",
    "                if filename.lower().endswith(('.png', '.jpg')):\n",
    "                    # Construct the full paths for the 'Before' and corresponding 'After' images.\n",
    "                    before_path = os.path.join(before_dir, filename)\n",
    "                    after_path = os.path.join(after_dir, filename)\n",
    "                    \n",
    "                    # Add the pair to the list only if the corresponding 'After' image exists.\n",
    "                    if os.path.exists(after_path):\n",
    "                        image_pairs.append((before_path, after_path, label))\n",
    "        return image_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of image pairs in the dataset.\"\"\"\n",
    "        return len(self.image_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Generates and returns one pair of images and its corresponding label.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): The index of the image pair to retrieve from the dataset.\n",
    "            \n",
    "        Returns:\n",
    "            tuple: A tuple containing (before_img, after_img, label).\n",
    "        \"\"\"\n",
    "        # Retrieve the file paths and label for the requested index.\n",
    "        before_path, after_path, label = self.image_pairs[idx]\n",
    "        \n",
    "        # Load the 'before' and 'after' images from their respective paths.\n",
    "        before_img = self._load_image(before_path)\n",
    "        after_img = self._load_image(after_path)\n",
    "            \n",
    "        return before_img, after_img, label\n",
    "        \n",
    "    def _load_image(self, path):\n",
    "        \"\"\"\n",
    "        Helper function to robustly load a single image from a given path.\n",
    "\n",
    "        Args:\n",
    "            path (str): The file path of the image to load.\n",
    "\n",
    "        Returns:\n",
    "            The loaded and transformed image, typically a torch.Tensor.\n",
    "        \"\"\"\n",
    "        # Use a context manager to ensure the file is properly closed after loading.\n",
    "        with Image.open(path) as img:\n",
    "            # Ensure the image is in RGB format, as many networks expect 3 channels.\n",
    "            image = img.convert(\"RGB\")\n",
    "            # Apply any specified transformations (e.g., resizing, tensor conversion).\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3bce44-3107-41ac-88ef-b6221f431ac4",
   "metadata": {
    "tags": []
   },
   "source": [
    "* Now that your `ChangeDetectionDataset` class is defined, create the main dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d96ff4-3278-43ff-9b9f-87eff7a61a48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the full dataset object. \n",
    "full_change_dataset = ChangeDetectionDataset(change_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1188bc-4c0a-4c9d-9a5c-131072d7fb03",
   "metadata": {},
   "source": [
    "### Preparing the Change Data for Training\n",
    "\n",
    "* First, define your pipelines of transformations for training and validation data.\n",
    "    * Because you will be using a model that was pre-trained on the ImageNet dataset, it is essential to normalize your images using the standard ImageNet `mean` and `std` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c179aff6-97e7-4a5c-b5cd-28fe7d3779d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ImageNet normalization statistics\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Transformations for the training set (with augmentation)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "# Transformations for validation set (no augmentation)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e789d662-4a27-4f68-8ab3-986f8c5c6c50",
   "metadata": {
    "tags": []
   },
   "source": [
    "* Now, you will split the main dataset and prepare the final `DataLoaders` for training.\n",
    "    * Divide the full dataset into an 80% training set and a 20% validation set.\n",
    "    * Create dataloaders for each set to handle batching and shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98be1ae6-970f-4592-bbf4-54374a2222b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the full dataset into training and validation sets.\n",
    "train_dataset, val_dataset = helper_utils.create_change_datasets_splits(\n",
    "    full_dataset=full_change_dataset,\n",
    "    train_split=0.8, \n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform\n",
    ")\n",
    "\n",
    "# Create a DataLoader for the training set.\n",
    "train_dataloader_change = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# Create a DataLoader for the validation set.\n",
    "val_dataloader_change = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Print the final \"virtual\" size of each dataset split.\n",
    "print(f\"Total training pairs: {len(train_dataset)}\")\n",
    "print(f\"Total validation pairs: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dd23c1-e150-4436-97b8-5ca004f6a581",
   "metadata": {},
   "source": [
    "### Visualizing a Training Sample\n",
    "\n",
    "* Visualize a random sample to confirm everything is working as expected.\n",
    "    * Since the training data uses augmentations like random rotation and flipping, the images will appear transformed. While they might seem slightly misaligned or different, be assured that they are the correct \"Before\" and \"After\" pair of the same location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3442bb-57ed-4672-a3ed-d2ac63b3435e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize a random pair from the training dataloader.\n",
    "helper_utils.show_random_pair(train_dataloader_change)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fd5fbb-54c0-4881-aee9-0103c6d76e17",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A New Backbone for a New Challenge\n",
    "\n",
    "Remember the modular `SiameseNetwork` wrapper you built earlier? Its flexible design, which allows for an interchangeable backbone and can process image pairs, is about to pay off. You can reuse that main network class without any changes.\n",
    "\n",
    "For this more complex satellite imagery task, you will upgrade the backbone. Instead of the simple CNN from the first use case, you will use a powerful, pre-trained [EfficientNet](https://docs.pytorch.org/vision/main/models/efficientnet.html) model.\n",
    "\n",
    "You will load the pre-trained weights and prepare the model for **fine-tuning**, where all of its layers will be trained. The modification process is one you are already familiar with: you will replace the model's final classifier head. As you saw before, instead of swapping it for a layer with a set number of classes, you will replace it with a linear layer that outputs a vector of your desired **embedding dimension**.\n",
    "\n",
    "* Load the **EfficientNet** embedding network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aac5ed0-475d-424b-8591-35e6c7d3cb2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the EfficientNet-based embedding network\n",
    "efficientnet_embedding = helper_utils.get_efficientnet_embedding_backbone(embedding_dim=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fd7fe4-311c-4b85-a5eb-ac2ea18be649",
   "metadata": {},
   "source": [
    "* Now, assemble your complete model by passing the `efficientnet_embedding` network into the main `SiameseNetwork` wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6331f3-589a-4a83-8ac2-b21cf82dcf8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the main Siamese model, using the EfficientNet network as its base\n",
    "siamese_efficientnet = SiameseNetwork(embedding_network=efficientnet_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858b99bb-2493-4f68-9ff8-e3ab4660562c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Teaching the Network to Detect Change\n",
    "\n",
    "With your model and data ready, the next step is to define the loss function and optimizer. For this task, the process is slightly different from the first use case because you must account for the imbalanced nature of your dataset.\n",
    "\n",
    "#### Addressing Class Imbalance\n",
    "\n",
    "You may have noticed that for the first use case, Signature Verification, you did not explicitly handle class imbalance. This is because that model was trained with **Triplet Loss**, which focuses on learning the relative distances between images in an embedding space. Its goal was not to classify an image into a specific category, so the number of examples per individual was less of a concern.\n",
    "\n",
    "For this Change Detection task, however, handling class imbalance is an **essential** step. Your dataset is divided into three distinct classes (`Positive`, `Negative`, `No_Change`) that have a different number of examples. If you ignore this, the model will become biased towards the most common class and will perform poorly on the rare but important change events. By calculating and using class weights, you force the model to pay more attention to the under represented classes, leading to a more balanced and useful result.\n",
    "\n",
    "* Run the cell below to compute the specific weights for each class in your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2be028-3bb9-4ee7-bad9-b7f0f922a584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the class weights using the full and train datasets.\n",
    "class_weights = helper_utils.compute_change_class_weights(\n",
    "    train_dataset=train_dataset,\n",
    "    full_untransformed_dataset=full_change_dataset\n",
    ")\n",
    "\n",
    "# Print class weights\n",
    "label_to_class = {v: k for k, v in full_change_dataset.class_to_label.items()}\n",
    "for i, weight in enumerate(class_weights):\n",
    "    print(f\"Class '{label_to_class[i]}': Weight = {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938b06b4-9ee3-4ee3-b0a3-ab427f6fc251",
   "metadata": {},
   "source": [
    "#### The Weighted Contrastive Loss\n",
    "\n",
    "For your first task, `TripletMarginLoss` was the perfect tool because your objective was to compare relative distances within a set of three images. Now, your goal has shifted. You are working with labeled **pairs** of images and must also account for class imbalance. This requires a new loss function designed for this specific challenge: the `WeightedContrastiveLoss`.\n",
    "\n",
    "This function is based on the principles of **Contrastive Loss**, which is designed specifically for learning from pairs. It elegantly adapts your three class problem (`Positive`, `Negative`, `No_Change`) into a binary similarity task:\n",
    "\n",
    "* **Similar Pairs** (labeled as `No_Change`): For these pairs, the loss function encourages the model to produce embeddings that are extremely close together, minimizing their distance.\n",
    "* **Dissimilar Pairs** (labeled as `Positive` or `Negative`): For these pairs, the loss function demands a clear separation. It pushes their embeddings far apart by enforcing a minimum **margin**.\n",
    "\n",
    "The most important component, however, is the \"Weighted\" aspect. The final loss calculated for each pair is multiplied by the class weight you computed earlier. This ensures that a mistake made on a rare class (like `Positive` change) is penalized more heavily, compelling your model to learn to detect these infrequent but important events effectively.\n",
    "\n",
    "#### The Mathematical Formula\n",
    "\n",
    "Let's break down how this works. The function first calculates the standard Euclidean distance between the two embeddings, $d = ||f(I_1) - f(I_2)||$. It then uses a binary label, $y$, where $y=0$ for a similar pair (`No_Change`) and $y=1$ for a dissimilar pair (`Positive` or `Negative`).\n",
    "\n",
    "The standard contrastive loss is then calculated as:\n",
    "\n",
    "$$L_{contrastive} = (1-y) \\cdot d^2 + y \\cdot \\max(0, \\text{margin} - d)^2$$\n",
    "\n",
    "This formula has two distinct parts that work in opposition:\n",
    "* If the pair is **similar** ($y=0$), the second term disappears. The loss becomes $L = d^2$. The model is penalized for *any* distance between the embeddings, pushing it to make them identical.\n",
    "* If the pair is **dissimilar** ($y=1$), the first term disappears. The loss becomes $L = \\max(0, \\text{margin} - d)^2$. The model is only penalized if the distance $d$ is *less than* the margin, forcing it to create a clear separation.\n",
    "\n",
    "Finally, to create the weighted loss, this result is scaled by the class weight, $w$:\n",
    "\n",
    "$$L_{weighted} = w \\cdot L_{contrastive}$$\n",
    "\n",
    "This final step ensures that the updates to your model's parameters are appropriately influenced by the rarity of each class, creating a more balanced and accurate final model.\n",
    "\n",
    "> 💡 **A 3-Class Dataset for a 2-Class Loss?**\n",
    ">\n",
    "> You might be wondering: why did you go to the trouble of preparing a dataset with three classes (`Positive`, `Negative`, `No_Change`), only to use a loss function that simplifies this into a binary problem (\"Change\" vs. \"No Change\")? And if it's a binary problem, what happens to the three class weights you just calculated?\n",
    ">\n",
    "> This is a deliberate and powerful strategy. The loss function works in two stages. First, it simplifies the comparison to a binary one: is the pair *similar* (`No_Change`) or *dissimilar* (`Positive` or `Negative`)? After calculating a preliminary loss based on this, it looks back at the **original three class label** and applies the specific weight for that class. So, a `Positive` pair's loss is scaled by the `Positive` weight, and a `Negative` pair's loss is scaled by the `Negative` weight.\n",
    ">\n",
    "> By doing this, you are not teaching the model to be a **classifier** that sorts image pairs into three rigid buckets. Instead, you are teaching it to be a **distance metric**, like a very intelligent ruler. The output is not a simple category, but a meaningful **distance score**. A small distance means \"no change,\" and a large distance means \"significant change.\"\n",
    ">\n",
    "> This is the key advantage. In the next sections, you will see how you can analyze all the distance scores from your validation set to find an **optimal distance threshold**. This gives you the power to tune the model's sensitivity after it has been trained, which is a huge benefit compared to using `CrossEntropyLoss` directly. This distance based method often provides more flexibility and control over the final decision making process.\n",
    "\n",
    "* Define the `WeightedContrastiveLoss` loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce775ba2-1827-4102-85d7-810912096656",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WeightedContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    A contrastive loss function that incorporates class weights to handle imbalance.\n",
    "    \n",
    "    It adapts a multi-class problem into a binary similarity problem where\n",
    "    'No_Change' is 'similar' and 'Positive'/'Negative' are 'dissimilar'.\n",
    "    \"\"\"\n",
    "    def __init__(self, device, margin=1.0, class_weights=None):\n",
    "        \"\"\"\n",
    "        Initializes the weighted contrastive loss function.\n",
    "        \n",
    "        Args:\n",
    "            device (torch.device): The device to move class weights to.\n",
    "            margin (float): The margin for dissimilar pairs.\n",
    "            class_weights (torch.Tensor, optional): A tensor of weights for each class.\n",
    "                                                      Shape: (num_classes,).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "        self.device = device\n",
    "        \n",
    "        # Move weights to the correct device once during initialization for efficiency.\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = class_weights.to(self.device)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        \"\"\"\n",
    "        Computes the weighted contrastive loss for a batch of embeddings.\n",
    "\n",
    "        Args:\n",
    "            output1 (torch.Tensor): Embeddings for the first set of images.\n",
    "            output2 (torch.Tensor): Embeddings for the second set of images.\n",
    "            label (torch.Tensor): The multi-class labels (0, 1, or 2) from the dataset.\n",
    "        \"\"\"\n",
    "        # Calculate the pairwise Euclidean distance between the output embeddings.\n",
    "        distances = F.pairwise_distance(output1, output2)\n",
    "        \n",
    "        # Convert multi-class labels (0, 1, 2) to binary similarity labels (1, 1, 0).\n",
    "        # A label of 2 ('No_Change') is considered similar (0), others are dissimilar (1).\n",
    "        binary_label = (label != 2).float()\n",
    "\n",
    "        # Calculate the contrastive loss for each sample in the batch.\n",
    "        loss_per_sample = (\n",
    "            # Loss for similar pairs aims to minimize the distance.\n",
    "            (1 - binary_label) * distances.pow(2) +\n",
    "            # Loss for dissimilar pairs aims to make the distance larger than the margin.\n",
    "            binary_label * torch.clamp(self.margin - distances, min=0).pow(2)\n",
    "        )\n",
    "\n",
    "        # Apply class-specific weights to the loss if they are provided.\n",
    "        if self.class_weights is not None:\n",
    "            # Gather the correct weight for each sample using its original multi-class label.\n",
    "            weights = self.class_weights[label.long()]\n",
    "            \n",
    "            # Multiply each sample's loss by its corresponding class weight.\n",
    "            loss_per_sample = loss_per_sample * weights\n",
    "            \n",
    "        # Return the mean of the (potentially weighted) losses for the batch.\n",
    "        return loss_per_sample.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71d3744-2ebc-4b2f-9380-3e77a7924c24",
   "metadata": {},
   "source": [
    "* Initialize your custom `WeightedContrastiveLoss` function.\n",
    "    * `margin=2.0`: This sets the desired separation distance for dissimilar pairs (Positive or Negative change).\n",
    "    * `class_weights=class_weights`: You pass the `class_weights` tensor you calculated earlier, which allows the loss function to counteract class imbalance.\n",
    "    * `device=device`: This ensures that the class weights tensor is moved to the correct compute device (e.g., your GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1843011-2c2e-4307-941c-c66233579389",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the custom weighted contrastive loss function with the calculated class weights.\n",
    "contrastive_loss = WeightedContrastiveLoss(margin=2.0, class_weights=class_weights, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b4e263-951e-4af1-9dcf-b0437ec4ad7b",
   "metadata": {},
   "source": [
    "#### Optimizer and Scheduler\n",
    "\n",
    "For this task, you'll use a more dynamic learning rate scheduler that can adapt to the model's performance during training.\n",
    "\n",
    "* Initialize the `AdamW` optimizer with an initial learning rate of `1e-3`.\n",
    "* Define a `ReduceLROnPlateau` scheduler. This scheduler will monitor the validation loss and automatically reduce the learning rate when it stops improving.\n",
    "    * It will wait for **2 full epochs** (`patience=2`) of no improvement before reducing the learning rate.\n",
    "    * When triggered, it will reduce the learning rate by a **factor of 0.2** (`factor=0.2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b5f061-8ea2-4e08-a01f-867d9523a351",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the AdamW optimizer for the new EfficientNet-based model\n",
    "optimizer_change = optim.AdamW(siamese_efficientnet.parameters(), lr=1e-3)\n",
    "\n",
    "# Initialize the new, more flexible scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_change,\n",
    "    mode='min',      # Reduce LR when the validation loss stops decreasing\n",
    "    factor=0.2,      # New LR = LR * factor\n",
    "    patience=2,      # Wait 2 epochs with no improvement before reducing LR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7e96d0-ab7d-4ac9-bbfc-3a656f590f14",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Orchestrating the Training Process\n",
    "\n",
    "With all the components prepared, you are ready to train your new network. You will use the `training_loop_change` function, which orchestrates the entire process. The key difference in this approach is a strategic one: unlike the signature task where you used a predefined threshold for validation, this loop's only goal is to find the model that achieves the **lowest validation loss**.\n",
    "\n",
    "This two step process is more flexible and stands in contrast to the previous method. For a **high stakes task** like signature verification, you provided a fixed threshold during validation. This approach gives you more **direct control**, forcing the model to create an embedding space tailored to your specific decision boundary.\n",
    "\n",
    "Here, the strategy is more exploratory. You first let the model train to create the best possible embedding space. **Only after training is complete** will you perform a separate evaluation to find the data driven **optimal threshold** that yields the best performance. While this means you do not predefine the final threshold value, separating the act of learning features from making a final decision often leads to a more robust result. Both techniques are valid, and the best choice depends on the requirements of your task.\n",
    "\n",
    "Here's a breakdown of how the function works:\n",
    "\n",
    "* **Training on Pairs**: For each step in a training epoch, the function feeds the model a batch of (\"Before\" image, \"After\" image, label) pairs. It generates two embeddings and uses the `WeightedContrastiveLoss` to calculate the loss and update the model's weights.\n",
    "\n",
    "* **Validating on Loss**: The function's sole purpose in the validation phase is to measure the average loss on the validation set. The model that achieves the lowest validation loss is considered the best, as this indicates it has learned the most effective embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f8beaa-8142-479f-9147-a101cc6874f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Uncomment and execute the line below if you wish to see the source code for the training function.\n",
    "\n",
    "# training_functions.display_code(training_functions.training_loop_change)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23456a66-2ddf-4a8e-899b-c03130888ca8",
   "metadata": {},
   "source": [
    "* You will train the model for 10 epochs. Feel free to experiment with this number, as a range of 10 to 15 epochs is recommended to achieve strong performance on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961d656c-6495-4272-bcee-3838f5480304",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the training process\n",
    "trained_efficientnet = training_functions.training_loop_change(\n",
    "    # The Siamese network model built on EfficientNet.\n",
    "    model=siamese_efficientnet,\n",
    "    # DataLoader for the training set.\n",
    "    train_loader=train_dataloader_change,\n",
    "    # DataLoader for the validation set.\n",
    "    val_loader=val_dataloader_change,\n",
    "    # The weighted contrastive loss function.\n",
    "    loss_fcn=contrastive_loss,\n",
    "    # The optimizer for updating model weights.\n",
    "    optimizer=optimizer_change,\n",
    "    # The learning rate scheduler.\n",
    "    scheduler=scheduler,\n",
    "    # The compute device (e.g., 'cpu' or 'cuda').\n",
    "    device=device,\n",
    "    # File path to save the model with the lowest validation loss.\n",
    "    save_path='./saved_models/best_change_siamese.pth',\n",
    "    # The total number of epochs for training.\n",
    "    n_epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29f6857-b998-440e-9317-5aeeb6fbd32d",
   "metadata": {},
   "source": [
    "#### Finding the Optimal Threshold\n",
    "\n",
    "Your model has now been trained to create a meaningful embedding space by focusing on a single goal: minimizing validation loss. As promised, it is now time for the second step of your strategy: turning the model's raw distance outputs into concrete, accurate decisions.\n",
    "\n",
    "The `evaluation_loop` function is designed for this specific purpose. Its job is to analyze all the distances your trained model produces for the validation set and find the single **optimal distance threshold** that best separates \"Change\" from \"No Change\" pairs.\n",
    "\n",
    "Here is a summary of its methodical approach:\n",
    "\n",
    "* Collect All Distances and Labels\n",
    "* Test Candidate Thresholds\n",
    "* Calculate Accuracy for Each Threshold\n",
    "* Identify and Return the Best Threshold\n",
    "\n",
    "This process will give you the optimal decision boundary for this model, which you can then use to make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de534a1-1ac5-4bed-a057-9f159038cad3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Uncomment and execute the line below if you wish to see the source code for the avaluation function.\n",
    "\n",
    "# training_functions.display_code(training_functions.evaluation_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb127d70-47cd-4c57-bda8-135a1445601f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model that was trained\n",
    "optimal_threshold = training_functions.evaluation_loop(\n",
    "    # Your fine-tuned EfficientNet Siamese model.\n",
    "    model=trained_efficientnet,\n",
    "    # The DataLoader for the change detection validation set.\n",
    "    data_loader=val_dataloader_change,\n",
    "    # The same weighted contrastive loss used during training.\n",
    "    loss_fcn=contrastive_loss,\n",
    "    # The compute device (e.g., 'cpu' or 'cuda').\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bebf98-5806-4763-9e01-de93d9f9c701",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Seeing Your Model in Action\n",
    "\n",
    "Excellent work. You have successfully trained a powerful embedding model and used a data-driven approach to find its optimal decision threshold. With these two key components, your change detection system is now complete.\n",
    "\n",
    "It is time to put it all together and see your model in action. First, you will perform a full quantitative evaluation using a **classification report** and **confusion matrix** to get a clear picture of your model's overall performance. Then, you will visualize individual predictions to get a more qualitative feel for its decisions. Finally, you will apply your complete system to a new, unseen pair of images, simulating a real-world prediction from start to finish.\n",
    "\n",
    "* Perform a full quantitative evaluation by generating a classification report and a confusion matrix for your model's performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed83c2f2-4a50-4793-b621-b832da4796ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call the function with your trained model, validation loader, and optimal threshold\n",
    "helper_utils.plot_confusion_matrix_and_metrics(\n",
    "    model=trained_efficientnet,\n",
    "    data_loader=val_dataloader_change,\n",
    "    threshold=optimal_threshold,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6dfa16-ce4b-4ed5-abcb-864fa58692a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### A Two-Step Approach Prediction\n",
    "\n",
    "Now for the interesting part: how do you get a three class prediction (`Positive`, `Negative`, `No Change`) from a model that was trained for a binary task (\"Change\" vs. \"No Change\")? \n",
    "\n",
    "You will use a powerful and common two step approach.\n",
    "\n",
    "**First**, your Siamese network acts as the primary change detector. It learned to be a **distance metric**, not a classifier. It uses the **optimal threshold** you found to make one simple decision: *has a significant change occurred?*\n",
    "\n",
    "The rule is straightforward: any distance calculated by the model that is **at or below** this threshold is classified as \"No Change.\" Any distance **above** the threshold is flagged as a \"Change\" and passed to the next step.\n",
    "\n",
    "**Second**, *only if the model flags a \"Change\"*, you apply a simple heuristic to determine the *type* of change. This is a common and powerful approach in many applications. For this lab, the heuristic is the same **HSV color analysis** logic that was used to create the original dataset labels (a greater than **5% increase or decrease** in greenery). In your own use cases, you could substitute this with any set of logical rules that fit your specific problem.\n",
    "\n",
    "This is a valid technique, not a shortcut. Your Siamese network was **never trained on the color analysis data**; it learned from raw pixels alone to identify complex visual changes. The color analysis is just an efficient, separate rule applied after the model has done the difficult work.\n",
    "\n",
    "* Run the cell below to see this two step prediction process in action on a random sample from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8448bf15-197f-4e56-97b0-9720da94372f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the model's performance on a few random samples from the validation set.\n",
    "helper_utils.show_change_val_predictions(\n",
    "    model=trained_efficientnet,\n",
    "    val_loader=val_dataloader_change,\n",
    "    model_threshold=optimal_threshold,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883daa8c-a910-4945-8c6e-4a9b8a74db5a",
   "metadata": {},
   "source": [
    "#### Real-World Application: Predicting Change on Unseen Data\n",
    "\n",
    "This is the final test for your complete system, which consists of your trained model and the optimal threshold. Your model has not memorized specific locations; instead, it has learned the general visual characteristics of what constitutes a significant change in greenery. Now you will see if it can apply that knowledge to a pair of images it has **never encountered before**.\n",
    "\n",
    "This tests the model's ability to **generalize** its knowledge to new locations. This is an essential test for any production model: proving it can perform reliably on new, unseen data from the same domain. To perform this test, you may use the following set of sample pair images that were held out from all training and validation:\n",
    "\n",
    "```\n",
    "./change_samples/\n",
    "│\n",
    "├── Negative/\n",
    "│   ├── After/\n",
    "│   │   ├── train_160.png\n",
    "│   │   ├── train_502.png\n",
    "│   │   └── train_944.png\n",
    "│   │\n",
    "│   └── Before/\n",
    "│       ├── train_160.png\n",
    "│       ├── train_502.png\n",
    "│       └── train_944.png\n",
    "│\n",
    "├── No_Change/\n",
    "│   ├── After/\n",
    "│   │   ├── train_1.png\n",
    "│   │   ├── train_241.png\n",
    "│   │   └── train_523.png\n",
    "│   │\n",
    "│   └── Before/\n",
    "│       ├── train_1.png\n",
    "│       ├── train_241.png\n",
    "│       └── train_523.png\n",
    "│\n",
    "└── Positive/\n",
    "    ├── After/\n",
    "    │   ├── train_25.png\n",
    "    │   ├── train_624.png\n",
    "    │   └── train_803.png\n",
    "    │\n",
    "    └── Before/\n",
    "        ├── train_25.png\n",
    "        ├── train_624.png\n",
    "        └── train_803.png\n",
    "```\n",
    "\n",
    "* First, you will define the path to the \"Before\" image you want to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd5dae2-9e7d-47dd-bea1-50fd619c4750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the path to the \"before\" image\n",
    "before_image = \"./change_samples/Positive/Before/train_803.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d36ba54-0916-421a-9157-19f73204f82d",
   "metadata": {},
   "source": [
    "* Next, you will define the path for the corresponding \"After\" image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec845210-a45c-4fd1-9dc6-237840445106",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the path to the \"after\" image\n",
    "after_image = \"./change_samples/Positive/After/train_803.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036112c8-dc60-484a-b064-033fbd8ef94d",
   "metadata": {},
   "source": [
    "* Finally, you will run the prediction function to get a final verdict from your complete system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cde84bd-a59e-424a-ad28-4135a1972c19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the model's performance on a unseen sample pairs.\n",
    "helper_utils.predict_greenery_change(\n",
    "    model=trained_efficientnet, \n",
    "    before_path=before_image, \n",
    "    after_path=after_image, \n",
    "    model_threshold=optimal_threshold, \n",
    "    transform=val_transform, \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d6ec56-a097-4682-b1cd-c3203f66d63b",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You have successfully navigated the fascinating world of similarity learning, building a complete and powerful system from the ground up. You have seen firsthand how a single, elegant architecture, the Siamese Network, can be adapted to solve two completely different and complex real-world problems.\n",
    "\n",
    "Throughout this lab, you have moved beyond traditional classification and mastered the art of teaching a model to understand similarity. You constructed a **modular Siamese Network** with an interchangeable backbone, then applied it to a security task by training it for **one shot signature verification** using `TripletMarginLoss`. You then pivoted to solve an environmental challenge, adapting your model with a **pre-trained backbone** and a custom `WeightedContrastiveLoss` to handle class imbalance. Finally, you explored **advanced evaluation strategies** and implemented a hybrid prediction system, combining your deep learning model with a logical heuristic.\n",
    "\n",
    "The skills you've developed here are incredibly versatile. The ability to measure similarity is at the heart of many advanced AI applications, such as face recognition, content recommendation systems, and plagiarism or duplicate detection.\n",
    "\n",
    "You now have the tools and the strategic understanding to apply similarity learning to your own unique challenges. Well done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
