{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd9d30dc-9d65-4635-bf15-33132255830a",
   "metadata": {},
   "source": [
    "# Beyond Shortcuts: DenseNet Architecture\n",
    "\n",
    "You've already built a solid foundation with linear models where data flows sequentially through a stack of layers. Now, you'll explore a more advanced design pattern that gives you direct control over the flow of information through your network. You've seen how ResNet's skip connections help with training deep networks by preserving the gradient. This lab takes that idea a step further by asking a powerful question: instead of reusing just one previous layer's output, what if a layer could access the features from *all* preceding layers? \n",
    "\n",
    "This is the core philosophy behind **DenseNet**, an architecture designed for maximum feature reuse and parameter efficiency. By building this model from its fundamental components, you'll gain a deep, practical understanding of how modern architectures are designed to solve complex problems.\n",
    "\n",
    "In this lab, you will:\n",
    "\n",
    "* Construct a custom **DenseNet** from its core building blocks: the `DenseLayer`, the `DenseBlock`, and the `TransitionLayer`.\n",
    "\n",
    "* Train your custom-built model from scratch on a challenging land use dataset with many classes but few images per class.\n",
    "\n",
    "* Apply the professional workflow of **transfer learning**, using a pre-trained `densenet121` to perform feature extraction.\n",
    "\n",
    "* Compare the from-scratch and feature-extraction approaches to see firsthand the dramatic efficiency and performance gains of leveraging pre-trained models.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdf1105",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db09bdba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import torchinfo\n",
    "\n",
    "import helper_utils\n",
    "\n",
    "# Set seed\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab57e96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup device priority: CUDA -> MPS -> CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using device: CUDA\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"Using device: MPS (Apple Silicon GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Using device: CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74530e2-f554-48a8-b251-a6c732f0e06c",
   "metadata": {},
   "source": [
    "## Upping the Ante: A Land Use Challenge\n",
    "\n",
    "You have already successfully trained a model to classify broad landscapes from an aerial perspective. Now, you will take on a more intricate task: moving from identifying general scenery to classifying specific land use. This requires a higher level of precision. Your model will not just find a residential zone; it will need to learn the subtle visual cues that distinguish between varying housing densities, different forms of architectural construction, and the unique textural patterns that separate a parking lot from a mobile home park or harbor.\n",
    "\n",
    "This new task comes with a distinct set of challenges. The [UC Merced Land Use dataset](https://www.kaggle.com/datasets/zeadomar/uc-mercedland) presents you with:\n",
    "\n",
    "* **More Granular Categories**: You will be classifying **21 distinct classes**, a significant increase in complexity that demands a more discerning model.\n",
    "\n",
    "* **Far Less Data Per Class**: The dataset is constructed with only **100 images for each category**. This scarcity presents a realistic professional challenge, demanding an architecture so efficient that it can deliver strong performance despite the limited data.\n",
    "\n",
    "A look at the directory structure reveals the granularity of the challenge ahead:\n",
    "\n",
    "```\n",
    "./UCMerced_LandUse/Images/\n",
    "├── agricultural/\n",
    "├── baseballdiamond/\n",
    "├── buildings/\n",
    "├── denseresidential/\n",
    "├── harbor/\n",
    "├── mediumresidential/\n",
    "├── mobilehomepark/\n",
    "├── parkinglot/\n",
    "├── runway/\n",
    "├── sparseresidential/\n",
    "├── storagetanks/\n",
    "├── tenniscourt/\n",
    "├── uc_airplane/\n",
    "├── uc_beach/\n",
    "├── uc_chaparral/\n",
    "├── uc_forest/\n",
    "├── uc_freeway/\n",
    "├── uc_golfcourse/\n",
    "├── uc_intersection/\n",
    "├── uc_overpass/\n",
    "└── uc_river/\n",
    "```\n",
    "\n",
    "* Run the next cells to define the path to your dataset and initialize a corresponding list of formatted class names for clear labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64d4e16-9ce1-483e-bc5e-f5765069209c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the path to the root directory where the image dataset is stored.\n",
    "dataset_path = \"./UCMerced_LandUse/Images/\"\n",
    "\n",
    "# Create an easy to read list of class names for use in plots and labels.\n",
    "class_names = ['Agricultural', 'Baseball Diamond', 'Buildings', 'Dense Residential',\n",
    "               'Harbor', 'Medium Residential', 'Mobile Home Park', 'Parking Lot',\n",
    "               'Runway', 'Sparse Residential', 'Storage Tanks', 'Tennis Court', \n",
    "               'Airplane', 'Beach', 'Chaparral', 'Forest', 'Freeway', 'Golf Course',\n",
    "               'Intersection', 'Overpass', 'River'\n",
    "              ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba064a8-0c28-45bf-9213-a98b1ebf2090",
   "metadata": {},
   "source": [
    "### Assemble Your Data Pipeline\n",
    "\n",
    "* Define your pipelines of transformations for training and validation data.\n",
    "    * Use the pre-calculated `mean` and `std` of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512c62c8-4ff3-4959-b476-4ff7471463a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pre-calculated mean and std of this dataset\n",
    "mean = [0.485, 0.490, 0.451]\n",
    "std = [0.214, 0.197, 0.191]\n",
    "\n",
    "# Transformations for the training set (with augmentation)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "\n",
    "# Transformations for validation set (no augmentation)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean,std=std),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027f1d56-9e29-4edc-8dc4-df2d2d524ff9",
   "metadata": {},
   "source": [
    "* Split the data into an 80% training set and a 20% validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32ae238-bbcf-4439-8422-d0572fb2c95b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the training and validation datasets by splitting the main dataset.\n",
    "train_dataset, val_dataset = helper_utils.create_datasets(\n",
    "    dataset_path, \n",
    "    train_transform,\n",
    "    val_transform,\n",
    "    train_split=0.8,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# Determine the number of unique classes from the dataset's properties.\n",
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "# Print a summary of the dataset split.\n",
    "print(f\"Total Number of Classes:  {num_classes}\")     \n",
    "print(f\"Training set size:        {len(train_dataset)}\")\n",
    "print(f\"Validation set size:      {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e83b04-47f1-491f-be7f-0adbd1abfd1f",
   "metadata": {},
   "source": [
    "* Create the dataloaders for your training and validation sets, defining a batch size of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58fd791",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the number of images to process in each batch.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the training and validation DataLoaders using the helper function.\n",
    "train_loader, val_loader = helper_utils.create_dataloaders(train_dataset, val_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63def05d-b378-4148-8984-d07438d53f35",
   "metadata": {},
   "source": [
    "### Visualize Training Samples\n",
    "\n",
    "* Run the next cell to visualize your training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcf0e24-6d7f-489f-84d3-c972d17bbb51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display the sample images from train set\n",
    "helper_utils.show_sample_images(train_dataset, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372ce1b3-9ab4-4456-8a1a-29e6600cc585",
   "metadata": {},
   "source": [
    "## Beyond Shortcuts: The DenseNet Philosophy\n",
    "\n",
    "In the last lab, you saw how an elegant architectural tweak, the skip connection, created an \"express lane\" for gradients, allowing you to build deeper networks. But this raises a new question: are we using the information flowing through the network as efficiently as possible?\n",
    "\n",
    "Each layer in a deep network learns new features, but the rich information from the earliest layers can become diluted as it passes through dozens of transformations. This is especially problematic for a task like yours, where you have **very few training images** for **many distinct classes**. In such a data-scarce environment, your model cannot afford to be redundant. It must squeeze every drop of value from the features it learns to avoid overfitting and generalize effectively.\n",
    "\n",
    "This is the challenge that the **DenseNet (Densely Connected Convolutional Network)** architecture was designed to solve. It takes the idea of shortcuts to its logical conclusion. Instead of just adding a previous layer's output, DenseNet asks a powerful question: What if every layer was directly connected to *every preceding layer*?\n",
    "\n",
    "This is achieved by concatenating the feature maps from all previous layers and passing them as input to the current layer. This creates a dense \"information highway\" where features are explicitly preserved and accumulated throughout the network. This design encourages massive **feature reuse**, which makes the model highly **parameter-efficient** and provides an even stronger solution to the vanishing gradient problem. For your specific challenge, this architecture is an ideal choice, as its data-efficient nature is perfectly suited to learning from a limited number of samples.\n",
    "\n",
    "**DenseNet** is constructed from three main components:\n",
    "\n",
    "* **DenseLayer**: A single convolutional layer that produces a small, consistent set of new feature maps (referred to as the `growth_rate`).\n",
    "\n",
    "* **DenseBlock**: The heart of the architecture; a stack of `DenseLayers` where each layer's output is concatenated with all previous inputs.\n",
    "\n",
    "* **TransitionLayer**: Connects `DenseBlocks` and handles downsampling to control the feature map size.\n",
    "\n",
    "You will now build this architecture from the ground up, examining each of these components in detail.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffbd015-61f3-4e66-b7d9-0a18b194c863",
   "metadata": {},
   "source": [
    "### `DenseLayer`: The Engine of Feature Reuse\n",
    "\n",
    "The `DenseLayer` is the fundamental building block of the DenseNet. Its design is focused on a single, powerful idea: instead of transforming and replacing information, each layer should add a small, new contribution to a collective bank of knowledge.\n",
    "\n",
    "To do this without becoming computationally expensive, the `DenseLayer` uses an efficient **bottleneck** strategy to generate a small number of new feature maps. The layer's defining characteristic is how it then integrates these new features with its input, ensuring that all prior information is preserved and carried forward. This process of accumulation is what forms the \"information highway.\"\n",
    "\n",
    "* **`__init__`**: \n",
    "> This constructor defines the two-part structure of the layer.\n",
    ">    * `dimension_reduction`: This `Sequential` block is the bottleneck. It contains a `BatchNorm`, `ReLU`, and a **1x1 Convolution**. Its sole purpose is to take the potentially large stack of input channels and squeeze them down into a much smaller, manageable number.\n",
    ">\n",
    ">    * `feature_extraction`: This second `Sequential` block performs the actual feature learning. It takes the reduced output from the bottleneck and uses a **3x3 Convolution** to generate a small, fixed number of new feature maps, defined by the `growth_rate`.\n",
    "\n",
    "* **`forward`**: \n",
    "> This method defines the data's path and executes the layer's signature logic: aggregating information.\n",
    ">    1.  **Generate New Features**: The input `x` is first passed through the `dimension_reduction` block and then the `feature_extraction` block. The result is a thin tensor containing only the brand-new features, which we call `new_features`.\n",
    ">\n",
    ">    2.  **Aggregate by Concatenation**: This is the pivotal moment. The `new_features` are merged with the original input `x` using `torch.cat`. This operation stacks the new feature maps onto the old ones along the channel dimension.\n",
    ">        * This step, appending new knowledge instead of transforming old knowledge, is the defining operation of DenseNet. The output of the layer is always thicker than the input, containing a complete record of past and present features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32071853",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DenseLayer(nn.Module):\n",
    "    \"\"\"A single dense layer module as described in the DenseNet architecture.\n",
    "\n",
    "    This layer implements the bottleneck design, where a 1x1 convolution reduces\n",
    "    the number of feature maps before a 3x3 convolution is applied. The output\n",
    "    feature maps are then concatenated with the input feature maps.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): The number of **input channels**.\n",
    "        growth_rate (int): The number of feature maps to produce (**k** in the paper).\n",
    "        bn_size (int): The multiplicative factor for the number of bottleneck channels.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, growth_rate=32, bn_size=4):\n",
    "        super(DenseLayer, self).__init__()\n",
    "\n",
    "        # Bottleneck layer: 1x1 convolution for dimensionality reduction.\n",
    "        self.dimension_reduction = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                in_channels, bn_size * growth_rate, kernel_size=1, stride=1, bias=False\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Feature extraction layer: 3x3 convolution to generate new features.\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.BatchNorm2d(bn_size * growth_rate),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                bn_size * growth_rate,\n",
    "                growth_rate,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Defines the forward pass of the dense layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after concatenating with the input.\n",
    "        \"\"\"\n",
    "        # Pass the input through the bottleneck and feature extraction layers.\n",
    "        new_features = self.dimension_reduction(x)\n",
    "        new_features = self.feature_extraction(new_features)\n",
    "        \n",
    "        # Concatenate the new feature maps with the original input feature maps.\n",
    "        concatenated_features = torch.cat((x, new_features), 1)\n",
    "\n",
    "        return concatenated_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab5df82-35f0-4805-ae76-cebfe51f34a0",
   "metadata": {},
   "source": [
    "#### Verifying the Blueprint: The `DenseLayer` Summary\n",
    "\n",
    "You can now visualize the `DenseLayer`'s structure to confirm its internal logic is working exactly as designed. The main goal is to see the signature concatenation operation in action by observing how the number of channels changes.\n",
    "\n",
    "* Instantiate the `DenseLayer` and define the input dimensions required to generate its blueprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b40156-a6cd-407e-81db-29edf4b02b51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an instance of the DenseLayer.\n",
    "denselayer = DenseLayer(\n",
    "    in_channels=3,      # Accepts an input with 3 channels (e.g., RGB).\n",
    "    growth_rate=12,     # Will produce 12 new feature maps.\n",
    "    bn_size=4           # The bottleneck layer will have (4 * 12) = 48 channels.\n",
    ")\n",
    "\n",
    "# Define the shape for a single image (Channels, Height, Width).\n",
    "img_shape = (3, 64, 64)\n",
    "\n",
    "# Define the full input shape for a batch of images.\n",
    "input_size =  (batch_size, *img_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec553b6a-9abc-41f4-8594-4f01118e5c9f",
   "metadata": {},
   "source": [
    "* Run the next cell to display the summary table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f22633",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a configuration dictionary to store parameters for the model summary.\n",
    "config = {\n",
    "    \"input_size\": input_size,\n",
    "    \"attr_names\": [\"input_size\", \"output_size\", \"num_params\"],\n",
    "    \"col_names_display\": [\"Input Shape \", \"Output Shape\", \"Param #\"],\n",
    "    \"depth\": 2\n",
    "}\n",
    "\n",
    "# Generate the model summary object using torchinfo with the specified configuration.\n",
    "summary = torchinfo.summary(\n",
    "    model=denselayer, \n",
    "    input_size=config[\"input_size\"], \n",
    "    col_names=config[\"attr_names\"], \n",
    "    depth=config[\"depth\"]\n",
    ")\n",
    "\n",
    "# Display the summary as a styled HTML table.\n",
    "print(\"--- Model Summary ---\\n\")\n",
    "helper_utils.display_torch_summary(summary, config[\"attr_names\"], config[\"col_names_display\"], config[\"depth\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a709fb33-c7e9-43df-be58-234fb72ac5a0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "This summary provides the proof that your `DenseLayer` is wired correctly. By tracing the `Input Shape` and `Output Shape` through the layer's internal components, you can confirm that the concatenation logic is performing exactly as intended.\n",
    "\n",
    "A key verification is to follow the number of channels:\n",
    "\n",
    "* The **bottleneck** (`dimension_reduction`) takes the initial 3 input channels and correctly expands them to **48** channels, which is the result of `bn_size * growth_rate` (4 * 12).\n",
    "\n",
    "* The `feature_extraction` block then processes these 48 channels to produce the **12** new feature maps defined by the `growth_rate`.\n",
    "\n",
    "Finally, the total **Output Shape** confirms the core logic: the original **3** channels have been successfully concatenated with the **12** new features to produce a final tensor with **15** channels. Your blueprint is working perfectly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6304c5c8-54a2-4ca0-badf-449c5cb8ae30",
   "metadata": {},
   "source": [
    "### `DenseBlock`: Accumulating Knowledge\n",
    "\n",
    "The `DenseBlock` is the heart of the DenseNet. It acts as a smart container that stacks multiple `DenseLayer`-s, wiring them together in the signature \"densely connected\" pattern. If a `DenseLayer` is a single sentence contributing a new piece of information, the `DenseBlock` is the paragraph where these sentences are sequentially combined to build a complex idea.\n",
    "\n",
    "Within the block, the output of one layer, containing all prior features plus its new ones, becomes the direct input for the next. This leads to a systematic growth in the network's collective knowledge, making the feature maps progressively \"thicker\" as they flow through the block.\n",
    "\n",
    "* **`__init__`**: \n",
    "> This constructor's primary role is to build the stack of `DenseLayers`. It loops for `num_layers` and creates a new `DenseLayer` in each iteration.\n",
    ">    * The key logic is in calculating the input channels for each new layer: `in_channels + i * growth_rate`. This formula is the mathematical heart of the block, ensuring that each `DenseLayer` is aware of the accumulated feature maps from all the layers that came before it.\n",
    "\n",
    "* **`forward`**: \n",
    "> The forward pass executes the information accumulation. Its logic is straightforward because the complex channel calculations were already handled in the constructor.\n",
    ">    *  **Initialize Features**: A tensor named `features` is initialized with the block's input, `x`.\n",
    ">\n",
    ">    *  **Sequentially Process**: The code iterates through the stack of layers. In each step, the current `features` tensor is passed into a `DenseLayer`, which returns an even thicker tensor (with `growth_rate` new channels). This thicker tensor becomes the new `features` for the next iteration.\n",
    ">\n",
    ">    *  **Return Final Output**: After the loop finishes, `features` holds the combined output of all layers in the block, which is then returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79754898",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Module):\n",
    "    \"\"\"A container for a sequence of DenseLayer modules.\n",
    "\n",
    "    This class groups multiple DenseLayer instances to form a single \"dense block\"\n",
    "    as described in the DenseNet architecture. Within the block, each layer\n",
    "    receives the feature maps from all preceding layers as its input.\n",
    "\n",
    "    Args:\n",
    "        num_layers (int): The number of **DenseLayer** modules in the block.\n",
    "        in_channels (int): The number of channels in the **input tensor**.\n",
    "        growth_rate (int): The number of new channels produced by each DenseLayer.\n",
    "        bn_size (int): The multiplicative factor for the bottleneck layer channels.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, in_channels, growth_rate=32, bn_size=4):\n",
    "        super(DenseBlock, self).__init__()\n",
    "\n",
    "        # Initialize a module list to hold all layers in the block.\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Sequentially add DenseLayer modules to the block.\n",
    "        for i in range(num_layers):\n",
    "            # The input channels for each new layer is the initial number of channels\n",
    "            # plus the accumulated growth from all previous layers.\n",
    "            layer = DenseLayer(\n",
    "                in_channels + i * growth_rate, growth_rate, bn_size\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Defines the forward pass for the DenseBlock.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor for the block.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after passing through all layers.\n",
    "        \"\"\"\n",
    "        # The 'features' tensor holds the concatenated outputs from all layers.\n",
    "        features = x\n",
    "        \n",
    "        # Pass the features through each dense layer in the block.\n",
    "        for layer in self.layers:\n",
    "            features = layer(features)\n",
    "            \n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36806e37-37d1-42ab-a6de-11b8c964e6ac",
   "metadata": {},
   "source": [
    "#### Tracing the Knowledge Accumulation\n",
    "\n",
    "You can now visualize exactly how the number of channels grows as data flows through the `DenseLayer` inside the `DenseBlock`. This will demonstrate the cumulative concatenation that is central to the architecture's design.\n",
    "\n",
    "* Instantiate the `DenseBlock` itself, configuring it to contain two consecutive `DenseLayer`-s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f9c6f6-275c-4fbe-9e80-aba040a5d58b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an instance of the DenseBlock.\n",
    "denseblock = DenseBlock(\n",
    "    in_channels=3,      # The block accepts an input with 3 channels.\n",
    "    growth_rate=12,     # Each DenseLayer within the block adds 12 channels.\n",
    "    bn_size=4,          # The bottleneck multiplier used in each DenseLayer.\n",
    "    num_layers=2,       # The block will contain 2 consecutive DenseLayers.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1f0d10-82a3-45c3-8516-816ad52de995",
   "metadata": {},
   "source": [
    "* Run the next cell to display the summary table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfd6a95-7a9c-45a4-95d7-29ca09ee6ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a configuration dictionary to store parameters for the model summary.\n",
    "config = {\n",
    "    \"input_size\": input_size, # (batch_size, *img_shape)\n",
    "    \"attr_names\": [\"input_size\", \"output_size\", \"num_params\"],\n",
    "    \"col_names_display\": [\"Input Shape \", \"Output Shape\", \"Param #\"],\n",
    "    \"depth\": 3\n",
    "}\n",
    "\n",
    "# Generate the model summary object using torchinfo with the specified configuration.\n",
    "summary = torchinfo.summary(\n",
    "    model=denseblock, \n",
    "    input_size=config[\"input_size\"], \n",
    "    col_names=config[\"attr_names\"], \n",
    "    depth=config[\"depth\"]\n",
    ")\n",
    "\n",
    "# Display the summary as a styled HTML table.\n",
    "print(\"--- Model Summary ---\\n\")\n",
    "helper_utils.display_torch_summary(summary, config[\"attr_names\"], config[\"col_names_display\"], config[\"depth\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea5f67c-655c-4038-b219-58775b647a72",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "This summary verifies that your `DenseBlock` is correctly stacking `DenseLayer`-s and accumulating features. By tracing the `Output Shape` of each internal layer, you can see the cumulative concatenation in action.\n",
    "\n",
    "The key is to follow the growth of the channel dimension:\n",
    "\n",
    "* The block takes an input with **3** channels. The first `DenseLayer` processes this and outputs a tensor with **15** channels (3 + 12).\n",
    "\n",
    "* This 15-channel tensor is then fed into the second `DenseLayer`, which adds another 12 features, resulting in the block's final output of **27** channels (15 + 12).\n",
    "\n",
    "This perfectly demonstrates the sequential growth and dense connectivity within the block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3691122-1664-4688-8a8a-0bac9ed3f193",
   "metadata": {},
   "source": [
    "### `TransitionLayer`: The Efficient Bridge\n",
    "\n",
    "After a `DenseBlock` has diligently accumulated knowledge and created a large, detailed feature map, the `TransitionLayer` steps in to act as a crucial bridge to the next block. If a `DenseBlock` is a dense chapter packed with information, the `TransitionLayer` is the concise summary at the chapter's end, preparing the reader for the next part of the story.\n",
    "\n",
    "Its purpose is twofold: to **compress** the information (reduce the number of channels) to keep the model efficient, and to **downsample** (reduce the height and width), which helps the network learn more abstract, high-level patterns.\n",
    "\n",
    "**`__init__`**: \n",
    "> The constructor builds a compact and efficient pipeline to perform both compression and downsampling in one go. It bundles these operations into a single `nn.Sequential` module.\n",
    ">    * The first key piece of logic is calculating the number of output channels: `out_channels = int(in_channels * compression_factor)`. This is the **compression** step, effectively summarizing the vast number of features from the preceding block into a more manageable set.\n",
    ">    * It then defines the `self.transition` pipeline, which first **stabilizes the incoming features** with `BatchNorm2d` and `ReLU`, and then performs its core tasks:\n",
    ">\n",
    ">        * A `1x1 Conv2d` layer to perform the actual channel reduction.\n",
    ">\n",
    ">        * An `AvgPool2d` layer with a stride of 2 to cut the feature map's height and width in half, achieving the **downsampling**.\n",
    "\n",
    "**`forward`**: \n",
    "> The forward pass is incredibly straightforward, as all the complex operations were already packaged neatly into the `self.transition` module during initialization.\n",
    ">    *  **Direct Execution**: It takes the large tensor output from a `DenseBlock` and simply passes it through the pre-built `transition` pipeline.\n",
    ">\n",
    ">    *  **Return Final Output**: The layer returns a new tensor that is both \"thinner\" (fewer channels) and smaller in spatial dimensions, perfectly prepared to be the input for the next `DenseBlock`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7e6012-81c8-4e27-a498-c5addee225b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransitionLayer(nn.Module):\n",
    "    \"\"\"A transition layer used between two dense blocks in a DenseNet.\n",
    "\n",
    "    This layer is responsible for reducing the number of channels (compression)\n",
    "    via a 1x1 convolution and downsampling the spatial dimensions of the\n",
    "    feature maps using average pooling.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): The number of channels in the **input tensor**.\n",
    "        compression_factor (float): A factor between 0 and 1 to reduce the\n",
    "                                    number of feature maps.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, compression_factor=0.5):\n",
    "        super(TransitionLayer, self).__init__()\n",
    "\n",
    "        # Determine the number of output channels after applying compression.\n",
    "        out_channels = int(in_channels * compression_factor)\n",
    "\n",
    "        # The layer consists of batch normalization, a 1x1 convolution for channel\n",
    "        # reduction, and average pooling for spatial downsampling.\n",
    "        self.transition = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Defines the forward pass for the transition layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The downsampled and compressed output tensor.\n",
    "        \"\"\"\n",
    "        return self.transition(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5297b0bf-7b1f-43ca-af6a-d52111c8f201",
   "metadata": {},
   "source": [
    "### `DenseNet`: Assembling the Full Architecture\n",
    "\n",
    "The `DenseNet` class is the master architect that assembles all the building blocks you have seen, `DenseLayer`, `DenseBlock`, and `TransitionLayer`, into a complete, powerful network. This class is responsible for creating the network's stem, building the main body by correctly stacking and connecting the blocks, and adding the final classification head.\n",
    "\n",
    "* **`__init__`**: \n",
    "> This method acts as the master blueprint for the entire network. It doesn't contain the layer logic itself but calls helper methods to construct the three main parts of the model: the initial feature extractor (`self.features`), the main body of dense blocks and transitions (`self.dense_blocks`), and the final `self.classifier`.\n",
    "\n",
    "* **`_get_initial_features`**: \n",
    "> This helper creates the network's \"stem.\" Its job is to perform an initial, aggressive downsampling of the input image. It uses a standard combination of a large-kernel convolution and max pooling to quickly reduce the spatial dimensions and create the first set of feature maps that will be fed into the main body of the network.\n",
    "\n",
    "* **`_get_dense_blocks`**: \n",
    "> This is the most critical helper method, responsible for building the network's \"body.\" It programmatically constructs the repeating pattern of `DenseBlock` -> `TransitionLayer`.\n",
    ">    * It iterates through the `block_config` tuple, which defines how many `DenseLayer`-s are in each `DenseBlock`.\n",
    ">    * A key responsibility of this method is to **track the number of feature maps**. It calculates the channel growth after each `DenseBlock` and the channel compression after each `TransitionLayer`, feeding the correct `in_channels` to the next component.\n",
    ">\n",
    ">    * After building the main blocks, it appends the final layers (like `BatchNorm2d` and `AdaptiveAvgPool2d`) that form the \"head\" of the network, preparing the data for the final classification layer.\n",
    "\n",
    "* **`forward`**: \n",
    "> This method defines the end-to-end journey of the data. The flow is very clean and sequential:\n",
    ">    *  The input image first passes through the initial `self.features` block.\n",
    ">\n",
    ">    *  The resulting feature maps are then passed sequentially through the entire list of `self.dense_blocks` (which contains the alternating `DenseBlock`-s and `TransitionLayer`-s).\n",
    ">\n",
    ">    *  Finally, the output is flattened and passed to the `self.classifier` to produce the final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db182fb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the DenseNet architecture.\n",
    "\n",
    "    Args:\n",
    "        growth_rate (int): The number of feature maps each layer adds.\n",
    "        block_config (tuple of ints): The number of layers in each dense block.\n",
    "        num_init_features (int): The number of filters in the initial convolutional layer.\n",
    "        bn_size (int): The multiplicative factor for the number of bottleneck channels.\n",
    "        compression_factor (float): The factor by which to reduce the number of channels \n",
    "                                    in transition layers.\n",
    "        num_classes (int): The number of output classes for the final classifier.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        growth_rate=32,\n",
    "        block_config=(6, 12, 24, 16),\n",
    "        num_init_features=64,\n",
    "        bn_size=4,\n",
    "        compression_factor=0.5,\n",
    "        num_classes=1000,\n",
    "    ):\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        # Define the initial feature extraction block.\n",
    "        self.features = self._get_initial_features(num_init_features)\n",
    "\n",
    "        # Define the main body of the network with the corrected call.\n",
    "        self.dense_blocks = self._get_dense_blocks(\n",
    "            num_init_features,\n",
    "            block_config,\n",
    "            growth_rate,\n",
    "            bn_size,\n",
    "            compression_factor,\n",
    "        )\n",
    "\n",
    "        # Define the final fully connected classification layer.\n",
    "        self.classifier = nn.Linear(self.num_features, num_classes)\n",
    "\n",
    "    def _get_initial_features(self, num_init_features):\n",
    "        \"\"\"\n",
    "        Creates the initial convolutional block for feature extraction and downsampling.\n",
    "        \n",
    "        Args:\n",
    "            num_init_features (int): The number of output channels for the first convolution.\n",
    "            \n",
    "        Returns:\n",
    "            nn.Sequential: A sequential container for the initial layers.\n",
    "        \"\"\"\n",
    "        # Create a sequential module for the initial layers.\n",
    "        convolution_block = nn.Sequential(\n",
    "            # 7x7 convolution with stride 2 for initial downsampling.\n",
    "            nn.Conv2d(\n",
    "                3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False\n",
    "            ),\n",
    "            # Batch normalization.\n",
    "            nn.BatchNorm2d(num_init_features),\n",
    "            # ReLU activation.\n",
    "            nn.ReLU(inplace=True),\n",
    "            # 3x3 max pooling with stride 2 for further downsampling.\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "        return convolution_block\n",
    "\n",
    "\n",
    "    def _get_dense_blocks(self, num_init_features, block_config, growth_rate, bn_size, compression_factor):\n",
    "        \"\"\"\n",
    "        Constructs the sequence of dense blocks and transition layers.\n",
    "        \n",
    "        Args:\n",
    "            num_init_features (int): The initial number of features.\n",
    "            block_config (tuple of ints): The number of layers for each dense block.\n",
    "            growth_rate (int): The growth rate for dense blocks.\n",
    "            bn_size (int): The bottleneck size factor.\n",
    "            compression_factor (float): The compression factor for transition layers.\n",
    "            \n",
    "        Returns:\n",
    "            nn.ModuleList: A list containing the dense blocks, transition layers, \n",
    "                           and final classification head components.\n",
    "        \"\"\"\n",
    "        # Create a ModuleList to store all network blocks.\n",
    "        dense_blocks = nn.ModuleList()\n",
    "        \n",
    "        # Initialize the number of features.\n",
    "        num_features = num_init_features\n",
    "\n",
    "        # Iterate through the block configurations to build the network.\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            # Create a new DenseBlock.\n",
    "            db = DenseBlock(\n",
    "                num_layers=num_layers,\n",
    "                in_channels=num_features,\n",
    "                growth_rate=growth_rate,\n",
    "                bn_size=bn_size,\n",
    "            )\n",
    "            # Add the DenseBlock to the list.\n",
    "            dense_blocks.append(db)\n",
    "\n",
    "            # Update the number of features based on the growth rate.\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "\n",
    "            # Add a transition layer after each dense block, except for the last one.\n",
    "            if i != len(block_config) - 1:\n",
    "                transition = TransitionLayer(\n",
    "                    in_channels=num_features, compression_factor=compression_factor\n",
    "                )\n",
    "                # Add the transition layer to the list.\n",
    "                dense_blocks.append(transition)\n",
    "                # Update the number of features after compression.\n",
    "                num_features = int(num_features * compression_factor)\n",
    "\n",
    "        # Add the final batch normalization layer.\n",
    "        dense_blocks.append(nn.BatchNorm2d(num_features))\n",
    "        # Add the final ReLU activation.\n",
    "        dense_blocks.append(nn.ReLU(inplace=True))\n",
    "        # Add global average pooling to create a fixed-size feature vector.\n",
    "        dense_blocks.append(nn.AdaptiveAvgPool2d((1, 1)))\n",
    "\n",
    "        # Store the final number of features for the classifier layer.\n",
    "        self.num_features = num_features\n",
    "        return dense_blocks\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the DenseNet model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: The output logits from the classifier.\n",
    "        \"\"\"\n",
    "        # Pass the input through the initial feature extractor.\n",
    "        x = self.features(x)\n",
    "\n",
    "        # Pass the features through each block in the main body.\n",
    "        for block in self.dense_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Flatten the output tensor for the fully connected layer.\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # Pass the flattened features through the classifier.\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c19613-cf3b-4a55-bc7e-f5bf8cc09ea1",
   "metadata": {},
   "source": [
    "#### Assembling the Final Blueprint\n",
    "\n",
    "With all the individual components built and verified, it's time to assemble the full `DenseNet` model. Generating a summary of the complete architecture allows you to perform a final check, ensuring that the `DenseBlock`-s and `TransitionLayer`-s are connected correctly and that the data dimensions change as expected from the input image to the final feature vector.\n",
    "\n",
    "* First, create an instance of the complete `DenseNet` model using its default configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425e6d77-3300-44f8-ab5c-4a0ceeba53e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the DenseNet model with its default configuration.\n",
    "densenet = DenseNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4e5cd0-e17c-4eb1-a42d-89c4f3bf7b4c",
   "metadata": {},
   "source": [
    "* Now, run the next cell to generate and display the summary of the full architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00716bb6-2459-4246-946f-b17fd86a0bc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a configuration dictionary to store parameters for the model summary.\n",
    "config = {\n",
    "    \"input_size\": input_size, # (batch_size, *img_shape)\n",
    "    \"attr_names\": [\"input_size\", \"output_size\", \"num_params\"],\n",
    "    \"col_names_display\": [\"Input Shape \", \"Output Shape\", \"Param #\"],\n",
    "    \"depth\": 2\n",
    "}\n",
    "\n",
    "# Generate the model summary object using torchinfo with the specified configuration.\n",
    "summary = torchinfo.summary(\n",
    "    model=densenet, \n",
    "    input_size=config[\"input_size\"], \n",
    "    col_names=config[\"attr_names\"], \n",
    "    depth=config[\"depth\"]\n",
    ")\n",
    "\n",
    "# Display the summary as a styled HTML table.\n",
    "print(\"--- Model Summary ---\\n\")\n",
    "helper_utils.display_torch_summary(summary, config[\"attr_names\"], config[\"col_names_display\"], config[\"depth\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24781ce6-33e0-4add-aa20-e76bd50bf6db",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "This summary is the final verification of your complete architecture, showing how all the individual components work together. By tracing the data flow from the initial image to the final prediction, you can confirm that the entire model is assembled correctly.\n",
    "\n",
    "Here’s what to look for in the summary table:\n",
    "\n",
    "* **The \"Stem\" (`features`)**: Observe how the initial block takes the `[3, 64, 64]` input image and performs aggressive downsampling, producing a `[64, 16, 16]` feature map. This is the starting point for the main body of the network.\n",
    "\n",
    "* **The `DenseBlock` Pattern: Look at the output of each `DenseBlock`. You'll see a massive increase in the number of channels (e.g., from **64** to **256** in the first block). This is the \"knowledge accumulation\" at work.\n",
    "\n",
    "* **The `TransitionLayer` Pattern**: After each `DenseBlock`, notice how the `TransitionLayer` performs its two jobs: it compresses the number of channels (e.g., from **256** to **128**) and halves the spatial dimensions (e.g., from `16x16` to `8x8`).\n",
    "\n",
    "* **The \"Head\" and `classifier`**: Finally, see how the last few layers (`AdaptiveAvgPool2d`) transform the final feature map into a feature vector of size `[1024, 1, 1]`. This is then flattened and passed to the `classifier` to produce the final `[1000]` output logits.\n",
    "\n",
    "This confirms that your blueprint has been successfully assembled, with each component performing its specific role in the data's journey through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45caadf3-c589-42a7-92ef-c8c25fd53fc3",
   "metadata": {},
   "source": [
    "## Initialize the Training Configurations\n",
    "\n",
    "You've meticulously built and verified the entire `DenseNet` architecture, from its smallest component to the fully assembled model. With your powerful, data-efficient engine ready, the next step is to prepare it for its mission: learning to classify the land use dataset.\n",
    "\n",
    "This involves setting up the three core components for any training loop: the model itself, a loss function to measure error, and an optimizer to update the model's weights.\n",
    "\n",
    "* Instantiate the `DenseNet` model.\n",
    "    * `num_classes`: Sets the model's final output layer to match the 21 classes in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cbaa93-a857-4345-86f6-740bafac6838",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the random seed to ensure that model is always initialized with the same random weights.\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Create an instance of your custom DenseNet, configuring the final layer for your specific number of classes.\n",
    "densenet_model = DenseNet(num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4746ad4f-8359-48ad-bdfc-3ed0f3b0de7b",
   "metadata": {},
   "source": [
    "* Define the Loss Function as `nn.CrossEntropyLoss`.\n",
    "* Define `Adam` with a learning rate of `0.001` as the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22668ad1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use CrossEntropyLoss as loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Use Adam with lr=0.001\n",
    "optimizer = optim.Adam(densenet_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096b1165",
   "metadata": {},
   "source": [
    "## Training and Validation\n",
    "\n",
    "You are now ready to put your custom-built architect to the test. You will train the model using the provided `training_loop_16_mixed` function, which handles the complete training and validation cycle using an efficient **16-bit mixed-precision** strategy.\n",
    "\n",
    "The function will track performance and return the three key results from the training run:\n",
    "\n",
    "* `model`: The trained model itself, loaded with the weights from the epoch that achieved the **highest validation accuracy**. This ensures you are working with the best version of your model.\n",
    "\n",
    "* `history`: A dictionary that records the performance metrics (`train_loss`, `val_loss`, `val_accuracy`) for each epoch.\n",
    "\n",
    "* `best_cm`: The confusion matrix calculated on the validation data from the **best-performing epoch**. This provides a detailed snapshot of the model's performance at its peak."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842cc63e-e5f7-4de4-a7fd-ab92ecd49f7c",
   "metadata": {},
   "source": [
    "* Feel free to set a different value for `num_epochs`. Since **DenseNet** is a deep architecture and you are training on a dataset with limited images per class, it may take more epochs to converge. A range of **20-25** should be sufficient to see good results. The value is currently set to **20**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd54f0af-6a43-411f-9c7f-136499d1bf33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the total number of full training cycles (epochs) to run.\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f86f79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Launch the training process for the custom-built DenseNet model.\n",
    "trained_densenet, history, confusion_matrix = helper_utils.training_loop_16_mixed(\n",
    "    model=densenet_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    loss_function=loss_function,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    "    save_path='./saved_models/best_trained_densenet.pth',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a47a01e-1b26-43d6-a6eb-d6b07940b6a9",
   "metadata": {},
   "source": [
    "### Visualizing Performance\n",
    "\n",
    "To better understand the model's learning journey toward its peak performance, you can now plot the metrics from the `history` object. The learning curves will visually confirm the analysis, showing the steady decrease in loss and the corresponding rise in accuracy across the training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0020e17b-eeff-4579-ae64-6b11dc38ba3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the learning curves to visualize the model's performance and highlight the best epoch.\n",
    "helper_utils.plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0fed7b-9e4f-4985-9e77-e4912c99fbf8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**The Architect's Success: Analyzing the Performance**\n",
    "\n",
    "The training results are in, and they are a clear testament to the power of the **DenseNet** architecture you built. Achieving a decent validation accuracy (>=**75%**) on a challenging 21-class dataset, with only **100** images per class and training from scratch, is a remarkable achievement.\n",
    "\n",
    "Let's break down what these plots tell you:\n",
    "\n",
    "* **Training and Validation Loss**: The plot on the left shows a healthy learning process. The **training loss** (blue) consistently decreases as the model learns from the data. Crucially, the **validation loss** (red) also follows a strong downward trend, which indicates that the model is generalizing well to unseen images and is not simply memorizing the training set.\n",
    "\n",
    "* **Validation Accuracy**: The plot on the right tells an even clearer story of success. The steady and significant climb in accuracy demonstrates that your model progressively learned the subtle visual cues needed to distinguish between difficult classes like `harbor`, `mediumresidential`, and `tenniscourt`.\n",
    "\n",
    "**Why Did This Work So Well?**\n",
    "\n",
    "This strong performance is not a coincidence; it is a direct result of the intelligent design of **DenseNet** that you implemented:\n",
    "\n",
    "* **Massive Feature Reuse**: On a data-scarce problem like this, a traditional network would struggle with overfitting. Your **DenseNet**, however, excelled because its architecture is inherently data-efficient. By concatenating feature maps from all preceding layers, the model is forced to reuse and build upon existing knowledge rather than re-learning features from scratch. It squeezes the maximum value out of every training sample.\n",
    "\n",
    "* **Strong Gradient Flow**: The dense \"information highway\" ensures that the learning signal (gradient) can flow directly and deeply through the network. This prevents the learning process from stalling and allows the entire model to train effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db58c70-e9c6-4f92-b2a2-9169af4a5c69",
   "metadata": {},
   "source": [
    "## Visualizing Predictions and Confusion Matrix\n",
    "\n",
    "The learning curves and peak accuracy score give you a strong quantitative measure of your model's success. However, to get a true qualitative feel for its capabilities, it is invaluable to see it in action.\n",
    "\n",
    "Visualizing predictions on individual images shows you not just *that* the model is working, but *how* it is performing on different classes. This is where you can see what your model gets right and, more importantly, where it might be making mistakes.\n",
    "\n",
    "* First, you will visualize predictions from your `trained_densenet` to see how it performs on the various land use types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea6bede-518a-466b-9a97-79e8f45b68a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the performance of the trained DenseNet by plotting its predictions on a sample of images from the validation set.\n",
    "helper_utils.visualize_predictions(trained_densenet, val_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc2ae95-4907-436c-a554-656cbda7c41a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The overall accuracy score confirms that your custom **DenseNet** successfully tackled the challenge of learning from a scarce dataset. Now you can dig deeper to see how its data-efficient design translated into specific successes.\n",
    "\n",
    "A **confusion matrix** is the perfect tool for this analysis. It will show you precisely which of the 21 classes the model mastered and which ones it found more challenging. This is where you can see if the model's feature reuse was effective enough to learn the subtle differences between visually similar categories, like different housing densities or types of infrastructure.\n",
    "\n",
    "* Plot the confusion matrix from the model's best epoch to get a detailed report card of its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedd1a93-e9eb-4305-a291-96e8b2fb8609",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the confusion matrix to visualize the model's class-by-class performance.\n",
    "helper_utils.plot_confusion_matrix(confusion_matrix, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d15b23-3c67-4fa3-b0fa-84f5533cebb8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "This detailed breakdown is a clear success, confirming that your custom-built **DenseNet** learned effectively despite the challenging, data-scarce nature of the dataset.\n",
    "\n",
    "The strong diagonal line in the confusion matrix shows that the model correctly classifies most images. It performs exceptionally well on classes with distinct structures, like `Baseball Diamond`, `Harbor`, and `Mobile Home Park`. This is a testament to the power of **DenseNet's** feature reuse; even with few examples, the model learned a rich set of features to identify these unique patterns.\n",
    "\n",
    "The matrix is also a valuable diagnostic tool, highlighting the subtle challenges of the task. As you might expect, the model occasionally confuses classes that are visually similar from an aerial view. For instance, it can be difficult to distinguish between different residential densities or to separate large green areas like a `Golf Course` from `Agricultural` land. This is a common challenge in geospatial analysis and provides a great insight into where the model could be improved with more data or further training.\n",
    "\n",
    "This entire process, from designing the architecture to training and analyzing it, gives you a deep and thorough understanding of how **DenseNet** works. You've proven that its data-efficient design is highly effective.\n",
    "\n",
    "However, in many real-world projects, you don't need to build and train these complex architectures from scratch. Just as you've seen with other famous architectures, the Torchvision library provides a `densenet` model that has already been trained on millions of images. Let's explore how you can leverage this powerful, pre-trained tool to achieve even better results with a fraction of the effort."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de866f70-c02b-4103-8fbd-ba7e142e19c2",
   "metadata": {},
   "source": [
    "## The Professional's Shortcut: Feature Extraction\n",
    "\n",
    "As you saw from training your custom model, achieving high accuracy from scratch is a slow and computationally intensive process. After **20 epochs**, your model reached an impressive validation accuracy of around **78%**. While this is a great result, it required significant training time.\n",
    "\n",
    "Fortunately, instead of starting from zero, you can benefit from **feature extraction** by using the [densenet121](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.densenet121.html) model directly from Torchvision models library, which has been trained on a massive dataset (like ImageNet). Since this model is already an expert at recognizing a vast library of visual features, you can **freeze** its pre-trained layers and simply replace the final **classifier head** with a new one suited for your 21 classes. By training only this small, new layer, you can achieve excellent results in just a few epochs.\n",
    "\n",
    "* Run the next cell to load the pretrained **densenet121** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5159501-de44-4739-88dc-5cdc9a4643cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained DenseNet-121 model and configure it for feature extraction.\n",
    "pretrained_densenet = helper_utils.load_pretrained_densenet(\n",
    "    num_classes=num_classes,\n",
    "    weights_path=\"./pretrained_densenet_weights/densenet121-a639ec97.pth\",\n",
    "    train_classifier_only=True,\n",
    "    seed=SEED                  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bacb81-23b8-42c7-a892-759b5840dfa7",
   "metadata": {},
   "source": [
    "* Define a new optimizer for the pretrained `densenet121`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0bad9d-2a67-417a-80dc-f9375dbea11c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pass only the parameters of the new, trainable classifier head to the optimizer.\n",
    "optimizer_pretrained = optim.Adam(\n",
    "    (p for p in pretrained_densenet.parameters() if p.requires_grad), \n",
    "    lr=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cac3bbd-93eb-4763-bff3-5b9359381e82",
   "metadata": {},
   "source": [
    "* Feel free to set a different value for `num_epochs`. Since you are only training a small classifier head on top of a powerful pre-trained base, the model learns extremely quickly. You will find that **5** epochs are more than enough to achieve a very high validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceecc735-bc7f-4a14-ae57-565538165af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of training epochs. A small number is sufficient for feature extraction.\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf996d7-26f6-4cf7-95c6-620d76fcb675",
   "metadata": {},
   "source": [
    "* Run the next cell to begin feature extraction by training the new classifier head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5566c12f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Launch the feature extraction process for the pre-trained DenseNet.\n",
    "feature_extracted_densenet, history, cm_feature_densenet = helper_utils.training_loop_16_mixed(\n",
    "    model=pretrained_densenet,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    loss_function=loss_function,\n",
    "    optimizer=optimizer_pretrained,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    "    save_path='./saved_models/best_pretrained_densenet.pth',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a29d73c-cba9-45d4-b5fd-2829e483ac79",
   "metadata": {},
   "source": [
    "### Visualizing Predictions and Confusion Matrix\n",
    "\n",
    "* Run the next cell to plot predictions made by the `feature_extracted_densenet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec80ec2-a9a3-45af-add3-efb36ac231cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize predictions from the feature-extracted model on a sample of validation images.\n",
    "helper_utils.visualize_predictions(feature_extracted_densenet, val_loader, class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a539f09b-c960-49f6-8a37-c2fca221eeaa",
   "metadata": {},
   "source": [
    "* Run the next cell to plot the confusion matrix for the `feature_extracted_densenet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18096c50-61e9-4610-90bf-9d7481ab9a90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the confusion matrix to visualize the model's class-by-class performance.\n",
    "helper_utils.plot_confusion_matrix(cm_feature_densenet, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9108ed7-91d1-452e-bec8-524ed0d4276f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The results from feature extraction are a dramatic showcase of the power and efficiency of transfer learning. The confusion matrix reveals an exceptionally strong diagonal line, and the per-class accuracies are outstanding across the board.\n",
    "\n",
    "This is the key takeaway: in just **5 epochs**, by training only a tiny fraction of the network's parameters, you have significantly surpassed the performance of your custom-built model that trained for 20 epochs. The model achieved near-perfect accuracy on many classes and showed marked improvement on visually similar categories that were previously challenging.\n",
    "\n",
    "This remarkable performance is possible because the model didn't start from scratch. It leveraged a vast library of features learned from the massive ImageNet dataset, and you simply guided its expert knowledge toward your specific land use problem. This is the essence of a modern, efficient deep learning workflow: standing on the shoulders of giants to achieve state-of-the-art results with a fraction of the effort."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d68aebb-834c-41cf-b031-77cb26229d12",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You began this journey by understanding the need to move beyond simple, linear models and take direct control over your network's data flow. This lab put that theory into practice. You explored **DenseNet**'s powerful approach to feature reuse, which, instead of adding connections like ResNet, concatenates feature maps from all preceding layers to create a dense \"information highway\".\n",
    "\n",
    "Building the model from scratch gave you a firsthand look at advanced architectural patterns. You used modular design to create reusable components and saw how `nn.ModuleList` enables dynamic model creation, allowing you to programmatically construct `DenseBlock`-s with a variable number of layers. The strong performance of your custom model on a data-scarce problem was a clear testament to the architecture's inherent data efficiency.\n",
    "\n",
    "Finally, you transitioned from architect to practitioner by applying the professional's shortcut: **transfer learning**. By freezing the pre-trained feature extractor and training only a new classifier head, you achieved superior results in a fraction of the time. This demonstrated that a deep understanding of an architecture's design is most powerful when combined with a strategic approach to leveraging existing knowledge. You've successfully designed, built, and applied a modern, state-of-the-art architecture to solve a complex problem efficiently."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
