{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9168e7c9",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Programming Assignment: Classification and Visual Search\n",
    "\n",
    "Congratulations on reaching the assignment of this module! You've journeyed through the concepts of custom architectures, exploring how designs like ResNet use skip connections for deeper training, DenseNet maximizes feature reuse through concatenation, and Siamese Networks learn powerful similarity functions. You've also seen the fundamental importance of **modular design**, building reusable `nn.Module` components, to manage complexity and enable flexibility.\n",
    "\n",
    "Now, it's time to put all these principles into practice in a real-world scenario. Step into the role of an AI engineer at a leading online fashion retailer. Your task is to build the core AI engine for an intelligent product catalog system. This involves creating two distinct but related capabilities: accurately **classifying** clothing items and powering a **visual search** engine for recommendations.\n",
    "\n",
    "This assignment challenges you to think like a professional engineer, focusing on **efficiency** and **reusability**. You won't just build models; you'll architect them using custom, modular blocks inspired by modern efficient networks. You'll see how a well-designed feature extractor can be leveraged for multiple downstream tasks, a common and powerful pattern in applied deep learning.\n",
    "\n",
    "Throughout this programming assignment, you will:\n",
    "\n",
    "* Implement the `InvertedResidualBlock`, the efficient core component inspired by MobileNetV2, practicing custom module creation.\n",
    "* Construct a `MobileNetBackbone` by stacking your custom blocks, demonstrating modular network assembly.\n",
    "* Build and train a `MobileNetLikeClassifier` using the backbone for multi-class fashion item categorization, applying techniques like weighted loss for imbalance.\n",
    "* Design a `TripleDataset` to generate anchor, positive, and negative examples, setting the stage for similarity learning.\n",
    "* Engineer a `SiameseEncoder` by **reusing** the trained backbone from your classifier, showcasing architectural flexibility.\n",
    "* Assemble and train a `SiameseNetwork` using `TripletMarginLoss` to create a meaningful embedding space for visual similarity.\n",
    "* Apply your trained Siamese network to perform visual search, retrieving similar fashion items from a catalog based on image embeddings.\n",
    "\n",
    "Let's start building this intelligent fashion catalog engine!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915bc80c-46b4-4122-b320-5cde8036eede",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='submission'></a>\n",
    "\n",
    "<h4 style=\"color:green; font-weight:bold;\">TIPS FOR SUCCESSFUL GRADING OF YOUR ASSIGNMENT:</h4>\n",
    "\n",
    "* All cells are frozen except for the ones where you need to submit your solutions or when explicitly mentioned you can interact with it.\n",
    "\n",
    "* In each exercise cell, look for comments `### START CODE HERE ###` and `### END CODE HERE ###`. These show you where to write the solution code. **Do not add or change any code that is outside these comments**.\n",
    "\n",
    "* You can add new cells to experiment but these will be omitted by the grader, so don't rely on newly created cells to host your solution code, use the provided places for this.\n",
    "\n",
    "* Avoid using global variables unless you absolutely have to. The grader tests your code in an isolated environment without running all cells from the top. As a result, global variables may be unavailable when scoring your submission. Global variables that are meant to be used will be defined in UPPERCASE.\n",
    "\n",
    "* To submit your notebook for grading, first save it by clicking the ðŸ’¾ icon on the top left of the page and then click on the `Submit assignment` button on the top right of the page.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c5e737-85e0-491e-a10d-e9395f56eef8",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Imports](#0)\n",
    "- [1 - Building a Fashion Item Classifier](#1)\n",
    "    - [1.1 - The Fashion Dataset: A Curated View](#1-1)\n",
    "    - [1.2 - Preparing the Data Pipeline](#1-2)\n",
    "    - [1.3 - Architecting the Classifier: Efficiency with Inverted Residuals](#1-3)\n",
    "        - **[Exercise 1 - InvertedResidualBlock](#ex-1)**  \n",
    "        - **[Exercise 2 - MobileNetBackbone](#ex-2)**\n",
    "    - [1.4 - Assembling the Full Classifier](#1-4)\n",
    "    - [1.5 - Training the Classifier](#1-5)  \n",
    "    - [1.6 - Evaluating the Classifier](#1-6)\n",
    "- [2 - Building a Visual Search Engine](#2)\n",
    "    - [2.1 - Powering Recommendations with Similarity Learning](#2-1)\n",
    "    - [2.2 - Teaching Similarity: The Triplet Dataset](#2-2)\n",
    "        - **[Exercise 3 - TripleDataset](#ex-3)**\n",
    "    - [2.3 - Architecting the Visual Search Model](#2-3)\n",
    "        - [2.3.1 - The Siamese Encoder: Reusing the Backbone](#2-3-1)\n",
    "        - [2.3.2 - The Siamese Network Wrapper](#2-3-2)\n",
    "    - [2.4 - Training the Siamese Network](#2-4)\n",
    "    - [2.5 - Performing Visual Search (Retrieval)](#2-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838a9e7c",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a name='0'></a>\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423568f9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "import unittests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb89ece",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.utils as vutils\n",
    "from IPython.display import display\n",
    "from torchvision import transforms\n",
    "import torchinfo\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6a7e6e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import helper_utils\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dffa5b2",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Building a Fashion Item Classifier\n",
    "\n",
    "Your first task as an AI engineer for the online fashion retailer is to build a model that can accurately categorize clothing items. This is a fundamental step for organizing the product catalog and enabling features like filtered search. You'll start by constructing a model inspired by the efficient **MobileNet architecture**, known for its performance on devices with limited computational resources, a valuable trait for potential deployment in mobile apps or embedded systems.\n",
    "\n",
    "<a name='1-1'></a>\n",
    "### 1.1 - The Fashion Dataset: A Curated View\n",
    "\n",
    "You'll be working with image data derived from the **clothing-dataset-small** collection, originally found [here](https://github.com/alexeygrigorev/clothing-dataset-small). For this assignment, a specific subset has been curated and organized into training and validation sets, focusing on the following distinct clothing categories: `dress`, `hat`, `longsleeve`, `pants`, `shoes`, `shorts`, and `t-shirt`.\n",
    "\n",
    "This curated dataset presents a realistic scenario where you need to build a robust model from a moderately sized collection of images.\n",
    "\n",
    "<a name='1-2'></a>\n",
    "### 1.2 - Preparing the Data Pipeline\n",
    "\n",
    "Before you can train your classifier, you need a solid data pipeline. This involves defining how images are transformed (resized, augmented, normalized) and how they are loaded in batches. You'll set up separate transformation pipelines for training (including augmentation to help the model generalize) and validation (only essential preprocessing).\n",
    "\n",
    "* First, define the image transformation pipelines for your training and validation datasets. The training pipeline includes augmentations like random flips and rotations to make your model more robust, while the validation pipeline only applies the necessary resizing and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac346f2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Compose the transformations for training: resize, augment, then preprocess\n",
    "train_transform = transforms.Compose([\n",
    "    # Resize images to a consistent square size (64x64 pixels)\n",
    "    transforms.Resize((64, 64)),\n",
    "    # Apply random horizontal flipping for data augmentation\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # Apply random rotation (up to 10 degrees) for data augmentation\n",
    "    transforms.RandomRotation(10),\n",
    "    # Convert PIL images to PyTorch tensors\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize tensor values to range [-1, 1] (using mean=0.5, std=0.5)\n",
    "    transforms.Normalize(mean=[0.5] * 3, std=[0.5] * 3),\n",
    "])\n",
    "\n",
    "\n",
    "# For validation: only resize and preprocess (no augmentation)\n",
    "val_transform = transforms.Compose([\n",
    "    # Resize images to the same consistent square size (64x64 pixels)\n",
    "    transforms.Resize((64, 64)),\n",
    "    # Convert PIL images to PyTorch tensors\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize tensor values to range [-1, 1] (using mean=0.5, std=0.5)\n",
    "    transforms.Normalize(mean=[0.5] * 3, std=[0.5] * 3),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8fbd97-5cf9-4137-8443-ae2c7c02b921",
   "metadata": {},
   "source": [
    "* Specify the location of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0a50e7-f48b-473e-bd6d-a1adcd00acdb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the path to the root directory containing the dataset\n",
    "dataset_path = \"./clothing-dataset-small\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b14abbb-3d42-4c9a-9834-6098b1dfe52e",
   "metadata": {},
   "source": [
    "* Now, load the training and validation datasets using the `load_datasets` helper function, applying the appropriate transformations you just defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cde8df0-b529-4ccf-a7d4-c0fbea897238",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the training and validation datasets\n",
    "train_dataset, validation_dataset = helper_utils.load_datasets(\n",
    "    # Path to the dataset directory\n",
    "    dataset_path=dataset_path,\n",
    "    # Apply the defined training transformations\n",
    "    train_transform=train_transform,\n",
    "    # Apply the defined validation transformations\n",
    "    val_transform=val_transform,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5dd57a-5b76-422c-8bdb-2979add17e4a",
   "metadata": {},
   "source": [
    "* Extract the class names directly from the loaded training dataset and determine the total number of classes your model will need to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9a626d-f357-4187-853d-7869791b1051",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the list of class names automatically inferred from the folder structure\n",
    "classes = train_dataset.classes\n",
    "\n",
    "# Get the total number of classes\n",
    "num_classes = len(classes)\n",
    "\n",
    "# Print the discovered class names\n",
    "print(f\"Classes: {classes}\")\n",
    "# Print the total count of classes\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a5d284-77b3-4160-ba1d-2408e40d8e97",
   "metadata": {},
   "source": [
    "* It's always a good practice to visualize your data. Run the cell below to see a sample of images from the training set along with their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5874b13-6391-46fb-b3cf-a347f94ab57d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display a grid of sample images from the training dataset with their labels\n",
    "helper_utils.show_sample_images(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8912b4-3aa1-4528-bd31-c5126547166f",
   "metadata": {},
   "source": [
    "* Finally, create the `DataLoader` instances for both training and validation sets. These will handle shuffling the training data and organizing the images into batches for efficient processing during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15f712a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Create DataLoaders for managing batching and shuffling\n",
    "train_loader, val_loader = helper_utils.create_dataloaders(\n",
    "    # Pass the training dataset\n",
    "    train_dataset=train_dataset,\n",
    "    # Pass the validation dataset\n",
    "    validation_dataset=validation_dataset,\n",
    "    # Define the number of images per batch\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6eb8c1",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a name='1-3'></a>\n",
    "### 1.3 - Architecting the Classifier: Efficiency with Inverted Residuals\n",
    "\n",
    "With your data pipeline ready, it's time to design the engine that will learn to classify the fashion items. For the online retailer, efficiency is key. The model might eventually run on various devices, possibly even directly within a mobile app. This calls for an architecture that balances accuracy with computational cost.\n",
    "\n",
    "You'll build a classifier inspired by **MobileNetV2**, a highly successful architecture known for its performance on resource constrained devices. The core innovation you'll implement is the **Inverted Residual Block**.\n",
    "\n",
    "Unlike the residual blocks you might have seen in ResNet (which typically have a wide -> narrow -> wide structure in terms of channels), the inverted residual block uses a narrow -> wide -> narrow approach. It first expands the input channels using a computationally inexpensive 1x1 convolution, then applies a lightweight **depthwise separable convolution** for spatial feature extraction in the expanded space, and finally projects the features back down to a lower dimension with another 1x1 convolution. A skip connection, similar to ResNet, is also used to help with gradient flow. This design significantly reduces the number of parameters and computations compared to standard convolutional blocks, making the model faster and more memory efficient.\n",
    "\n",
    "You'll construct this classifier modularly:\n",
    "1.  First, you'll build the `InvertedResidualBlock` itself.\n",
    "2.  Then, you'll stack these blocks to create a `MobileNetBackbone`.\n",
    "3.  Finally, you'll add a simple classification `head` to the backbone to get the final `MobileNetLikeClassifier`.\n",
    "\n",
    "This modular approach makes your code cleaner, easier to debug, and highly reusable principles you've seen emphasized throughout your learning journey. Let's start building the fundamental block.\n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### Exercise 1 - InvertedResidualBlock\n",
    "\n",
    "Your first exercise is to implement the `InvertedResidualBlock`. This block forms the heart of your efficient classifier. You need to define the layers in the `__init__` method and specify how data flows through them in the `forward` method.\n",
    "\n",
    "**Your Task**:\n",
    "\n",
    "**Inside the `__init__` method**:\n",
    "> * **Expansion Layer (`self.expand`)**: Define an `nn.Sequential` block. This block needs to first expand the channels using a 1x1 convolution, then apply batch normalization, and finally use a ReLU activation.\n",
    ">\n",
    "> * **Projection Layer (`self.project`)**: Define another `nn.Sequential` block. This one will project the channels back down. It needs a 1x1 convolution (this time without a following activation) and a batch normalization layer.\n",
    "\n",
    "**Inside the `forward` method**:\n",
    "\n",
    "> * **Save the Skip Connection**: Before doing anything, you must save a reference to the input tensor `x`. This will be used for the residual connection.\n",
    ">\n",
    "> * Define the Main Path**: Pass the input `x` sequentially through your `self.expand`, `self.depthwise`, and `self.project` layers.\n",
    ">\n",
    "> * **Apply the Shortcut**: Inside the `if self.shortcut is not None:` block, you need to process the *original* input `x` (which you saved) through the `self.shortcut` module.\n",
    ">\n",
    "> * **Add the Residual**: After the `if` block, add your saved (and potentially processed) `skip` tensor to the output of the main path (`out`).\n",
    "\n",
    "<details> <summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
    "\n",
    "If you find yourself stuck, here is a more detailed breakdown.\n",
    "\n",
    "**For the `__init__` method**:\n",
    "\n",
    "* `self.expand`: You are building an `nn.Sequential`.\n",
    "    * The first layer is an `nn.Conv2d`. You need to set its `in_channels` to `in_channels`, `out_channels` to `hidden_dim`, and `kernel_size` to `1`. Remember to set `bias=False`.\n",
    "    * The second layer is `nn.BatchNorm2d`. You just need to pass the number of features, which is `hidden_dim`.\n",
    "    * The third layer is `nn.ReLU(inplace=True)`.\n",
    ">\n",
    "* `self.project`: This is another `nn.Sequential`.\n",
    "    * The first layer is an `nn.Conv2d`. This one goes from `hidden_dim` to `out_channels` with a `kernel_size` of `1` and `bias=False`.\n",
    "    * The second layer is `nn.BatchNorm2d`, and its number of features should match the output, so it's `out_channels`.\n",
    "\n",
    "**For the forward method**:\n",
    "\n",
    "* **Saving the skip**: `skip = x`\n",
    ">\n",
    "* **Main path**: You just need to call the layers you defined: `out = self.expand(x)`\n",
    "    * Follow this pattern to call `self.depthwise` on the result (`out`), and then `self.project` on that result.\n",
    ">\n",
    "* **Applying the shortcut**: Inside the `if` block, you need to update the skip variable: `skip = apply self.shortcut to the original input x`\n",
    ">\n",
    "* **Adding the residual**: This is a simple tensor addition: `out = out + skip (or use +=)`\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12775fb4",
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CLASS: InvertedResidualBlock\n",
    "\n",
    "class InvertedResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements an inverted residual block, often used in architectures like MobileNetV2.\n",
    "    \n",
    "    This block features an expansion phase (1x1 convolution), a depthwise\n",
    "    convolution (3x3 convolution), and a projection phase (1x1 convolution).\n",
    "    It utilizes a residual connection between the input and the output of the projection.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, stride, expansion_factor, shortcut=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the InvertedResidualBlock module.\n",
    "\n",
    "        Args:\n",
    "            in_channels: The number of channels in the input tensor.\n",
    "            out_channels: The number of channels in the output tensor.\n",
    "            stride (int): The stride to be used in the depthwise convolution.\n",
    "            expansion_factor (int): The factor by which to expand the input channels\n",
    "                                    in the expansion phase.\n",
    "            shortcut: An optional module to be used for the shortcut connection,\n",
    "                      typically to match dimensions if the stride is > 1 or\n",
    "                      if channel counts differ.\n",
    "        \"\"\"\n",
    "        # Initialize the parent nn.Module\n",
    "        super().__init__()\n",
    "        # Calculate the number of channels for the intermediate (expanded) representation\n",
    "        # The hidden dimension is the expanded number of channels\n",
    "        hidden_dim = in_channels * expansion_factor\n",
    "\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        # Define the expansion phase, which increases channel dimension\n",
    "        # Expansion phase: increases the number of channels\n",
    "        self.expand = nn.Sequential(\n",
    "            # 1x1 pointwise convolution\n",
    "            nn.Conv2d(in_channels, hidden_dim, kernel_size=1, bias=False),\n",
    "            # Batch normalization\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "        ) \n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Define the depthwise convolution phase\n",
    "        # Depthwise convolution: lightweight spatial convolution per channel\n",
    "        self.depthwise = nn.Sequential(\n",
    "            # 3x3 depthwise convolution\n",
    "            nn.Conv2d(\n",
    "                in_channels=hidden_dim,\n",
    "                out_channels=hidden_dim,\n",
    "                kernel_size=3,\n",
    "                stride=stride,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            # Batch normalization\n",
    "            nn.BatchNorm2d(num_features=hidden_dim),\n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        # Define the projection phase, which reduces channel dimension\n",
    "        # Projection phase: reduces the number of channels to out_channels\n",
    "        self.project = nn.Sequential(\n",
    "            # 1x1 pointwise convolution (linear)\n",
    "            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, bias=False),\n",
    "            # Batch normalization\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        ) \n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Store the provided shortcut module\n",
    "        # Optional shortcut connection for residual learning\n",
    "        self.shortcut = shortcut\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the InvertedResidualBlock.\n",
    "\n",
    "        Args:\n",
    "            x: The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after applying the block operations\n",
    "                          and the residual connection, followed by a ReLU activation.\n",
    "        \"\"\"\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        # Store the original input for the residual connection\n",
    "        # Save input for residual connection\n",
    "        skip = x\n",
    "\n",
    "        # Apply the expansion phase\n",
    "        # Forward pass through the block\n",
    "        # Expand channels\n",
    "        out = self.expand(x)\n",
    "        \n",
    "        # Apply the depthwise convolution\n",
    "        # Apply depthwise convolution\n",
    "        out = self.depthwise(out)\n",
    "        \n",
    "        # Apply the projection phase\n",
    "        # Project back to out_channels\n",
    "        out = self.project(out)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Check if a separate shortcut module is defined\n",
    "        # If shortcut exists (for matching dimensions), use it\n",
    "        # DO NOT REMOVE `None` from the `if` condition\n",
    "        if self.shortcut is not None:\n",
    "            \n",
    "        ### START CODE HERE ###\n",
    "            \n",
    "            # Apply the shortcut module to the original input\n",
    "            # Use the shortcut connection to match dimensions\n",
    "            skip = self.shortcut(x)\n",
    "\n",
    "        # Add the (potentially transformed) input (skip connection) to the output\n",
    "        # Add the skip connection\n",
    "        out = out + skip\n",
    "        ### END CODE HERE ###      \n",
    "        \n",
    "        # Apply the final ReLU activation\n",
    "        return F.relu(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfafb84",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# --- Verification ---\n",
    "# Define parameters for a sample block instance\n",
    "batch_size=32\n",
    "in_ch = 16 # Input channels\n",
    "out_ch = 16 # Output channels (same for stride=1)\n",
    "stride = 1\n",
    "exp_factor = 3 # Expansion factor\n",
    "img_size = 32 # Input image height/width\n",
    "\n",
    "# Instantiate the block\n",
    "block = InvertedResidualBlock(\n",
    "    in_channels=in_ch,\n",
    "    out_channels=out_ch,\n",
    "    stride=stride,\n",
    "    expansion_factor=exp_factor,\n",
    ")\n",
    "\n",
    "# Define the input tensor shape\n",
    "input_size = (batch_size, in_ch, img_size, img_size)\n",
    "\n",
    "# Configuration for torchinfo summary\n",
    "config = {\n",
    "    \"input_size\": input_size,\n",
    "    \"attr_names\": [\"input_size\", \"output_size\", \"num_params\"],\n",
    "    \"col_names_display\": [\"Input Shape \", \"Output Shape\", \"Param #\"],\n",
    "    \"depth\": 3 # Show layers up to 3 levels deep\n",
    "}\n",
    "\n",
    "# Generate the summary\n",
    "summary = torchinfo.summary(\n",
    "    model=block,\n",
    "    input_size=config[\"input_size\"],\n",
    "    col_names=config[\"attr_names\"],\n",
    "    depth=config[\"depth\"]\n",
    ")\n",
    "\n",
    "# Display the formatted summary\n",
    "print(\"--- Block Summary (Stride=1, Same Channels) ---\\n\")\n",
    "helper_utils.display_torch_summary(summary, config[\"attr_names\"], config[\"col_names_display\"], config[\"depth\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d65bda1",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Expected Output:\n",
    "\n",
    "<div style=\"float: left;\">\n",
    "\n",
    "| Layer (type (var_name):depth-idx) | Input Shape | Output Shape | Param # |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| InvertedResidualBlock (InvertedResidualBlock) | [32, 16, 32, 32] | [32, 16, 32, 32] | -- |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;Sequential (expand): 1-1 | [32, 16, 32, 32] | [32, 48, 32, 32] | -- |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Conv2d (0): 2-1 | [32, 16, 32, 32] | [32, 48, 32, 32] | 768 |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BatchNorm2d (1): 2-2 | [32, 48, 32, 32] | [32, 48, 32, 32] | 96 |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ReLU (2): 2-3 | [32, 48, 32, 32] | [32, 48, 32, 32] | -- |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;Sequential (depthwise): 1-2 | [32, 48, 32, 32] | [32, 48, 32, 32] | -- |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Conv2d (0): 2-4 | [32, 48, 32, 32] | [32, 48, 32, 32] | 20,736 |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BatchNorm2d (1): 2-5 | [32, 48, 32, 32] | [32, 48, 32, 32] | 96 |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ReLU (2): 2-6 | [32, 48, 32, 32] | [32, 48, 32, 32] | -- |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;Sequential (project): 1-3 | [32, 48, 32, 32] | [32, 16, 32, 32] | -- |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Conv2d (0): 2-7 | [32, 48, 32, 32] | [32, 16, 32, 32] | 768 |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BatchNorm2d (1): 2-8 | [32, 16, 32, 32] | [32, 16, 32, 32] | 32 |\n",
    "\n",
    "</div>\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "---\n",
    "**Total params:** 22,496  \n",
    "**Trainable params:** 22,496  \n",
    "**Non-trainable params:** 0  \n",
    "**Total mult-adds:** 0.73 GB  \n",
    "\n",
    "---\n",
    "**Input size (MB):** 2.00  \n",
    "**Forward/backward pass size (MB):** 56.00  \n",
    "**Params size (MB):** 0.09  \n",
    "**Estimated Total Size (MB):** 58.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9847a00b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "unittests.exercise_1(InvertedResidualBlock)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e6ba04-bfb0-4b1d-93f8-9911be54aa5e",
   "metadata": {},
   "source": [
    "<a name='ex-2'></a>\n",
    "### Exercise 2 - MobileNetBackbone\n",
    "\n",
    "Now that you have a working `InvertedResidualBlock`, you'll assemble the main feature extractor, the `MobileNetBackbone`. This involves creating an initial \"stem\" layer to process the input image and then stacking several `InvertedResidualBlock` instances using a helper method.\n",
    "\n",
    "**Your Task**:\n",
    "\n",
    "**Inside the `__init__` method**:\n",
    "> * You must define the `self.blocks` sequential layer.\n",
    "> * You will do this by calling the `self._make_block` helper function three times, once for each block in the sequence:\n",
    ">\n",
    ">    * The first block: `in_channels=16`, `out_channels=24`, `stride=2`, `expansion_factor=3`.\n",
    ">    * The second block: `in_channels=24`, `out_channels=32`, `stride=2`, `expansion_factor=3`.\n",
    ">    * The third block: `in_channels=32`, `out_channels=64`, `stride=2`, `expansion_factor=6`.\n",
    "\n",
    "**Inside the `_make_block` method**:\n",
    "> * **Shortcut Logic**: First, you must define the boolean `condition` that determines if a shortcut connection is needed. A shortcut is necessary if the input and output dimensions are different, which happens if the `stride` is not 1 OR if the `in_channels` and `out_channels` do not match.\n",
    ">\n",
    "> * **Block Instantiation**: At the end of the method, you must create and return an instance of the `InvertedResidualBlock` (which you built in the previous exercise). You have all the required parameters available as arguments to the `_make_block` function.\n",
    "\n",
    "<details> <summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
    "\n",
    "If you find yourself stuck, here is a more detailed breakdown.\n",
    "\n",
    "**For the `__init__` method**:\n",
    "\n",
    "* You are filling in the `nn.Sequential` for `self.blocks`.\n",
    "* The first line will be `self._make_block(16, 24, stride=2, expansion_factor=3),`\n",
    "* You should follow this exact pattern for the next two blocks, using the parameters specified in the instructions for each line.\n",
    "\n",
    "**For the `_make_block` method**:\n",
    "\n",
    "* **Defining the condition**: You need to write a boolean expression that checks for two things.\n",
    "    > `condition = (check if in_channels is not equal to out_channels) or (check if stride is not equal to 1)`\n",
    "\n",
    "* **Instantiating the `block`**:\n",
    "    * You are just calling the constructor for the `InvertedResidualBlock` class.\n",
    "    * You need to pass all five arguments that `_make_block` received:\n",
    "    > `block = create a new InvertedResidualBlock(using in_channels, out_channels, stride, expansion_factor, and shortcut)`\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44ddc26",
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CLASS: MobileNetBackbone\n",
    "\n",
    "class MobileNetBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a simplified MobileNet-like backbone feature extractor.\n",
    "\n",
    "    This class defines the initial stem and a sequence of inverted residual blocks\n",
    "    to extract features from an input image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the layers of the MobileNet backbone.\n",
    "        \"\"\"\n",
    "        # Call the parent class (nn.Module) constructor\n",
    "        super().__init__()\n",
    "        # Define the initial \"stem\" convolution layer\n",
    "        # This layer reduces spatial size and increases channel depth\n",
    "        self.stem = nn.Sequential(\n",
    "            # 3x3 convolution with stride 2\n",
    "            nn.Conv2d(3, 16, 3, stride=2, padding=1, bias=False),  # 3 input channels (RGB), 16 output\n",
    "            # Apply batch normalization\n",
    "            nn.BatchNorm2d(16),\n",
    "            # Apply ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        # Define the main stack of custom MobileNet-like blocks\n",
    "        self.blocks = nn.Sequential(  \n",
    "            # Each block progressively increases channels and reduces spatial dimensions\n",
    "            # Create the first block\n",
    "            self._make_block(\n",
    "                in_channels=16,\n",
    "                out_channels=24,\n",
    "                stride=2,\n",
    "                expansion_factor=6,\n",
    "            ),\n",
    "            # Create the second block\n",
    "            self._make_block(\n",
    "                in_channels=24,\n",
    "                out_channels=32,\n",
    "                stride=2,\n",
    "                expansion_factor=6,\n",
    "            ),\n",
    "            # Create the third block\n",
    "            self._make_block(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                stride=2,\n",
    "                expansion_factor=6,\n",
    "            ),\n",
    "        )  \n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    def _make_block(self, in_channels, out_channels, stride=1, expansion_factor=6):\n",
    "        \"\"\"\n",
    "        Helper method to create a single InvertedResidualBlock.\n",
    "\n",
    "        Arguments:\n",
    "            in_channels: Number of input channels.\n",
    "            out_channels: Number of output channels.\n",
    "            stride: The stride to be used in the depthwise convolution.\n",
    "            expansion_factor: The factor to expand the channels internally.\n",
    "        \"\"\"\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        # Determine if a shortcut connection is needed\n",
    "        # A shortcut is needed if input/output channels differ or if stride > 1\n",
    "        condition = (in_channels != out_channels) or (stride > 1)\n",
    "        # If a shortcut is needed\n",
    "        if condition:\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "            # Define the shortcut connection\n",
    "            shortcut = nn.Sequential(\n",
    "                # 1x1 convolution to match dimensions and apply stride\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride=stride, bias=False),\n",
    "                # Apply batch normalization\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # No shortcut connection is needed\n",
    "            shortcut = nn.Identity()\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        # Instantiate the InvertedResidualBlock\n",
    "        block = InvertedResidualBlock(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            stride=stride,\n",
    "            expansion_factor=expansion_factor,\n",
    "            shortcut=shortcut,\n",
    "        )\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Return the created block\n",
    "        return block\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the backbone.\n",
    "\n",
    "        Arguments:\n",
    "            x: The input tensor (e.g., a batch of images).\n",
    "\n",
    "        Returns:\n",
    "            The output feature map tensor.\n",
    "        \"\"\"\n",
    "        # Pass the input through the initial stem layer\n",
    "        x = self.stem(x)\n",
    "        # Pass the result through the main stack of blocks\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        # Return the final feature map\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b983721f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# --- Verification ---\n",
    "# Define parameters for verification\n",
    "batch_size=32\n",
    "img_size = 64 # Input image height/width\n",
    "depth = 3 # Summary depth\n",
    "\n",
    "# Instantiate the backbone\n",
    "backbone = MobileNetBackbone()\n",
    "\n",
    "# Define the input tensor shape\n",
    "input_size = (batch_size, 3, img_size, img_size)\n",
    "\n",
    "# Configuration for torchinfo summary\n",
    "config = {\n",
    "    \"input_size\": input_size,\n",
    "    \"attr_names\": [\"input_size\", \"output_size\", \"num_params\"],\n",
    "    \"col_names_display\": [\"Input Shape \", \"Output Shape\", \"Param #\"],\n",
    "    \"depth\": depth\n",
    "}\n",
    "\n",
    "# Generate the summary\n",
    "summary = torchinfo.summary(\n",
    "    model=backbone,\n",
    "    input_size=config[\"input_size\"],\n",
    "    col_names=config[\"attr_names\"],\n",
    "    depth=config[\"depth\"]\n",
    ")\n",
    "\n",
    "# Display the formatted summary\n",
    "print(\"--- Backbone Summary ---\\n\")\n",
    "helper_utils.display_torch_summary(summary, config[\"attr_names\"], config[\"col_names_display\"], config[\"depth\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51110f2",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Expected Output:\n",
    "\n",
    "<div style=\"float: left;\">\n",
    "\n",
    "| Layer (type (var_name):depth-idx) | Input Shape | Output Shape | Param # |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| MobileNetBackbone (MobileNetBackbone) | [32, 3, 64, 64] | [32, 64, 4, 4] | -- |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;Sequential (stem): 1-1 | [32, 3, 64, 64] | [32, 16, 32, 32] | -- |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Conv2d (0): 2-1 | [32, 3, 64, 64] | [32, 16, 32, 32] | 432 |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BatchNorm2d (1): 2-2 | [32, 16, 32, 32] | [32, 16, 32, 32] | 32 |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ReLU (2): 2-3 | [32, 16, 32, 32] | [32, 16, 32, 32] | -- |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;Sequential (blocks): 1-2 | [32, 16, 32, 32] | [32, 64, 4, 4] | -- |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;InvertedResidualBlock (0): 2-4 | [32, 16, 32, 32] | [32, 24, 16, 16] | -- |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sequential (expand): 3-1 | [32, 16, 32, 32] | [32, 48, 32, 32] | 864 |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sequential (depthwise): 3-2 | [32, 48, 32, 32] | [32, 48, 16, 16] | 20,832 |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sequential (project): 3-3 | [32, 48, 16, 16] | [32, 24, 16, 16] | 1,200 |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sequential (shortcut): 3-4 | [32, 16, 32, 32] | [32, 24, 16, 16] | 432 |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;InvertedResidualBlock (1): 2-5 | [32, 24, 16, 16] | [32, 32, 8, 8] | -- |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sequential (expand): 3-5 | [32, 24, 16, 16] | [32, 72, 16, 16] | 1,872 |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sequential (depthwise): 3-6 | [32, 72, 16, 16] | [32, 72, 8, 8] | 46,800 |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sequential (project): 3-7 | [32, 72, 8, 8] | [32, 32, 8, 8] | 2,368 |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sequential (shortcut): 3-8 | [32, 24, 16, 16] | [32, 32, 8, 8] | 832 |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;InvertedResidualBlock (2): 2-6 | [32, 32, 8, 8] | [32, 64, 4, 4] | -- |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sequential (expand): 3-9 | [32, 32, 8, 8] | [32, 192, 8, 8] | 6,528 |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sequential (depthwise): 3-10 | [32, 192, 8, 8] | [32, 192, 4, 4] | 332,160 |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sequential (project): 3-11 | [32, 192, 4, 4] | [32, 64, 4, 4] | 12,416 |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sequential (shortcut): 3-12 | [32, 32, 8, 8] | [32, 64, 4, 4] | 2,176 |\n",
    "    \n",
    "</div>\n",
    "<div style=\"clear: both;\"></div>      \n",
    "\n",
    "---\n",
    "**Total params:** 428,944  \n",
    "**Trainable params:** 428,944  \n",
    "**Non-trainable params:** 0  \n",
    "**Total mult-adds:** 0.53 GB  \n",
    "\n",
    "---\n",
    "**Input size (MB):** 1.50  \n",
    "**Forward/backward pass size (MB):** 65.75  \n",
    "**Params size (MB):** 1.64  \n",
    "**Estimated Total Size (MB):** 68.89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e68e8c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "unittests.exercise_2(MobileNetBackbone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aba994",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a name='1-4'></a>\n",
    "### 1.4 - Assembling the Full Classifier\n",
    "\n",
    "With the backbone complete, the final step is to assemble the full `MobileNetLikeClassifier`. This involves combining your `MobileNetBackbone` with a simple classification head. The head takes the feature map produced by the backbone, reduces its spatial dimensions using **Adaptive Average Pooling**, flattens the result into a vector, and finally uses a **Linear layer** to produce the raw output scores (logits) for each class.\n",
    "\n",
    "This modular assembly highlights the power of separating feature extraction (backbone) from the final task specific layers (head), making it easy to adapt the backbone for different purposes later on.\n",
    "\n",
    "* The `MobileNetLikeClassifier` class is provided below. Instantiate it, making sure to pass the correct `num_classes` for your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda331ff",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "class MobileNetLikeClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A classifier model that combines a feature extraction\n",
    "    backbone with a simple classification head.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        \"\"\"\n",
    "        Initializes the classifier components.\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): The number of output classes for the final\n",
    "                               classification layer.\n",
    "        \"\"\"\n",
    "        # Initialize the parent nn.Module\n",
    "        super().__init__()\n",
    "\n",
    "        # Backbone extracts features from input images\n",
    "        self.backbone = MobileNetBackbone()\n",
    "\n",
    "        # Head processes the features to produce class predictions\n",
    "        self.head = nn.Sequential(\n",
    "            # Reduce spatial dimensions to 1x1\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            # Flatten the features into a 1D vector\n",
    "            nn.Flatten(),\n",
    "            # Map the flattened features to the number of output classes\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the classifier.\n",
    "\n",
    "        Args:\n",
    "            x: The input tensor (batch of images).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The raw, unnormalized output scores (logits)\n",
    "                          for each class.\n",
    "        \"\"\"\n",
    "        # Pass the input through the feature extraction backbone\n",
    "        x = self.backbone(x)\n",
    "        # Pass the features through the classification head\n",
    "        x = self.head(x)\n",
    "        # Return the final output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1889293-912e-48f9-bb22-f43b1eb2c8fb",
   "metadata": {},
   "source": [
    "* Instantiate the complete classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bad5e30",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Ensure num_classes matches the number of categories in your dataset\n",
    "mobilenet_classifier = MobileNetLikeClassifier(num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc9bce5",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Generate and display a summary of the complete `MobileNetLikeClassifier`. This final check ensures the backbone and head are connected correctly, and you can see the data flow from the input image to the final class logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac5b8d6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Define parameters for verification\n",
    "batch_size=32\n",
    "img_size = 64 # Input image height/width\n",
    "depth = 3 # Summary depth\n",
    "\n",
    "# Define the input tensor shape\n",
    "input_size = (batch_size, 3, img_size, img_size)\n",
    "\n",
    "# Configuration for torchinfo summary\n",
    "config = {\n",
    "    \"input_size\": input_size,\n",
    "    \"attr_names\": [\"input_size\", \"output_size\", \"num_params\"],\n",
    "    \"col_names_display\": [\"Input Shape \", \"Output Shape\", \"Param #\"],\n",
    "    \"depth\": depth # Show layers up to 3 levels deep for detail\n",
    "}\n",
    "\n",
    "# Generate the summary for the complete classifier\n",
    "summary = torchinfo.summary(\n",
    "    model=mobilenet_classifier,\n",
    "    input_size=config[\"input_size\"],\n",
    "    col_names=config[\"attr_names\"],\n",
    "    depth=config[\"depth\"]\n",
    ")\n",
    "\n",
    "# Display the formatted summary\n",
    "print(\"--- Classifier Summary ---\\n\")\n",
    "helper_utils.display_torch_summary(summary, config[\"attr_names\"], config[\"col_names_display\"], config[\"depth\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5353ce",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a name='1-5'></a>\n",
    "### 1.5 - Training the Classifier\n",
    "\n",
    "Your efficient `MobileNetLikeClassifier` is fully assembled! Now it's time to teach it how to distinguish between different fashion items. This involves setting up the essential components for the training process:\n",
    "\n",
    "**Class Weights**: Real-world datasets are often imbalanced, meaning some classes have many more examples than others. Training directly on such data can bias the model towards the majority classes. To counteract this, you'll calculate **class weights**. These weights give more importance to under-represented classes during loss calculation, encouraging the model to learn them effectively.\n",
    "\n",
    "**Loss Function**: Measures how far the model's predictions are from the true labels. `CrossEntropyLoss` is suitable for this multi-class classification task, and you'll configure it to use the calculated class weights.\n",
    "\n",
    "**Optimizer**: Adjusts the model's weights based on the loss to improve performance. `Adam` is a popular and effective choice.\n",
    "\n",
    "**Learning Rate Scheduler**: Dynamically adjusts the learning rate during training. A `StepLR` scheduler will decrease the learning rate periodically, which can help the model converge more effectively.\n",
    "\n",
    "* First, calculate the class weights based on the distribution of samples in your training dataset. Then, define the weighted `CrossEntropyLoss` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e69e082",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Calculate class weights to handle imbalance in the dataset\n",
    "class_weights = helper_utils.compute_class_weights(train_dataset)\n",
    "\n",
    "# Move the weights tensor to the correct device (e.g., 'cuda' or 'cpu')\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "# Define the loss function for multi-class classification, incorporating the calculated class weights\n",
    "loss_fcn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Print the calculated weights for verification\n",
    "print(\"Calculated class weights:\")\n",
    "for i, weight in enumerate(class_weights):\n",
    "    print(f\"- Class '{classes[i]}': {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8a9941-96ba-4f7d-b72b-fa011cf3159c",
   "metadata": {},
   "source": [
    "* Next, define the optimizer (`Adam`) and the learning rate scheduler (`StepLR`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33578275-ce58-412c-82ef-7d682881eedd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the Adam optimizer, passing the model's parameters and initial learning rate\n",
    "optimizer = torch.optim.Adam(mobilenet_classifier.parameters(), lr=0.01)\n",
    "\n",
    "# Define a learning rate scheduler that reduces the LR by a factor of 0.1 every 5 epochs\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a82386e",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now you are ready to start the training loop. The `helper_utils.training_loop` function will handle iterating through the data, calculating loss (using your weighted loss function), backpropagating, and updating the model weights for a specified number of epochs.\n",
    "\n",
    "* Set the number of epochs for this training run. Feel free to start with a small number (like 5) to see the process working, and increase it later for better performance if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d29031a-a63d-49c2-a796-ffdf96b8ba4b",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# EDITABLE CELL:\n",
    "\n",
    "# Set the number of epochs for training\n",
    "n_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29f9969-a6f5-45ca-b5f7-dbd2ae7edfb5",
   "metadata": {},
   "source": [
    "* Launch the training process using the `training_loop` helper function. This will train your `mobilenet_classifier` and return the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103f712f-42f9-4668-9952-f8a540bec79a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start the training loop\n",
    "trained_classifier =  helper_utils.training_loop(\n",
    "    mobilenet_classifier, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    loss_fcn, \n",
    "    optimizer, \n",
    "    scheduler, \n",
    "    device, \n",
    "    n_epochs=n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5018742c-6bb8-41b7-9c67-f5466181a1a8",
   "metadata": {},
   "source": [
    "<a name='1-6'></a>\n",
    "### 1.6 - Evaluating the Classifier\n",
    "\n",
    "Excellent work! You've successfully trained your custom-built `MobileNetLikeClassifier`. You went through the entire process: designing a reusable block (`InvertedResidualBlock`), assembling a backbone (`MobileNetBackbone`) from those blocks, adding a classification head, and orchestrating the training loop with weighted loss.\n",
    "\n",
    "Keep in mind that the primary goal of these exercises was to practice building models with **modular components** and understanding how custom architectures are constructed. Achieving extremely high accuracy often requires more extensive data preprocessing, augmentation, and longer training times than used here. The accuracy you see reflects the model learning from the provided data structure, but the real accomplishment is mastering the *construction* process itself.\n",
    "\n",
    "* Now, let's visualize some predictions from your trained model. This gives you a qualitative sense of its performance. Remember, given the focus on architecture rather than extensive tuning, you might see some incorrect predictions, which is perfectly normal in the model development cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5826dde4-5a7b-466e-94b5-4c12956d496d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display predictions\n",
    "helper_utils.display_random_predictions_per_class(trained_classifier, val_loader, classes, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855c758a",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Building a Visual Search Engine\n",
    "\n",
    "You've successfully built a classifier to categorize fashion items. Now, you'll tackle a different, equally important challenge for the online retailer: **visual search**. This feature allows customers to find products similar to an item they like, enhancing discoverability and potentially increasing sales.\n",
    "\n",
    "<a name='2-1'></a>\n",
    "### 2.1 - Powering Recommendations with Similarity Learning\n",
    "\n",
    "Imagine a customer browsing your store finds a t-shirt they like. How can you instantly show them other t-shirts with a similar style, color, or pattern? Or, what if they upload a picture of a hat they saw elsewhere â€“ how can you find the closest match in your inventory? Manually tagging and linking thousands of products is impractical.\n",
    "\n",
    "This is where **similarity learning** and **Siamese Networks** shine. Instead of learning to classify items, a Siamese Network learns to map images into a special \"embedding space\" where visually similar items are located close together. By comparing the \"distances\" between items in this space, you can power several high impact features:\n",
    "\n",
    "* **Visual Search (\"Shop the Look\"):** Allow customers to upload photos and find matching items in your catalog. (Only this is covered in this notebook later)\n",
    "* **Smarter Recommendations (\"You Might Also Like\"):** Show relevant alternatives based on visual similarity to the currently viewed product.\n",
    "* **Handling Out-of-Stock Items:** Suggest visually similar in stock alternatives when a product isn't available.\n",
    "\n",
    "To build this system, you'll first need to prepare the data in a specific way that teaches the model the concept of similarity.\n",
    "\n",
    "<a name='2-2'></a>\n",
    "### 2.2 - Teaching Similarity: The Triplet Dataset\n",
    "\n",
    "Unlike standard classification where the model learns from single images and labels, a Siamese Network learns from **comparisons**. To teach it which fashion items are visually similar (belonging to the same category, in this simplified case) and which are dissimilar, you need to provide examples in a specific format: **triplets**.\n",
    "\n",
    "Each triplet consists of three images:\n",
    "1.  **Anchor:** An image of a fashion item (e.g., a specific t-shirt).\n",
    "2.  **Positive:** *Another* image from the **same category** as the anchor (e.g., a different t-shirt).\n",
    "3.  **Negative:** An image from a **different category** than the anchor (e.g., a pair of pants).\n",
    "\n",
    "By training on these triplets, the model learns to pull the anchor and positive embeddings closer together in the embedding space, while pushing the anchor and negative embeddings further apart. This process sculpts the embedding space, making it meaningful for visual search.\n",
    "\n",
    "You'll create a custom PyTorch `Dataset` called `TripleDataset` that generates these triplets on the fly from your existing fashion dataset.\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### Exercise 3 - TripleDataset\n",
    "\n",
    "Your task is to complete the `TripleDataset` class. It wraps your existing `train_dataset` and generates triplets dynamically. You need to implement the logic for selecting appropriate positive and negative examples for a given anchor image.\n",
    "\n",
    "**Your Task**:\n",
    "\n",
    "**Inside the `_get_positive_negative_indices` method**:\n",
    "\n",
    "> * **Find Positive Index**: Get the list of all indices that match the `anchor_label`. From that list, select one random index. You should use the <code>[random.choice()](https://www.w3schools.com/python/ref_random_choice.asp)</code> function for this.\n",
    ">\n",
    "> * **Find Negative Index**: A random `negative_label` (different from the `anchor_label`) is already selected for you. You just need to get the list of all indices for that `negative_label` and select one random index from it, again using <code>[random.choice()](https://www.w3schools.com/python/ref_random_choice.asp)</code>.\n",
    "\n",
    "**Inside the `__getitem__` method**:\n",
    "\n",
    "> * **Get Anchor**: Get the `anchor_image` and `anchor_label` from `self.dataset` using the provided `idx`.\n",
    ">\n",
    "> * **Get Indices**: Call the `_get_positive_negative_indices` helper function, passing it the `anchor_label` to get the `positive_index` and `negative_index`.\n",
    ">\n",
    "> * **Get Positive & Negative Images**: Use the `positive_index` and `negative_index` you just received to get the corresponding `positive_image` and `negative_image` from `self.dataset`.\n",
    "\n",
    "<details> <summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
    "\n",
    "If you find yourself stuck, here is a more detailed breakdown.\n",
    "\n",
    "**For the `_get_positive_negative_indices` method**:\n",
    "\n",
    "* **Positive**:\n",
    "    * First, get the list: `positive_indices = self.labels_to_indices[anchor_label]`\n",
    "    * Then, pick one: `positive_index = use random.choice() on the positive_indices list`\n",
    ">\n",
    "* **Negative**:\n",
    "    * This follows the same pattern as the positive one.\n",
    "    * First, get the list: `negative_indices = self.labels_to_indices[negative_label]`\n",
    "    * Then, pick one: `negative_index = use random.choice() on the negative_indices list`\n",
    ">\n",
    "* You will need to use `random.choice(sequence)`, which returns a random element from a non-empty sequence.\n",
    "\n",
    "**For the `__getitem__` method**:\n",
    "\n",
    "* **Anchor**: `anchor_image, anchor_label = self.dataset[idx]`\n",
    ">\n",
    "* **Get Indices**: `positive_index, negative_index = call self._get_positive_negative_indices(passing the anchor_label)`\n",
    ">\n",
    "* **Get Positive/Negative Images**:\n",
    "    * When you get an item from `self.dataset`, it returns `(image, label)`. You only want the image.\n",
    "        > `positive_image, _ = self.dataset[positive_index]`\n",
    "    * Do the exact same thing for `negative_image` using `negative_index`.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344a0309",
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CLASS: TripleDataset\n",
    "\n",
    "class TripleDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom Dataset class that returns triplets of images (anchor, positive, negative).\n",
    "\n",
    "    This class wraps a standard dataset and, for a given index, returns the \n",
    "    item at that index (anchor), a random item with the same label (positive),\n",
    "    and a random item with a different label (negative).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        \"\"\"\n",
    "        Initializes the TripleDataset.\n",
    "\n",
    "        Args:\n",
    "            dataset: The base dataset (e.g., torchvision.datasets) \n",
    "                     which contains (data, label) pairs.\n",
    "        \"\"\"\n",
    "\n",
    "        # Store the original dataset\n",
    "        self.dataset = dataset\n",
    "\n",
    "        # Get a list of all available labels\n",
    "        self.labels = range(len(dataset.classes))\n",
    "\n",
    "        # Create a mapping from labels to their corresponding indices in the dataset\n",
    "        self.labels_to_indices = self._get_labels_to_indices()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of items in the dataset.\n",
    "        \"\"\"\n",
    "        # The length is the same as the original wrapped dataset\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def _get_labels_to_indices(self):\n",
    "        \"\"\"\n",
    "        Creates a dictionary mapping each label to a list of indices.\n",
    "        \n",
    "        Returns:\n",
    "            A dictionary where keys are labels and values are lists of \n",
    "            indices in the dataset that have that label.\n",
    "        \"\"\"\n",
    "        # Initialize an empty dictionary\n",
    "        labels_to_indices = {}\n",
    "        # Iterate over the entire dataset\n",
    "        for idx, (_, label) in enumerate(self.dataset):\n",
    "            # If the label is not yet in the dictionary, add it with an empty list\n",
    "            if label not in labels_to_indices:\n",
    "                labels_to_indices[label] = []\n",
    "            # Append the current index to the list for its label\n",
    "            labels_to_indices[label].append(idx)\n",
    "        # Return the completed map\n",
    "        return labels_to_indices\n",
    "    \n",
    "\n",
    "    def _get_positive_negative_indices(self, anchor_label):\n",
    "        \"\"\"\n",
    "        Finds random indices for a positive and a negative sample.\n",
    "\n",
    "        Args:\n",
    "            anchor_label: The label of the anchor sample.\n",
    "\n",
    "        Returns:\n",
    "            A tuple (positive_index, negative_index).\n",
    "        \"\"\"\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        # Get all indices for the anchor label\n",
    "        positive_indices = self.labels_to_indices[anchor_label]\n",
    "        # Randomly select one index from the list of positive indices\n",
    "        positive_index = random.choice(positive_indices)\n",
    "\n",
    "        # Get all indices for a negative label\n",
    "        # Randomly choose a label that is different from the anchor label\n",
    "        negative_label = random.choice([label for label in self.labels if label != anchor_label]) \n",
    "        \n",
    "        # Get all indices for the chosen negative label\n",
    "        negative_indices = self.labels_to_indices[negative_label]\n",
    "        # Randomly select one index from the list of negative indices\n",
    "        negative_index = random.choice(negative_indices)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return positive_index, negative_index\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves a triplet (anchor, positive, negative) for a given index.\n",
    "\n",
    "        Args:\n",
    "            idx: The index of the anchor item.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing the anchor image, positive image, and negative image.\n",
    "        \"\"\"\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        # Get the anchor image and label\n",
    "        anchor_image, anchor_label = self.dataset[idx]\n",
    "\n",
    "        # Get positive and negative indices based on the anchor label\n",
    "        positive_index, negative_index = self._get_positive_negative_indices(anchor_label)\n",
    "\n",
    "        # Get a positive image (same label)\n",
    "        positive_image, _ = self.dataset[positive_index]\n",
    "\n",
    "        # Get a negative image (different label)\n",
    "        negative_image, _ = self.dataset[negative_index]\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return (anchor_image, positive_image, negative_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2117a274",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# --- Verification Cell ---\n",
    "# Set seed for reproducibility of random sampling within the dataset\n",
    "random.seed(42)\n",
    "\n",
    "# Create a copy of the validation dataset to use as a base for the toy triplet dataset\n",
    "toy_dataset_base = copy.deepcopy(validation_dataset)\n",
    "\n",
    "# Apply a simple transformation\n",
    "toy_dataset_base.transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Instantiate the TripleDataset using the prepared base dataset\n",
    "triple_dataset_toy = TripleDataset(dataset=toy_dataset_base)\n",
    "\n",
    "# Retrieve the first triplet (index 0) from the dataset\n",
    "anchor_img, positive_image, negative_image = triple_dataset_toy[0]\n",
    "\n",
    "# Display the triplet\n",
    "images_list = [anchor_img, positive_image, negative_image]\n",
    "grid = vutils.make_grid(images_list, nrow=3, padding=2) # nrow=3 ensures side-by-side display\n",
    "grid_pil = transforms.ToPILImage()(grid)\n",
    "print(\"Sample Triplet (Anchor, Positive, Negative):\")\n",
    "display(grid_pil)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ba5d2c",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Expected Output:**\n",
    "\n",
    "**Anchor**, **Positive (same class)**, **Negative (different class)**\n",
    "\n",
    "<img src=\"./nb_images/anchor.png\" width=\"150\"/> <img src=\"./nb_images/positive.png\" width=\"150\"/> <img src=\"./nb_images/negative.png\" width=\"150\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3809b1f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "unittests.exercise_3(TripleDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df462e9-3315-41ff-9c88-fff387e70198",
   "metadata": {},
   "source": [
    "* Now that your `TripleDataset` class is defined and verified, create an instance using your main `train_dataset`. Then, wrap it in a DataLoader to handle batching and shuffling during the training of your Siamese network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb3fb3c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Instantiate the TripleDataset using the main training dataset\n",
    "triple_dataset = TripleDataset(train_dataset)\n",
    "\n",
    "# Create a DataLoader for the TripleDataset\n",
    "siamese_dataloader = torch.utils.data.DataLoader(\n",
    "    triple_dataset,\n",
    "    batch_size=32,  # Define the number of triplets per batch\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baca9d92-7566-4d23-87e7-ac26ab080908",
   "metadata": {},
   "source": [
    "---\n",
    "# Submission Note\n",
    "\n",
    "Congratulations! You've completed the final graded exercise of this assignment.\n",
    "\n",
    "If you've successfully passed all the unit tests above, you've completed the core requirements of this assignment. Feel free to [submit](#submission) your work now. The grading process runs in the background, so it will not disrupt your progress and you can continue on with the rest of the material.\n",
    "\n",
    "**ðŸš¨ IMPORTANT NOTE** If you have passed all tests within the notebook, but the autograder shows a system error after you submit your work:\n",
    "\n",
    "<div style=\"background-color: #1C1C1E; border: 1px solid #444444; color: #FFFFFF; padding: 15px; border-radius: 5px;\">\n",
    "    <p><strong>Grader Error: Grader feedback not found</strong></p>\n",
    "    <p>Autograder failed to produce the feedback...</p>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "This is typically a temporary system glitch. The most common solution is to resubmit your assignment, as this often resolves the problem. Occasionally, it may be necessary to resubmit more than once. \n",
    ">\n",
    "If the error persists, please reach out for support in the [DeepLearning.AI Community Forum](https://community.deeplearning.ai/c/course-q-a/pytorch-for-developers/pytorch-advanced-architectures-and-deployment/562).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d785df83",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a name='2-3'></a>\n",
    "### 2.3 - Architecting the Visual Search Model\n",
    "\n",
    "With your `TripleDataset` ready to feed comparison examples, you can now build the Siamese Network itself. This architecture cleverly reuses the feature extraction capabilities you already built for the classifier, demonstrating the power of modular design.\n",
    "\n",
    "<a name='2-3-1'></a>\n",
    "#### 2.3.1 - The Siamese Encoder: Reusing the Backbone\n",
    "\n",
    "The core of the Siamese Network is the **encoder**. This is the part that takes an image and converts it into a meaningful numerical representation (an embedding). Instead of building a new encoder from scratch, you'll leverage the `MobileNetBackbone` you created earlier. This is a prime example of **parameter sharing** and modularity in action â€“ the backbone learns general visual features, which are useful for both classification and similarity tasks.\n",
    "\n",
    "You'll wrap this backbone in a `SiameseEncoder` class. This wrapper adds a simple \"representation head\" consisting of an `AdaptiveAvgPool2d` layer followed by a `Flatten` layer. This head takes the 2D feature map produced by the backbone and converts it into a fixed size 1D vector â€“ the final embedding.\n",
    "\n",
    "* The `SiameseEncoder` class is defined below. Notice how it takes a `backbone` as input during initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc66a679",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "class SiameseEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements an encoder module suitable for Siamese networks.\n",
    "\n",
    "    This class takes a pre-defined backbone (feature extractor) and adds a\n",
    "    representation head (pooling + flatten) to produce a fixed-size vector\n",
    "    embedding for an input image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, backbone):\n",
    "        \"\"\"\n",
    "        Initializes the SiameseEncoder.\n",
    "\n",
    "        Args:\n",
    "            backbone (nn.Module): The convolutional neural network to use\n",
    "                                  as the feature extractor.\n",
    "        \"\"\"\n",
    "        # Initialize the parent nn.Module\n",
    "        super(SiameseEncoder, self).__init__()\n",
    "\n",
    "        # Store the provided backbone model\n",
    "        self.backbone = backbone\n",
    "\n",
    "        # Define the representation head\n",
    "        self.representation = nn.Sequential(\n",
    "            # Apply adaptive average pooling to reduce spatial dimensions to 1x1\n",
    "            # This makes the output size independent of the input image size (after backbone)\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            # Flatten the 1x1 feature map into a 1D vector\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass through the encoder.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor (e.g., a batch of images).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The final 1D embedding vector for the input.\n",
    "        \"\"\"\n",
    "        # 1. Extract features using the backbone\n",
    "        features = self.backbone(x)\n",
    "        # 2. Convert feature map to a fixed-size vector using the representation head\n",
    "        representation = self.representation(features)\n",
    "        # Return the resulting embedding vector\n",
    "        return representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22066389",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Instantiate the `SiameseEncoder`. You'll pass the backbone, `trained_classifier.backbone` *from your previously created `mobilenet_classifier`* to demonstrate reuse. This is efficient â€“ the backbone's learned features are directly transferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e43fc23",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Create the Siamese Encoder instance\n",
    "# Reuse the backbone from the classifier model you built earlier!\n",
    "siamese_encoder = SiameseEncoder(\n",
    "    backbone=trained_classifier.backbone # Pass the existing backbone\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8bd5ed",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a name='2-3-2'></a>\n",
    "#### 2.3.2 - The Siamese Network Wrapper\n",
    "\n",
    "Now you create the main `SiameseNetwork` class. This acts as a wrapper around your `siamese_encoder`. Its primary role during training is to take the three images of a triplet (anchor, positive, negative), pass *each* of them through the *same* `siamese_encoder` instance (ensuring shared weights), and return the three resulting embeddings. It also includes a `get_embedding` method, which is useful for processing single images during inference (when you want to find similar items).\n",
    "\n",
    "* The `SiameseNetwork` class is defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d245eaf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the main Siamese Network structure.\n",
    "\n",
    "    This network takes multiple inputs (anchor, positive, negative during training)\n",
    "    and processes each through a shared `embedding_network` (the SiameseEncoder)\n",
    "    to produce corresponding embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_network):\n",
    "        \"\"\"\n",
    "        Initializes the SiameseNetwork.\n",
    "\n",
    "        Args:\n",
    "            embedding_network (nn.Module): The shared encoder network (e.g., SiameseEncoder)\n",
    "                                           that generates embeddings from images.\n",
    "        \"\"\"\n",
    "        # Initialize the parent nn.Module\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        # Store the shared embedding network\n",
    "        self.embedding_network = embedding_network\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        \"\"\"\n",
    "        Defines the forward pass for training with image triplets.\n",
    "\n",
    "        Args:\n",
    "            anchor (torch.Tensor): The batch of anchor images.\n",
    "            positive (torch.Tensor): The batch of positive images (same class as anchor).\n",
    "            negative (torch.Tensor): The batch of negative images (different class from anchor).\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the embeddings for anchor, positive, and negative images.\n",
    "                   (anchor_output, positive_output, negative_output)\n",
    "        \"\"\"\n",
    "        # Process the anchor image through the embedding network\n",
    "        anchor_output = self.embedding_network(anchor)\n",
    "        # Process the positive image through the *same* embedding network (shared weights)\n",
    "        positive_output = self.embedding_network(positive)\n",
    "        # Process the negative image through the *same* embedding network (shared weights)\n",
    "        negative_output = self.embedding_network(negative)\n",
    "\n",
    "        # Return the generated embeddings\n",
    "        return anchor_output, positive_output, negative_output\n",
    "\n",
    "    def get_embedding(self, image):\n",
    "        \"\"\"\n",
    "        Generates an embedding for a single input image. Used for inference/retrieval.\n",
    "\n",
    "        Args:\n",
    "            image (torch.Tensor): The input image tensor (should include batch dimension).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The embedding vector for the image.\n",
    "        \"\"\"\n",
    "        # Pass the single image through the embedding network\n",
    "        return self.embedding_network(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ea8dff-8fb0-4117-9f8f-1d9c9ca677d5",
   "metadata": {},
   "source": [
    "* Instantiate the `SiameseNetwork`, passing the encoder you created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b561b549-eee8-4e2f-b5c8-b9dff4fc71f8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the Siamese Network\n",
    "siamese_network = SiameseNetwork(embedding_network=siamese_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e485efb-9219-4101-8389-93d22e207f01",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='2-4'></a>\n",
    "### 2.4 - Training the Siamese Network\n",
    "\n",
    "You have the data generator (`TripleDataset`) and the model architecture (`SiameseNetwork` wrapping the `SiameseEncoder`). Now, set up the final pieces needed for training:\n",
    "\n",
    "* **Loss Function**: You'll use `nn.TripletMarginLoss`. This loss function directly implements the core idea of Siamese training: it calculates the distances between the anchor positive pair ($d(A, P)$) and the anchor negative pair ($d(A, N)$) and penalizes the model if $d(A, P)$ is not smaller than $d(A, N)$ by at least a specified `margin`. The goal is $d(A, P) + \\text{margin} < d(A, N)$.\n",
    "\n",
    "* **Optimizer**: `torch.optim.AdamW` is a good choice for optimizing the network's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32862bb6",
   "metadata": {
    "deletable": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# EDITABLE CELL: Feel free to play around with different \"margin\" and \"lr\" values\n",
    "\n",
    "# Define the Triplet Margin Loss function\n",
    "# margin=1.0: Enforces that the negative sample should be at least 1.0 distance unit farther than the positive sample\n",
    "# p=2.0: Use Euclidean distance (L2 norm)\n",
    "loss_fcn = nn.TripletMarginLoss(margin=1.0, p=2.0)\n",
    "\n",
    "# Define the AdamW optimizer, passing the Siamese network's parameters and a learning rate\n",
    "optimizer = torch.optim.AdamW(siamese_network.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73df2880-88f6-49be-a61b-a103d35209f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, train the Siamese network using the `siamese_training_loop` helper function. This loop focuses solely on minimizing the `TripletMarginLoss`. Unlike classification, the goal here isn't to achieve a specific accuracy during training, but rather to organize the embedding space effectively by pulling similar items (anchor, positive) closer and pushing dissimilar items (anchor, negative) apart. The decreasing loss value is your main indicator of progress. The true test of the model's success will be its performance in the visual search task later. Train for a small number of epochs initially to verify the setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee274c0-caf6-47e8-b9b3-6fdc8256cb42",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# EDITABLE CELL:\n",
    "\n",
    "# Define the number of training epochs\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08ea4c9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Run the training loop for the Siamese network\n",
    "helper_utils.siamese_training_loop(\n",
    "    # The Siamese model instance\n",
    "    model=siamese_network,\n",
    "    # The DataLoader providing triplets\n",
    "    dataloader=siamese_dataloader,\n",
    "    # The TripletMarginLoss function\n",
    "    loss_fcn=loss_fcn,\n",
    "    # The AdamW optimizer\n",
    "    optimizer=optimizer,\n",
    "    # The compute device\n",
    "    device=device,\n",
    "    # The number of epochs to train\n",
    "    n_epochs=num_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226ac5e8",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<br>\n",
    "\n",
    "**Remark:** Training for longer is necessary to achieve good performance. For the classifier, consider at least 15 epochs, and for the Siamese retrieval system, aim for at least 10 epochs. The short training here is just to demonstrate the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b19ce8a",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a name='2-5'></a>\n",
    "### 2.5 - Performing Visual Search (Retrieval)\n",
    "\n",
    "With a trained Siamese network, you now have an encoder (`siamese_encoder`) capable of turning fashion images into meaningful embedding vectors. The final step is to use these embeddings to find similar items. This process typically involves:\n",
    "\n",
    "1.  **Selecting a Query Image:** Choose an image for which you want to find similar items.\n",
    "2.  **Generating Query Embedding:** Pass the query image through the trained `siamese_encoder` to get its embedding vector.\n",
    "3.  **Generating Catalog Embeddings:** Process all images in your product catalog (represented here by the `validation_dataset`) through the `siamese_encoder` to create an embedding for each item. In a real system, these would be pre calculated and stored in a database for fast lookup.\n",
    "4.  **Calculating Distances:** Compute the distance (e.g., Euclidean distance) between the query embedding and all catalog embeddings.\n",
    "5.  **Ranking:** Sort the catalog items based on their distance to the query, from smallest (most similar) to largest (least similar).\n",
    "6.  **Retrieving Top Results:** Select the top N items with the smallest distances.\n",
    "\n",
    "#### Selecting the Query Image\n",
    "\n",
    "You have two options for selecting the image you want to search for:\n",
    "\n",
    "1.  **Use a Provided Sample Image:** You can use one of the sample images available in the `./images/` directory. Here are the paths:\n",
    "    * `./images/dress.jpg`\n",
    "    * `./images/hat.jpg`\n",
    "    * `./images/longsleeve.jpg`\n",
    "    * `./images/pant.jpg`\n",
    "    * `./images/shoes.jpg`\n",
    "    * `./images/shorts.jpg`\n",
    "    * `./images/t_shirt.jpg`\n",
    ">\n",
    "2.  **Upload Your Own Image:** Use the widget below to upload a custom JPG image.\n",
    "    * Running `helper_utils.upload_jpg_widget()` displays an upload widget.\n",
    "    * You can only upload images with a `.jpg` extension.\n",
    "    * Each image file size must not exceed 5 MB.\n",
    "    * After successful upload, the widget will display the file path (e.g., `./uploads/your_image.jpg`). Copy this path.\n",
    "    * You can reuse the widget multiple times without rerunning the cell.\n",
    "\n",
    "* Run the cell below to display the image upload widget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08f564d-8b3f-4a38-bc0b-6fa007dae830",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display the widget for uploading JPG images\n",
    "helper_utils.upload_jpg_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982ffde6-68ec-4900-86df-42673ccab3d1",
   "metadata": {},
   "source": [
    "* Now, set the `image_path` variable below to the path of your chosen image (either one of the provided samples or the path displayed after uploading your own)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cdd421-cdc3-45a5-9ada-f5bf14f32670",
   "metadata": {
    "deletable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EDITABLE CELL: Set the path for your query image\n",
    "\n",
    "# Replace the example path below with the path to your desired query image\n",
    "# Example using a provided image: image_path = './images/hat.jpg'\n",
    "# Example using an uploaded image: image_path = './uploads/your_uploaded_image.jpg'\n",
    "\n",
    "image_path = './images/t_shirt.jpg' ### Add your image path here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceb3bc9-d22c-41b0-b68b-252ad8a1e6fa",
   "metadata": {},
   "source": [
    "* Load and display your selected query image using the `get_query_img` helper function to confirm your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f087611",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Load the selected image using the helper function\n",
    "query_img = helper_utils.get_query_img(image_path)\n",
    "# Display the query image\n",
    "print(\"Query Image:\")\n",
    "display(query_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53822d05",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Define a function `get_query_img_embedding` to process a single PIL image: apply the necessary transformations, pass it through the trained encoder, and return its embedding as a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e26b9b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def get_query_img_embedding(encoder, transform, img, device):\n",
    "    \"\"\"\n",
    "    Generates an embedding vector for a single query PIL image.\n",
    "\n",
    "    Args:\n",
    "        encoder (nn.Module): The trained embedding model (e.g., SiameseEncoder).\n",
    "        transform (callable): The torchvision transforms to apply (e.g., resize, normalize).\n",
    "        img (PIL.Image): The input query image.\n",
    "        device (torch.device): The device ('cuda' or 'cpu') to perform inference on.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The embedding vector as a NumPy array.\n",
    "    \"\"\"\n",
    "    # Apply the transformations (resize, ToTensor, normalize)\n",
    "    tensor_img = transform(img)\n",
    "\n",
    "    # Add a batch dimension (B, C, H, W) as the model expects batches\n",
    "    # and move the tensor to the specified device\n",
    "    query_img_tensor = tensor_img.unsqueeze(0).to(device)\n",
    "\n",
    "    # Set the encoder to evaluation mode (important for layers like BatchNorm, Dropout)\n",
    "    encoder.eval()\n",
    "    # Perform inference without calculating gradients to save memory and computation\n",
    "    with torch.no_grad():\n",
    "        # Pass the image tensor through the encoder model\n",
    "        query_img_embedding = encoder(query_img_tensor)\n",
    "\n",
    "    # Move the resulting embedding tensor from the device (e.g., GPU) back to the CPU\n",
    "    # and convert it into a NumPy array for easier handling (e.g., distance calculations)\n",
    "    query_img_embedding_np = query_img_embedding.cpu().numpy()\n",
    "    # Return the embedding as a NumPy array\n",
    "    return query_img_embedding_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81d068f-e76e-4d69-b47c-8f9e3d688000",
   "metadata": {},
   "source": [
    "* Generate the embedding for your `query_img` using the function you just defined and your trained `siamese_encoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b9e33d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Generate the embedding for the sample query image\n",
    "query_img_embedding = get_query_img_embedding(siamese_encoder, val_transform, query_img, device)\n",
    "\n",
    "# Print the shape of the resulting embedding vector (should be [1, embedding_dim])\n",
    "print(\"Shape of query image embedding:\", query_img_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6445ce",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Now, generate embeddings for all images in your \"catalog\" (using the `validation_dataset` here as a stand in for a full product catalog). The `get_embeddings` helper function efficiently processes the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35251b56",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Use the validation dataset as the \"catalog\" of items to search within\n",
    "catalog = validation_dataset\n",
    "\n",
    "# Use a helper function to efficiently generate embeddings for all items in the catalog\n",
    "print(\"Generating embeddings for the catalog...\")\n",
    "embeddings = helper_utils.get_embeddings(siamese_encoder, catalog, device)\n",
    "print(f\"Generated {len(embeddings)} embeddings for the catalog.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab8fcdd",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Finding and Displaying Similar Items\n",
    "\n",
    "* With embeddings generated for the query and the entire catalog, use the `find_closest` helper function to identify the indices of the `num_samples` most similar images in the catalog based on Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2980af16-173b-4c57-8fe3-0061dd37d7ff",
   "metadata": {
    "deletable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EDITABLE CELL: # Define how many similar items to retrieve\n",
    "\n",
    "num_samples = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d8a4d5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Use a helper function to find the indices of the items in the catalog\n",
    "# whose embeddings are closest (smallest Euclidean distance) to the query embedding\n",
    "print(f\"Finding the top {num_samples} closest items...\")\n",
    "closest_indices = helper_utils.find_closest(embeddings, query_img_embedding, num_samples)\n",
    "print(\"Indices of closest items:\", closest_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7364b23",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Finally, retrieve the actual images and their labels from the catalog using the `closest_indices` and display them. These are the top visual matches for your query image according to your trained Siamese network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cc27aa",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Loop through the indices of the closest images found\n",
    "print(f\"\\nDisplaying the {num_samples} most similar items found in the catalog:\")\n",
    "for idx_c in closest_indices:\n",
    "    # Retrieve the image and its true label from the catalog dataset using the index\n",
    "    # The helper function likely handles converting tensor back to displayable format\n",
    "    img_c, label_idx_c = helper_utils.get_image(catalog, idx_c) # Assuming get_image returns PIL + label index\n",
    "    label_c = catalog.classes[label_idx_c] # Get class name from index\n",
    "    # Print the class label of the retrieved image\n",
    "    print(f\"Retrieved Item - Class: {label_c} (Index: {idx_c})\")\n",
    "    # Display the retrieved image\n",
    "    display(img_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51829fd",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Fantastic work completing this assignment and concluding Module 1! You've successfully applied advanced PyTorch concepts to build a sophisticated, dual-purpose AI system for a realistic e-commerce scenario. By acting as an AI engineer, you've moved beyond simple sequential models and embraced the power of custom, modular architectures.\n",
    "\n",
    "You began by constructing an efficient classifier, mastering the implementation of the `InvertedResidualBlock` and assembling it into a `MobileNetBackbone`. Then, showcasing true engineering resourcefulness, you seamlessly repurposed that trained backbone to create a `SiameseEncoder`. You adeptly handled the shift to similarity learning by designing a `TripleDataset` and training a `SiameseNetwork` with `TripletMarginLoss`, effectively sculpting an embedding space where visual similarity translates to proximity. Finally, you brought it all together by implementing a practical visual search function, demonstrating how these embeddings can power features like product recommendations or \"shop the look\".\n",
    "\n",
    "This assignment solidified key principles from the module: the importance of **modular design** for flexibility and reuse, the ability to implement **custom blocks** and architectures beyond simple sequences, and the distinct approaches required for **classification versus similarity learning**. The skills you've honed here, designing efficient blocks, reusing components, and adapting models for different objectives, are directly applicable to building state-of-the-art AI systems in many professional domains. Well done on building this intelligent fashion engine!"
   ]
  }
 ],
 "metadata": {
  "grader_version": "1",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
