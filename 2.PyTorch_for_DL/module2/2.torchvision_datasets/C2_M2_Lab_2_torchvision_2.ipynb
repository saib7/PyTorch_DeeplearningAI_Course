{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b251884b-a63b-4b4a-ba45-93c93c354c86",
   "metadata": {},
   "source": [
    "# TorchVision Datasets\n",
    "\n",
    "Before building any computer vision model, and after defining the problem you are trying to solve, you must answer one important question: **what data will you train it on?** In deep learning, the quality and structure of your dataset are fundamental to your model's performance.\n",
    "\n",
    "Fortunately, **TorchVision** provides access to a rich collection of well-known, pre-formatted datasets, saving you the effort of writing data loading and preprocessing code from scratch. These datasets are designed to integrate seamlessly into a PyTorch training pipeline, giving you a reliable way to get high-quality data into your workflow.\n",
    "\n",
    "\n",
    "In this lab, you'll get hands-on experience with the `torchvision.datasets` module. You will learn to:\n",
    "\n",
    "* Load and inspect a standard built-in dataset like `CIFAR-10`, understanding its key initialization parameters.\n",
    "* Load datasets that have unique loading requirements, such as `EMNIST`.\n",
    "* Load your own images using the generic `ImageFolder` data loader.\n",
    "* Generate placeholder data for testing and debugging using `FakeData`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f1be10-a7f4-4575-8fff-ac5a75f585fc",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee667a17-fc83-4f96-b917-6f0fa0d8a9fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import helper_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c3489-649e-4b0d-94d8-d1d6536dedeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set dataset directory\n",
    "root_dir = './pytorch_datasets'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f145c54-c646-41d1-b92c-8e7acec3a66d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using Pre-built Datasets\n",
    "\n",
    "TorchVision offers a variety of popular, ready-to-use datasets that can be downloaded and used with just a few lines of code, which are perfect for benchmarking models suited for different computer vision tasks. Understanding how to effectively load and process data is the foundational first step for any computer vision project.\n",
    "\n",
    "**Available Datasets by Task**\n",
    "\n",
    "Below is a breakdown of **some** of the most common datasets available, categorized by the problem they are designed to solve.\n",
    "\n",
    "* **Image Classification**: Predict a single class label for an entire image.\n",
    "    * **Datasets**: [MNIST](https://docs.pytorch.org/vision/0.21/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST), [Fashion-MNIST](https://docs.pytorch.org/vision/0.21/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST), [EMNIST](https://docs.pytorch.org/vision/0.21/generated/torchvision.datasets.EMNIST.html#torchvision.datasets.EMNIST), [CIFAR-10](https://docs.pytorch.org/vision/0.21/generated/torchvision.datasets.CIFAR10.html#torchvision.datasets.CIFAR10), [CIFAR-100](https://docs.pytorch.org/vision/0.21/generated/torchvision.datasets.CIFAR100.html#torchvision.datasets.CIFAR100), [STL-10](https://docs.pytorch.org/vision/0.21/generated/torchvision.datasets.STL10.html#torchvision.datasets.STL10), [LSUN](https://docs.pytorch.org/vision/0.21/generated/torchvision.datasets.LSUN.html#torchvision.datasets.LSUN)\n",
    "* **Object Detection & Segmentation**: Object detection involves drawing bounding boxes around objects, while segmentation involves classifying each pixel in the image. Many datasets provide annotations for both.\n",
    "    * **Datasets**: Pascal VOC ([VOCDetection](https://docs.pytorch.org/vision/0.21/generated/torchvision.datasets.VOCDetection.html#torchvision.datasets.VOCDetection)/[VOCSegmentation](https://docs.pytorch.org/vision/0.21/generated/torchvision.datasets.VOCSegmentation.html#torchvision.datasets.VOCSegmentation)), COCO ([CocoDetection](https://docs.pytorch.org/vision/0.21/generated/torchvision.datasets.CocoDetection.html#torchvision.datasets.CocoDetection)/[CocoCaptions](https://docs.pytorch.org/vision/0.21/generated/torchvision.datasets.CocoCaptions.html#torchvision.datasets.CocoCaptions)), [Cityscapes](https://docs.pytorch.org/vision/0.21/generated/torchvision.datasets.Cityscapes.html#torchvision.datasets.Cityscapes), [CelebA](https://docs.pytorch.org/vision/0.21/generated/torchvision.datasets.CelebA.html#torchvision.datasets.CelebA)\n",
    "* **Video Classification**: Classify actions or events occurring in video clips.\n",
    "    * **Datasets**: [UCF-101](https://docs.pytorch.org/vision/0.21/generated/torchvision.datasets.UCF101.html#torchvision.datasets.UCF101), [Kinetics](https://docs.pytorch.org/vision/0.21/generated/torchvision.datasets.Kinetics.html#torchvision.datasets.Kinetics), [HMDB51](https://docs.pytorch.org/vision/0.21/generated/torchvision.datasets.HMDB51.html#torchvision.datasets.HMDB51)\n",
    "\n",
    "Now that you have an idea of the breadth of datasets TorchVision provides, let's dive into the practical details of using them. While the specifics can vary, most datasets share a common set of principles for initialization and data handling. To get started, we'll walk through one of the most frequently used benchmarks in computer vision: `CIFAR10`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8d1da3-197d-45c1-9782-6efb509c4324",
   "metadata": {},
   "source": [
    "### Standard Dataset Example: CIFAR10\n",
    "\n",
    "Throughout your work with PyTorch, you've been initializing PyTorch built-in datasets from `torchvision` using code that looks like this:\n",
    "\n",
    "```Python\n",
    "cifar_dataset = datasets.CIFAR10(root=root_dir, \n",
    "                                 train=True, \n",
    "                                 download=True,\n",
    "                                )\n",
    "```\n",
    "\n",
    "You pass in a few parameters, and everything just works.\n",
    "\n",
    "But what exactly is each of these initial parameters, or \"flags\" doing? Understanding them is the first step to mastering how you handle data in PyTorch. Let's break down this common example.\n",
    "\n",
    "* `root`: A string specifying the directory path where the dataset will be stored. If the data already exists at this path, PyTorch will use it.\n",
    "\n",
    "* `train`: A boolean that selects the data split. For `CIFAR-10`, `True` loads the 50,000 training images, while `False` loads the 10,000 test images.\n",
    "\n",
    "* `download`: A boolean that, if `True`, downloads the dataset from the internet if it's not found in your `root` directory. If `False`, an error will occur, **but only if the dataset isn't already present** at the `root` path.\n",
    "\n",
    "Let's see this in action.\n",
    "\n",
    "* Run the code cell below to initialize the [CIFAR10](https://docs.pytorch.org/vision/0.9/datasets.html#torchvision.datasets.CIFAR10) dataset.\n",
    "* You'll use the train split by setting `train=True` and allow the data to be downloaded by setting `download=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd83837-1e39-42d1-a843-ba897eb464ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the CIFAR-10 training dataset\n",
    "cifar_dataset = datasets.CIFAR10(\n",
    "    root=root_dir,      # Path to the directory where the data is/will be stored\n",
    "    train=True,         # Specify that you want the training split of the dataset\n",
    "    download=True       # Download the data if it's not found in the root directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff95121-d01a-4115-a197-dc8a3d4bf6e2",
   "metadata": {},
   "source": [
    "For a simple download using the flags you set, each item in the dataset is a tuple containing two things:\n",
    "\n",
    "* An **image** in the PIL format.\n",
    "* An **integer** representing the label for that image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9c70a4-5c80-4519-b73b-c088304e0804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the first sample (at index 0), which is a (image, label) tuple\n",
    "image, label = cifar_dataset[0]\n",
    "\n",
    "print(f\"Image Type:        {type(image)}\")\n",
    "# Since `image` a PIL Image object, its dimensions are accessed using the .size attribute.\n",
    "print(f\"Image Dimensions:  {image.size}\")\n",
    "print(f\"Label Type:        {type(label)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1bacda-f703-4f2b-a29e-710e7add00a1",
   "metadata": {},
   "source": [
    "#### Preparing the Data with Transformations\n",
    "\n",
    "As you're already familiar, PyTorch models require input data to be **tensors**, not the PIL Images. You also need transformations to normalize the data and augment it, which helps in training a robust model.\n",
    "\n",
    "After you've downloaded a dataset and perhaps done some initial analysis on it, you can prepare it for your model. You do this by assigning a transformation pipeline directly to the dataset's `.transform` attribute. This modifies the dataset in-place, so from that point on, any time you access an item, the transformation will be applied automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c79300-1a9e-41bb-b864-5c170093d24d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a transformations pipeline\n",
    "cifar_transformation = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # The mean and std values are standard for the CIFAR-10 dataset\n",
    "    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),\n",
    "                         std=(0.2023, 0.1994, 0.2010)\n",
    "                        )\n",
    "])\n",
    "\n",
    "# Assign the entire transformation pipeline to the dataset's .transform attribute\n",
    "cifar_dataset.transform = cifar_transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4775ebaf-2fb3-4253-8e2f-c5def58da952",
   "metadata": {},
   "source": [
    "* Notice how the transformation is applied automatically.\n",
    "    * The `image` is now a properly shaped PyTorch tensor, but the `label` is still an integer, as the transformation pipeline only targeted the `image`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7794b3ae-bdde-4f4f-bd3f-d5d2f9d38cd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Access the first item again\n",
    "image, label = cifar_dataset[0]\n",
    "\n",
    "print(f\"Image Type:                   {type(image)}\")\n",
    "# Since the `image` is now a PyTorch Tensor, its dimensions are accessed using the .shape attribute.\n",
    "print(f\"Image Shape After Transform:  {image.shape}\")\n",
    "print(f\"Label Type:                   {type(label)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2842238f-cd3f-43c5-8ad9-33decddb99d6",
   "metadata": {},
   "source": [
    "* Use a `DataLoader` to grab a small batch of your transformed images and display them in a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb94a145-f626-4d24-bdad-813fa86022f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the datalaoder\n",
    "cifar_dataloader = data.DataLoader(cifar_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc71f24f-ef60-4d8d-8226-30b5e82bc50f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "helper_utils.display_images(cifar_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcf380e-2e96-4de4-8d8c-46bb3a3ab3fd",
   "metadata": {},
   "source": [
    "#### The Direct Approach: Using the 'transform' Parameter\n",
    "\n",
    "The method you just saw, assigning to `.transform` after loading, is powerful and flexible, especially when you're exploring a new dataset.\n",
    "\n",
    "However, in most real-world projects, you already know which transformations you need right from the start. For this, there's a more direct and common approach. You can pass your transformation pipeline directly into the dataset's initializer using the `transform` parameter.\n",
    "\n",
    "```Python\n",
    "cifar_dataset = datasets.CIFAR10(...\n",
    "                                 transform=cifar_transformation\n",
    "                                )\n",
    "```\n",
    "\n",
    "This tells the dataset to apply these transformations from the moment it's created, all in one clean step.\n",
    "\n",
    "* Run the code cell below to initialize the `CIFAR10` dataset.\n",
    "* You'll use the test split data this time by setting `train=False`, and apply the transformation pipeline directly using the `transform` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c06bb27-fae9-4193-87cc-6fb6ea591cb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cifar_dataset = datasets.CIFAR10(root=root_dir, \n",
    "                                 train=False, \n",
    "                                 download=True,\n",
    "                                 transform=cifar_transformation\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96ee06e-019c-4617-89f0-dc0c15fb597c",
   "metadata": {},
   "source": [
    "* Use a `DataLoader` to grab a small batch of your newly loaded test images and display them in a grid.\n",
    "\n",
    "**Note**: Setting `shuffle=True` so that a random batch of images is displayed each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dd8cc5-3629-4e24-a92a-314bcdd10c5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cifar_dataloader = data.DataLoader(cifar_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06858a0b-2c4f-457e-8636-74265c69611b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "helper_utils.display_images(cifar_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedf1c05-5964-4d6f-97f9-7f30c9a146dc",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The entire process you just saw used the `CIFAR10` dataset. While the methods for loading data and applying transformations are more or less the same across `torchvision`, it's important to remember that **each dataset is unique**.\n",
    "\n",
    "Some datasets might have different parameters, while others might have fewer (for example, some datasets don't have a pre-defined `train`/`test` split). It's always a good habit to explore [torchvision datasets](https://docs.pytorch.org/vision/0.9/datasets.html) and read their official documentation to fully understand how to use each one correctly.\n",
    "\n",
    "With that in mind, let’s look at another dataset, `EMNIST`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7002c4e-ac2d-4ded-93c0-36002914e3c5",
   "metadata": {},
   "source": [
    "### Dataset with Special Parameters Example: EMNIST\n",
    "\n",
    "While many datasets share similar initialization parameters, some have unique requirements. The EMNIST dataset is one such example. Let's take a look.\n",
    "\n",
    "* First, define the transformation pipeline for the `EMNIST` dataset.\n",
    "    * Since raw images in this dataset are not oriented correctly for easy visualization, you'll **rotate** and **vertically flip** them.\n",
    "    * For normalization, you'll use `transforms.Normalize((0.5,), (0.5,))`. This will be explained in a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac535d2-d6c8-4821-bd79-b3878aa270c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the transformation pipeline\n",
    "emnist_transformation = transforms.Compose([\n",
    "    # 90-degree rotation, it randomly rotates between +90 degrees and +90 degrees\n",
    "    transforms.RandomRotation(degrees=(90, 90)),\n",
    "    # p=1.0 guarantees vertical flip\n",
    "    transforms.RandomVerticalFlip(p=1.0),\n",
    "    transforms.ToTensor(),\n",
    "    # Normalizes the tensor, rescaling pixels from [0, 1] to [-1, 1]\n",
    "    transforms.Normalize((0.5,), (0.5,)) # The mean and std must be in a tuple\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672a1567-709d-4805-a600-96b3ffdf2658",
   "metadata": {},
   "source": [
    "Unlike CIFAR-10, the `EMNIST` dataset class requires an additional parameter: `split`. This is because the EMNIST dataset is actually **a collection of six different datasets**, and you must specify which one you want to load.\n",
    "\n",
    "```python\n",
    "emnist_digits_dataset = datasets.EMNIST(root=root_dir,\n",
    "                                        split= ...,  # Need to specify a `split`\n",
    "                                        train=True,\n",
    "                                        download=True,\n",
    "                                        transform=emnist_transformation\n",
    "                                       )\n",
    "```\n",
    "\n",
    "The available splits are: `byclass`, `bymerge`, `balanced`, `letters`, `digits`, and `mnist`.\n",
    "\n",
    "Because each split contains different data, there is no single, standard way to normalize the entire EMNIST collection.\n",
    "\n",
    "For the `digits` split that you'll be using, `transforms.Normalize((0.5,), (0.5,))` is a standard and effective choice. The values must be in a tuple because the function expects a sequence of means and standard deviations for each image channel.\n",
    "\n",
    "* Run the code cell below to initialize the [EMNIST](https://docs.pytorch.org/vision/0.9/datasets.html#torchvision.datasets.EMNIST) dataset.\n",
    "* You'll specify the `digits` split to load only the handwritten digit images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b577ef-5b7c-4c40-b10d-8e0c83900b68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emnist_digits_dataset = datasets.EMNIST(root=root_dir,\n",
    "                                        split='digits',  # Specify the 'digits' split\n",
    "                                        train=False,\n",
    "                                        download=True,\n",
    "                                        transform=emnist_transformation\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42aadaf-63d5-4b1c-87e8-d1d037e4c479",
   "metadata": {},
   "source": [
    "* Use a `DataLoader` to grab a small batch of your loaded `EMNIST` images and display them in a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee190e4-f6c4-4121-b470-979324dca9cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emnist_digits_dataloader = data.DataLoader(emnist_digits_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843b9ffd-1911-47fe-8500-57b692dff7d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "helper_utils.display_images(emnist_digits_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6c711f-3f73-426a-b4c3-adb77569ff12",
   "metadata": {},
   "source": [
    "## Custom and Specialized Datasets\n",
    "\n",
    "Beyond pre-defined datasets, TorchVision provides powerful tools for working with your own images and generating test data.\n",
    "\n",
    "### Loading Custom Images: ImageFolder\n",
    "\n",
    "`ImageFolder` is a generic dataset loader that lets you use your own images without writing a custom `Dataset` class. It's incredibly useful, but it requires you to organize your images into a specific folder structure.\n",
    "\n",
    "The main requirement for `ImageFolder` is that your root directory must contain one sub-directory for each class. `ImageFolder` automatically uses the name of each sub-directory as the class label.\n",
    "\n",
    "For this example, you'll be using the following directory structure:\n",
    "\n",
    "```\n",
    "./tiny_fruit_and_vegetable/\n",
    "├── Apple__Healthy/\n",
    "│   ├── FreshApple (2).jpg\n",
    "│   └── ...\n",
    "├── Guava__Healthy/\n",
    "│   ├── FreshGuava (179).jpg\n",
    "│   └── ...\n",
    "├── Mango__Healthy/\n",
    "│   ├── 22.jpg\n",
    "│   └── ...\n",
    "├── Pomegranate__Healthy/\n",
    "│   ├── FreshPomegranate (3).jpg\n",
    "│   └── ...\n",
    "└── Strawberry__Healthy/\n",
    "    ├── 48.jpg\n",
    "    └── ...\n",
    "```\n",
    "\n",
    "When you point `ImageFolder` to the `./tiny_fruit_and_vegetable` directory, it will:\n",
    "\n",
    "* Automatically find **five classes** based on the sub-directory names: `Apple__Healthy`, `Guava__Healthy`, `Mango__Healthy`, `Pomegranate__Healthy`, and `Strawberry__Healthy`.\n",
    "* Assign an integer label to each class (e.g., `Apple__Healthy` --> `0`, `Guava__Healthy` --> `1`, and so on).\n",
    "* Assume that all images inside the `Apple__Healthy/` folder belong to class `0`, all images in `Guava__Healthy/` belong to class `1`, etc.\n",
    "\n",
    "**NOTE:** you can explore the folder on the left sidebar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54b0c7d-becc-44c1-8b3e-a44a178880ee",
   "metadata": {},
   "source": [
    "* Define the path for the `tiny_fruit_and_vegetable` directory of the dataset. `ImageFolder` will use this path to find the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edd9892-f288-4fe9-bf99-1dcb102195f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_dir = './tiny_fruit_and_vegetable'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a647fcfe-a427-42a2-a3b9-f252cfccfc2b",
   "metadata": {},
   "source": [
    "Just like the built-in datasets, `ImageFolder` allows you to pass a transformation pipeline to be applied to your images as they are loaded. For any deep learning task, this pipeline must **at least convert the images to tensors**. Furthermore, to enable batch processing with a `DataLoader`, it's also essential to **resize the images to a uniform size**.\n",
    "\n",
    "For this example, you'll also normalize the images. While it's best practice to use the *actual* mean and standard deviation of your specific dataset for optimal performance, using `0.5` for both is a common and effective starting point that scales your pixel values to the `[-1, 1]` range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf94078-b372-46f7-a8af-7b8a5cf4eecc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a transformation pipeline\n",
    "image_transformation = transforms.Compose([\n",
    "    transforms.Resize((100, 100)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize( \n",
    "        mean=[0.5, 0.5, 0.5],\n",
    "        std=[0.5, 0.5, 0.5]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a138bd4c-607c-40bd-b8a5-809766b0a692",
   "metadata": {},
   "source": [
    "* Run the code cell below to initialize the [ImageFolder](https://docs.pytorch.org/vision/main/generated/torchvision.datasets.ImageFolder.html) dataset.\n",
    "* You'll pass in arguments:\n",
    "    * `root`: The path to the root directory that contains your class subfolders.\n",
    "    * `transform`: An **optional** argument for the transformation pipeline to be applied. While you can pass `None`, for this case, you will use the pipeline you defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763bcf65-71d0-416c-8e3b-9229a614048e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fruit_dataset = datasets.ImageFolder(root=root_dir,\n",
    "                                     transform=image_transformation\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38aa526b-b2cc-4366-902b-dfd22bfba5ee",
   "metadata": {},
   "source": [
    "> #### Side Note:\n",
    ">\n",
    "> As you've seen, each dataset is unique. You might have noticed that unlike `CIFAR10` or `EMNIST`, you  didn't pass a `train=True` or `train=False` argument here. That's because `ImageFolder` works with your own custom directory of images, which doesn't have a predefined split.\n",
    ">\n",
    "> So, how do you create training and validation/testing sets when using `ImageFolder`? While there are several ways to approach this, two strategies are particularly common.\n",
    ">\n",
    "> **Strategy 1: Split the Dataset Object**\n",
    ">\n",
    "> The first strategy is to load the entire dataset from your root folder into a single `ImageFolder` object. You would then use a PyTorch function, like `torch.utils.data.random_split`, to programmatically divide that single dataset object into smaller training and testing datasets. This approach is very convenient if you plan to apply the exact same transformations to both splits.\n",
    ">\n",
    "> **Strategy 2: Pre-split the Directories**\n",
    ">\n",
    "> The second common strategy is to organize your file system into `train` and `test` subdirectories before you even run your code. Each of these directories would contain the class subfolders inside them. You would then create two separate `ImageFolder` datasets, one pointing to the \"train\" directory and the other to the \"test\" directory. This method is ideal when you need to apply different transformation pipelines to your training and testing data, such as including data augmentation only for the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbbdfd3-e707-4607-8b9a-2381b838fb6e",
   "metadata": {},
   "source": [
    "* Use a `DataLoader` to grab a small batch of your loaded `ImageFolder` images and display them in a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bab55b-f8fe-4de2-8a08-c63182cee559",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fruit_dataloader = data.DataLoader(fruit_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cb699c-180b-4b2e-907d-98fe561662dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "helper_utils.display_images(fruit_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5e077f-e7e1-4501-9981-d3da120d2d30",
   "metadata": {},
   "source": [
    "### Generating Synthetic Data: FakeData\n",
    "\n",
    "`FakeData` is a handy `torchvision` dataset that generates random images and labels on the fly. It's perfect for quickly testing a training loop, a data loading pipeline, or a model architecture without waiting for a real dataset to download. You can specify key properties like the number of images, their dimensions, and the number of classes.\n",
    "\n",
    "It doesn't fall under \"custom datasets\" in the traditional sense, as you aren't providing the data, but it's a specialized tool for development and debugging.\n",
    "\n",
    "* Run the code cell below to initialize the [FakeData](https://docs.pytorch.org/vision/main/generated/torchvision.datasets.FakeData.html#torchvision.datasets.FakeData) dataset.\n",
    "* You'll pass in arguments:\n",
    "    * `size`: The total number of fake samples you want in the dataset.\n",
    "    * `image_size`: A tuple defining the dimensions of the generated images in the format `(Channels, Height, Width)`.\n",
    "    * `num_classes`: The number of possible classes for the labels.\n",
    "    * `transform`: An **optional** transformation pipeline to apply to the images as they are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d09001-c1e9-4de6-b824-e77c8baaad2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a transformation pipeline\n",
    "fake_data_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Initialize the FakeData dataset\n",
    "fake_dataset = datasets.FakeData(\n",
    "    size=1000,                    # Total number of fake images\n",
    "    image_size=(3, 32, 32),       # (Channels, Height, Width)\n",
    "    num_classes=10,               # Number of possible classes\n",
    "    transform=fake_data_transform # Apply the transformation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4cc02d-9ca3-46d6-bd2e-000e6a788727",
   "metadata": {},
   "source": [
    "* Use a `DataLoader` to grab a small batch of your generated `FakeData` images and display them in a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fd6bb3-9fc9-4583-8abe-e7f9e9c56004",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fake_dataloader = data.DataLoader(fake_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6e3049-d1d5-4977-acdf-69696e68f9c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "helper_utils.display_images(fake_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be5cd9e-bbaf-4b52-a729-337e348c2b50",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Try Yourself: Practice Loading Datasets\n",
    "\n",
    "Now it's your turn to apply what you've learned. The following exercises will challenge you to load different datasets, making sure to use the correct transformations for each one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cee1459-34f6-478d-96ec-ca7f7407d332",
   "metadata": {},
   "source": [
    "### Exercise 1: FashionMNIST\n",
    "\n",
    "* Define a simple transformation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9bc6b7-5ac3-4472-9c8e-14d0c2353095",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a transformation pipeline\n",
    "grayscale_transformation = transforms.Compose([\n",
    "    transforms.Resize((100, 100)),\n",
    "    transforms.ToTensor(),\n",
    "    # Use a mean and std for a single channel\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,)) \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be25a99f-9885-435e-9e56-a98fa1ad8a2d",
   "metadata": {},
   "source": [
    "* Re-define the root directory back as `'./pytorch_datasets'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53537d0-0580-4b0c-b6d9-b3d9637ee807",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set dataset directory\n",
    "root_dir = './pytorch_datasets'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed80263-72bf-4f23-8c77-b02f218f5473",
   "metadata": {},
   "source": [
    "* Write the code below to initialize the [FashionMNIST](https://docs.pytorch.org/vision/main/generated/torchvision.datasets.FashionMNIST.html) **`Train`** dataset.\n",
    "* For `transform`, use the `grayscale_transformation` pipeline you defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69878e7-739f-4575-a845-daedccb1b657",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    fashion_mnist_dataset = datasets.FashionMNIST(\n",
    "        root=root_dir,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=grayscale_transformation\n",
    "    )\n",
    "    \n",
    "    print(\"\\033[92mDataset loaded successfully!\")\n",
    "    \n",
    "except:\n",
    "    print(\"\\033[91mSomething went wrong, try again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764e9c8d-5f97-46a8-a3a3-0f97ea25cc82",
   "metadata": {},
   "source": [
    "<br>\n",
    "<details>\n",
    "<summary><span style=\"color:green;\"><strong>Solution (Click here to expand)</strong></span></summary>\n",
    "\n",
    "```python\n",
    "fashion_mnist_dataset = datasets.FashionMNIST(root=root_dir, \n",
    "                                              train=True, \n",
    "                                              download=True,\n",
    "                                              transform=grayscale_transformation\n",
    "                                             )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63622fd5-2623-405c-95cb-819e0f0f7143",
   "metadata": {},
   "source": [
    "* To confirm that everything has loaded correctly, visualize a small batch of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cafcec-2212-4e7b-9155-bf9246a8955f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fashion_mnist_dataloader = data.DataLoader(fashion_mnist_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9bb125-22e0-44e6-a279-799633394288",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "helper_utils.display_images(fashion_mnist_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c98f11-11d4-4e2a-b155-0cd90a2b39cf",
   "metadata": {},
   "source": [
    "### Exercise 2: SVHN\n",
    "\n",
    "* Define a transformation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4a8e24-9370-4bb0-bd7b-d4a097d66026",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a 3-channel transformation\n",
    "svhn_transformation = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0524ef18-0db1-4614-ae90-9d7e10938c8b",
   "metadata": {},
   "source": [
    "* Write the code below to initialize the [SVHN](https://docs.pytorch.org/vision/main/generated/torchvision.datasets.SVHN.html) (Street View House Numbers) dataset.\n",
    "* Set the following arguments as:\n",
    "    * From `split`, get `'test'`.\n",
    "    * Use `svhn_transformation` for `transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2c4b44-7293-4480-82f4-f1f7cfea506d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    svhn_dataset = datasets.SVHN(\n",
    "        root=root_dir,\n",
    "        split='train',\n",
    "        download=True,\n",
    "        transform=svhn_transformation\n",
    "    )\n",
    "    \n",
    "    print(\"\\033[92mDataset loaded successfully!\")\n",
    "    \n",
    "except:\n",
    "    print(\"\\033[91mSomething went wrong, try again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5aeb84-325b-4405-88e1-fbf36a99b52d",
   "metadata": {},
   "source": [
    "<br>\n",
    "<details>\n",
    "<summary><span style=\"color:green;\"><strong>Solution (Click here to expand)</strong></span></summary>\n",
    "\n",
    "```python\n",
    "svhn_dataset = datasets.SVHN(root=root_dir,\n",
    "                             split='test',\n",
    "                             download=True,\n",
    "                             transform=svhn_transformation\n",
    "                            )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5bc93a-bb71-45b3-98ac-f959dc10cfe4",
   "metadata": {},
   "source": [
    "* To confirm that everything has loaded correctly, visualize a small batch of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e470a234-df6c-4f3d-a1a6-cf6fc9232750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "svhn_dataloader = data.DataLoader(svhn_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091736d0-498d-414d-802e-b3c63612f432",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "helper_utils.display_images(svhn_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c2e19e-4385-498a-bed7-7ee4e4af7000",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "As you've seen, TorchVision datasets are far more than just collections of images. They are organized, standardized, and designed to integrate with the entire PyTorch ecosystem, allowing you to focus on building great models rather than getting stuck on boilerplate code.\n",
    "\n",
    "You now have the foundational skills to load a wide variety of data, from popular benchmarks like **CIFAR-10** and **SVHN**, to your own custom image collections. By mastering the use of `torchvision.datasets`, you have a solid and efficient foundation to build, test, and scale your computer vision models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
