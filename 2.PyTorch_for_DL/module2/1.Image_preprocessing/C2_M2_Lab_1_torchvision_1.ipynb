{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe3471d-2087-4a5d-bc49-429514ac5aa7",
   "metadata": {},
   "source": [
    "# TorchVision for Pre-Processing\n",
    "\n",
    "Welcome to the first lab of this module! Before a deep learning model can learn to \"see\", the visual data you feed it must be carefully prepared. Raw images come in various sizes and formats, but neural networks require a standardized input, specifically, a **tensor**.\n",
    "\n",
    "This is where **TorchVision** comes in. As PyTorch’s standard toolkit for computer vision, it provides powerful and efficient tools designed to handle the common, often tedious, components of a vision workload. Instead of reinventing the wheel, you can use TorchVision’s battle tested data pipelines and functions to focus on building innovative models.\n",
    "\n",
    "In this lab, you will:\n",
    "\n",
    "* Practice converting images between the common **Pillow (PIL)** image format and **PyTorch Tensors**.\n",
    "* Explore handy **TorchVision utilities** like `make_grid` and `save_image` to simplify debugging and visualize batches of images.\n",
    "* Apply individual transformations to resize, crop, and augment images, and see their effects.\n",
    "* Define and implement a **custom transformation** from scratch to add a unique effect on the dataset.\n",
    "* Chain transforms together using `transforms.Compose` to build powerful and reusable **preprocessing and data augmentation pipelines**.\n",
    "\n",
    "By the end of this lab, you'll have a solid understanding of how to build a complete image preparation workflow from the ground up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4907eb78-1a5a-4aed-93aa-077ed55ecd57",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce55862f-d978-4c04-ae20-0f27b6d8bcae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from IPython.display import Image as DisplayImage\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torchvision import datasets\n",
    "from torchvision.io import decode_image\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import helper_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7883998-4b81-4571-a5d4-f0ff6bc62a70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if the OxfordIIITPet data folder exists\n",
    "ox3_pet_data_path = './oxford3pet_data'\n",
    "if os.path.exists(ox3_pet_data_path) and os.path.isdir(ox3_pet_data_path):\n",
    "    ox3_pet_download = False  # Data folder exists, will be loaded from\n",
    "else:\n",
    "    ox3_pet_download = True  # Data folder doesn't exist, will be downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320a7e6b-f501-47ac-bc52-9d21e2364be7",
   "metadata": {},
   "source": [
    "## Image Conversion (PIL and Tensor)\n",
    "\n",
    "Your first practical step in any computer vision workflow is to bridge the gap between how humans see images and how machines process them. An image file is a grid of pixels, often handled by a library like Pillow (PIL). A neural network, however, requires a numerical format it can perform calculations on: a **Tensor**.\n",
    "\n",
    "Gaining fluency in converting between these two formats is a foundational skill. It's the mechanism that lets you load, process, and inspect your data at every stage of a project.\n",
    "\n",
    "To begin, you will perform a \"roundtrip\" conversion. By taking a PIL image, changing it to a PyTorch Tensor, and then converting it back, you'll confirm that this core operation is seamless and preserves your data perfectly. This is a vital sanity check before building more complex pipelines.\n",
    "\n",
    "* Load an image with Pillow from a file using the Pillow (PIL) library.\n",
    "    * Note that Pillow reports dimensions in `(Width, Height)` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282e8e3e-9da6-4935-aca9-f86114b367bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load an image\n",
    "image = Image.open('./images/mangoes.jpg')\n",
    "\n",
    "# Dimensions of the original PIL image\n",
    "print(\"Original PIL Image Dimensions:\", image.size)\n",
    "print(f\"The maximum pixel value is: {image.getextrema()[0][1]}, and the minimum is: {image.getextrema()[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5702a900-63f3-4884-9b62-217a2be676a5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* `transforms.ToTensor()`: Converts a PIL image object into a PyTorch Tensor.\n",
    "    * **Dimension Change**: This transform rearranges the image data from Pillow's `(Width, Height)` format to PyTorch's `(Channels, Height, Width)` format.  It also scales the image's pixel values from the `[0, 255]` range to a floating point `[0.0, 1.0]` range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca9376d-26f7-4222-9993-867c6dc1b2c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the PIL image to a PyTorch Tensor\n",
    "img_tensor = transforms.ToTensor()(image)\n",
    "\n",
    "# Dimensions (shape) of the tensor\n",
    "# [C, H, W] format\n",
    "print(f\"Dimensions After Converting to a Tensor: {img_tensor.shape}\")\n",
    "print(f\"The maximum pixel value is: {img_tensor.max()}, and the minimum is: {img_tensor.min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1629e246-75a8-439a-814d-6a0f2b1ea65d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* `transforms.ToPILImage()`: Converts a PyTorch Tensor back into a PIL image object.\n",
    "    * **Dimension Change**: It performs the reverse, converting a `(Channels, Height, Width)` tensor back into a PIL image that reports its size as `(Width, Height)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68219f31-4c3b-4715-bde3-86ddbbd5e88a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the tensor back to a PIL image\n",
    "img_pil = transforms.ToPILImage()(img_tensor)\n",
    "\n",
    "# Dimensions of the converted back PIL image\n",
    "print(\"Dimensions After Converting Back to PIL:\", img_pil.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed9e991-a7c4-4231-8b17-7323ae0d9a2b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* Display the original and the converted images side-by-side to visually confirm that the \"roundtrip\" process preserved the image content perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a4b7fb-d9c9-4230-b75b-f82de3b69240",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the original and converted images\n",
    "helper_utils.show_images([image, img_pil], titles=(\"Original Image\", \"After PIL→Tensor→PIL conversion\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e5aca0-406d-43ae-9bf2-472d78ab1c6d",
   "metadata": {},
   "source": [
    "## TorchVision Utilities for Image Handling\n",
    "\n",
    "TorchVision equips you with a powerful toolkit for the practical logistics of a computer vision project. These utilities are designed to manage the entire lifecycle of your image data, from initial loading to final output. Mastering them allows you to efficiently load data, debug your pipeline by visualizing what your model is seeing, and save your results professionally.\n",
    "\n",
    "You will now explore three indispensable functions that address these core needs:\n",
    "\n",
    "* `decode_image`: Instantly converts compressed image files like JPEGs or PNGs directly into tensors.\n",
    "\n",
    "* `make_grid`: Arranges a batch of images into a clean, single grid, which is perfect for at-a-glance inspection and analysis.\n",
    "\n",
    "* `save_image`: Saves your tensor-based images back to a standard file format, making it easy to share results for reports or presentations.\n",
    "\n",
    "### Decoding Images into Tensors with `decode_image`\n",
    "\n",
    "The `decode_image` function is designed for one primary purpose: to efficiently convert an image file directly into a **numerical PyTorch tensor** for computation. It reads an image and immediately returns a tensor in the `[Channels, Height, Width]` format, ready for subsequent processing. Unlike `Image.open`, which returns a visual PIL object for immediate display or editing, the tensor from `decode_image` is a purely numerical object. This makes it the ideal choice when your workflow begins with computation, not visualization.\n",
    "\n",
    "* <code>[decode_image()](https://docs.pytorch.org/vision/stable/generated/torchvision.io.decode_image.html)</code>: Loads an image (e.g., JPEG, PNG) and converts it into a `torch.uint8` PyTorch Tensor.\n",
    "    * **Dimension Ordering**: The output tensor follows the standard PyTorch **channel-first convention** (`[C, H, W]`), which important for model compatibility. 'C' is the number of channels (e.g., 3 for an RGB image), 'H' is height, and 'W' is width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9b2932-bd84-4440-a984-a3e70589b06c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the path to the image file.\n",
    "image_path = './images/apples.jpg'\n",
    "\n",
    "# Load the image\n",
    "image = decode_image(image_path)\n",
    "\n",
    "print(f\"Image tensor dimensions: {image.shape}\")\n",
    "print(f\"Image tensor dtype: {image.dtype}\")\n",
    "print(f\"The maximum pixel value is: {image.max()}, and the minimum is: {image.min()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5da3e44-520a-4a66-84a6-4c051ea8e72b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the DisplayImage to render the image\n",
    "DisplayImage(image_path, width=500, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a63cc75-e98b-4acc-9be7-6babc748c030",
   "metadata": {},
   "source": [
    "### Creating Image Grids with `make_grid`\n",
    "\n",
    "In deep learning, you almost always process data in **batches**, not single images. But how do you get a quick, holistic view of an entire batch at once? Displaying them individually is inefficient.\n",
    "\n",
    "The `make_grid` function is the professional solution to this common challenge. It’s an essential utility that takes a batch of image tensors and arranges them into a single, clean grid for easy inspection. This is a vital step for visual debugging, allowing you to instantly check the results of your data augmentation or see exactly what a `DataLoader` is feeding to your model.\n",
    "\n",
    "* Load a batch of images from a local folder (`\"./images/\"`).\n",
    "    * The images are loaded as a single tensor containing all the image data, which is what the `make_grid` function expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6da3bf-f20e-4f95-b9bc-88519ea3fc08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a batch of images (./images/ contains only 6 images). The images are loaded as 300x300 pixels\n",
    "images_tensor = helper_utils.load_images(\"./images/\")\n",
    "\n",
    "# The size is 6 images x 3 color channels x 300 pixels height x 300 pixels width\n",
    "print(f\"Image tensor dimensions: {images_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb765bdb-0c32-4c3d-99f0-2585ad45f349",
   "metadata": {},
   "source": [
    "* Arrange the loaded images into a grid using the <code>[make_grid()](https://docs.pytorch.org/vision/main/generated/torchvision.utils.make_grid.html)</code> function.\n",
    "    * `tensor:` This needs to be a batch of images, as a **4D tensor** (`(B x C x H x W)`), to be placed into the grid.\n",
    "    * `nrow=3`: Arranges the images with **3 images in each row**.\n",
    "    * `padding=5`: Adds **5 pixels** of space between the images.\n",
    "    * `normalize=True`: Shifts the image pixel values to a standard range (0 to 1) for consistent display.\n",
    "\n",
    "Feel free to modify the parameters, and observe how it affects the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eb29f9-97e5-4819-b247-67b564ff06be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make a grid from the loaded images (2 rows of 3 for 6 images)\n",
    "grid = vutils.make_grid(tensor=images_tensor, nrow=3, padding=5, normalize=True)\n",
    "\n",
    "# the shape comes from \n",
    "# num_images/nrow*pixel_height+(num_images/nrow+1)*padding = 2*300+3*5 = 615\n",
    "# nrow*pixel_width+(nrow+1)*padding = 3*300+4*5 = 920\n",
    "print(f\"Image tensor dimensions: {grid.shape}\")\n",
    "print(f\"The maximum pixel value is: {grid.max()}, and the minimum is: {grid.min()}\\n\")\n",
    "\n",
    "# Display the grid of images using a helper function\n",
    "helper_utils.display_grid(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79febfa5-5fb6-4560-bf26-f81e188ecc7a",
   "metadata": {},
   "source": [
    "### Saving Tensors as Images with `save_image`\n",
    "\n",
    "Your work inside a PyTorch environment, whether it's a data grid you've created for debugging or an image generated by a model, exists as a tensor. To make this work tangible and shareable for reports, presentations, or future use, you need to export it back into a standard image file.\n",
    "\n",
    "The `save_image` function is the straightforward solution for this final step. It takes your tensor and efficiently saves it as a high-quality image file, such as a PNG or JPG, completing the \"roundtrip\" from file to tensor and back again.\n",
    "\n",
    "* <code>[save_image()](https://docs.pytorch.org/vision/main/generated/torchvision.utils.save_image.html)</code>: Saves the image tensor to a file.\n",
    "    * `tensor`: The tensor of image(s) to be saved.\n",
    "    * `fp`: The file path, with the name and format for the output image.\n",
    "    \n",
    "You can observe the image file in the left sidebar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52870a7-1e9d-4cb6-be2c-2b09f73210d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the path to save the image file.\n",
    "image_path = \"./fruits_grid.png\"\n",
    "\n",
    "# Save the grid as a PNG image\n",
    "vutils.save_image(tensor=grid, fp=image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44178f65-0bad-4d5a-9945-236c11fe22c0",
   "metadata": {},
   "source": [
    "* Once the tensor has been saved to a file using `save_image`, it can be treated like any regular image and displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce9ee19-5838-4bc9-bd0e-0d697a063f77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the DisplayImage to render the image\n",
    "DisplayImage(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56dcbb2-405f-470e-a879-c72fe543ac3d",
   "metadata": {},
   "source": [
    "## Image Transformations and Data Augmentation\n",
    "\n",
    "Image transformations are a fundamental part of preparing data for a neural network. You use them not only to standardize the size and format of images but also to perform **data augmentation**. Augmentation artificially increases the diversity of your training data by creating modified versions of existing images. This helps the model become more robust and generalize better to new, unseen data.\n",
    "\n",
    "The key thing to remember is that the **order in which you apply these transformations matters**. A common and effective practice is to first apply geometric transformations (like resizing and cropping), then color and other augmentations, and finally, convert the image to a tensor and normalize it. This ensures that augmentations are applied consistently and that the final data is in the correct format for the model.\n",
    "\n",
    "### A Closer Look at Individual Transformations\n",
    "\n",
    "Before you combine transformations into a powerful pipeline, examine some of the most common ones individually. Understanding the specific effect of each transform is important for building an effective data augmentation strategy.\n",
    "\n",
    "As you explore these different transformation techniques, you're encouraged to experiment with various configurations to observe their distinct outputs and how they might impact your model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d80370-6911-4a1b-a4d0-d58631e64998",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_image = Image.open('./images/strawberries.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0732cf-fbf2-4f4e-a81b-73277f78ec69",
   "metadata": {},
   "source": [
    "#### Resize\n",
    "\n",
    "The [Resize](https://docs.pytorch.org/vision/main/generated/torchvision.transforms.Resize.html) transform is a common preprocessing step to ensure all images in a batch have the same dimensions.\n",
    "\n",
    "* It rescales an input PIL image to a desired `size`.\n",
    "    * `size`: The target output size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2638bdbe-189b-4a2e-b732-37341bafe0eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the resize transformation (50x50 square)\n",
    "resize_transform = transforms.Resize(size=50)\n",
    "\n",
    "# Apply the transformation\n",
    "resized_image = resize_transform(original_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65912191-0279-416a-8ea5-8754359efcd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (Width, Height)\n",
    "print(f\"Original Dimensions: {original_image.size}\")\n",
    "print(f\"Resized Dimensions:  {resized_image.size}\\n\")\n",
    "\n",
    "helper_utils.show_images(\n",
    "    images=[original_image, resized_image], \n",
    "    titles=(\"Original\", \"Resized to (50, 50)\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba06b5a4-b241-483b-bb6f-3a4290936c7a",
   "metadata": {},
   "source": [
    "#### CenterCrop\n",
    "\n",
    "The [CenterCrop](https://docs.pytorch.org/vision/main/generated/torchvision.transforms.CenterCrop.html) transform is used to focus on the central part of an image, removing potentially distracting background from the edges.\n",
    "\n",
    "* It extracts a square patch from the center of the image.\n",
    "    * `size`: An integer defining the height and width of the desired crop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2b1d8c-ecd7-4102-a99c-5925d963bb84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the center crop transformation (256x256)\n",
    "center_crop_transform = transforms.CenterCrop(size=256)\n",
    "\n",
    "# Apply the transformation\n",
    "cropped_image = center_crop_transform(original_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319f28a0-b9a5-4605-b2cf-99aa8585803e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (Width, Height)\n",
    "print(f\"Original Dimensions: {original_image.size}\")\n",
    "print(f\"Cropped Dimensions:  {cropped_image.size}\\n\")\n",
    "\n",
    "helper_utils.show_images(\n",
    "    images=[original_image, cropped_image],\n",
    "    titles=(\"Original\", \"Center Crop (256, 256)\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f7358b-d54e-4f22-88a1-cbb6bbef6c0d",
   "metadata": {},
   "source": [
    "#### RandomResizedCrop\n",
    "\n",
    "The [RandomResizedCrop](https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomResizedCrop.html) transform is a data augmentation technique that randomly crops a portion of the image and then resizes it to a given size. This helps the model become more robust to variations in object scale and position within the image. Because it involves random selection, each time you apply this transform, you'll likely get a slightly different cropped and resized version of the image.\n",
    "\n",
    "* It crops a random portion of an image and resizes it to a specified size.\n",
    "    * `size`: The target output size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6efca60-621a-422b-8d8f-e59e9f0cfe17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the RandomResizedCrop transformation (224x224)\n",
    "random_resized_crop_transform = transforms.RandomResizedCrop(size=224)\n",
    "\n",
    "# Apply the transformation\n",
    "cropped_resized_image_1 = random_resized_crop_transform(original_image)\n",
    "cropped_resized_image_2 = random_resized_crop_transform(original_image)\n",
    "cropped_resized_image_3 = random_resized_crop_transform(original_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce284642-ee78-452d-b2a2-fa34c9178d15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (Width, Height)\n",
    "print(f\"Original Dimensions: {original_image.size}\")\n",
    "print(f\"RandomResizedCrop 1 Dimensions:  {cropped_resized_image_1.size}\")\n",
    "print(f\"RandomResizedCrop 2 Dimensions:  {cropped_resized_image_2.size}\")\n",
    "print(f\"RandomResizedCrop 3 Dimensions:  {cropped_resized_image_3.size}\\n\")\n",
    "\n",
    "helper_utils.show_images(\n",
    "    images=[original_image, cropped_resized_image_1],\n",
    "    titles=(\"Original (2048, 2048)\", \"RandomResizedCrop 1 (224, 224)\")\n",
    ")\n",
    "helper_utils.show_images(\n",
    "    images=[cropped_resized_image_2, cropped_resized_image_3],\n",
    "    titles=(\"RandomResizedCrop 2 (224, 224)\", \"RandomResizedCrop 3 (224, 224)\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3168050-9ac8-482d-9aff-1ebf69ef9478",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### RandomHorizontalFlip\n",
    "\n",
    "[RandomHorizontalFlip](https://docs.pytorch.org/vision/main/generated/torchvision.transforms.RandomHorizontalFlip.html) is a data augmentation technique that randomly flips the image horizontally. It teaches the model that an object's identity doesn't change if it's mirrored.\n",
    "\n",
    "* It flips the image horizontally with a given probability.\n",
    "    * `p`: The probability of the flip being applied. The default is 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5435bc-8cf2-48da-a6b8-2d8682412a66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the horizontal flip transformation\n",
    "# Set p=1.0 to guarantee the flip happens for this demonstration\n",
    "flip_transform = transforms.RandomHorizontalFlip(p=1.0)\n",
    "\n",
    "# Apply the transformation\n",
    "flipped_image = flip_transform(original_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866c9787-9c03-4145-bbfe-4987f2023695",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "helper_utils.show_images(\n",
    "    images=[original_image, flipped_image],\n",
    "    titles=(\"Original\", \"RandomHorizontalFlip (p=1.0)\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e11716-dd6b-4437-b3d2-e1153c55953a",
   "metadata": {},
   "source": [
    "#### ColorJitter\n",
    "\n",
    "[ColorJitter](https://docs.pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html), a data augmentation technique, makes the model more robust to variations in lighting and color by randomly altering the image's color properties.\n",
    "\n",
    "* It randomly changes the brightness, contrast, and saturation of an image.\n",
    "    * The `brightness`, `contrast` and `saturation` parameters control the range of the random adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49a9c4e-312c-42a8-a418-31d984ce4cd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the ColorJitter transformation\n",
    "# The values determine the random range for each property.\n",
    "jitter_transform = transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5)\n",
    "\n",
    "# Apply the transformation\n",
    "jittered_image = jitter_transform(original_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5450f8b4-4db6-42e7-a35e-dd9c3846db13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "helper_utils.show_images(\n",
    "    images=[original_image, jittered_image],\n",
    "    titles=(\"Original\", \"ColorJitter\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d3a57f-02a1-4036-9799-29fa90cec0e2",
   "metadata": {},
   "source": [
    "#### Custom Transformations\n",
    "\n",
    "In some cases, you may need a specific operation that isn't built-in, like simulating a particular type of camera noise. In these situations, you can create your own custom transformations.\n",
    "\n",
    "* A custom transform is created by defining a Python class with a `__call__` method.\n",
    "    * This method takes a PIL image as input and returns the modified image, allowing it to be seamlessly integrated into a `transforms.Compose` pipeline.\n",
    "* Define a transform to apply salt and pepper noise, which adds random white (255) and black (0) pixels to an image. This particular transformation is considered a data augmentation technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db20d51-8f90-41ff-85e4-f3c98561b037",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SaltAndPepperNoise:\n",
    "    \"\"\"\n",
    "    A custom transform to add salt and pepper noise to a PIL image.\n",
    "\n",
    "    Args:\n",
    "        salt_vs_pepper (float): The ratio of salt to pepper noise.\n",
    "                                (e.g., 0.5 is an equal amount of each).\n",
    "        amount (float): The total proportion of pixels to be affected by noise.\n",
    "    \"\"\"\n",
    "    def __init__(self, salt_vs_pepper=0.5, amount=0.04):\n",
    "        self.s_vs_p = salt_vs_pepper\n",
    "        self.amount = amount\n",
    "\n",
    "    def __call__(self, image):\n",
    "        # Make a copy of the image\n",
    "        output = np.copy(np.array(image))\n",
    "\n",
    "        # Add Salt Noise\n",
    "        num_salt = np.ceil(self.amount * image.size[0] * image.size[1] * self.s_vs_p)\n",
    "        # Generate random coordinates for salt noise\n",
    "        coords = [np.random.randint(0, i - 1, int(num_salt)) for i in image.size]\n",
    "        # Set pixels to white\n",
    "        output[coords[1], coords[0]] = 255  \n",
    "\n",
    "        # Add Pepper Noise\n",
    "        num_pepper = np.ceil(self.amount * image.size[0] * image.size[1] * (1.0 - self.s_vs_p))\n",
    "        # Generate random coordinates for pepper noise\n",
    "        coords = [np.random.randint(0, i - 1, int(num_pepper)) for i in image.size]\n",
    "        # Set pixels to black\n",
    "        output[coords[1], coords[0]] = 0\n",
    "\n",
    "        # Convert the NumPy array back to a PIL image\n",
    "        return Image.fromarray(output)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + f'(salt_vs_pepper={self.s_vs_p}, amount={self.amount})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f70e5f-e04c-4851-8947-14a259030677",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate your custom transformation\n",
    "sp_transform = SaltAndPepperNoise(salt_vs_pepper=0.5, amount=0.5)\n",
    "\n",
    "# Apply the transformation\n",
    "sp_image = sp_transform(original_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57403fd2-54b0-40f8-9284-179de2a669a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "helper_utils.show_images(\n",
    "    images=[original_image, sp_image],\n",
    "    titles=(\"Original\", \"With Salt & Pepper Noise\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671baac4-142d-4cc2-b736-28b807833722",
   "metadata": {},
   "source": [
    "#### Normalize\n",
    "\n",
    "[Normalize](https://docs.pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html?highlight=normalize) is a preprocessing step that standardizes the pixel values of an image. It subtracts the mean and divides by the standard deviation for each channel. This helps the model converge faster during training.\n",
    "\n",
    "* It operates on an image tensor, subtracting the mean and dividing by the standard deviation for each channel.\n",
    "    * `mean`: A sequence of mean values for each channel.\n",
    "    * `std`: A sequence of standard deviation values for each channel.\n",
    "\n",
    "**Note**: `transforms.ToTensor()` must always be applied before this transformation, as it operates on tensors, not PIL images.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1558008-c25f-45db-891a-4cecebf3d75b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert to tensor (scales to [0, 1])\n",
    "tensor_image = transforms.ToTensor()(original_image)\n",
    "\n",
    "# Define the normalization transform using ImageNet stats\n",
    "normalize_transform = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "# Apply the transformation\n",
    "normalized_tensor = normalize_transform(tensor_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155348f3-b685-42e5-87fc-13a504a26468",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the distribution before and after normalization\n",
    "helper_utils.plot_histogram(tensor_image, normalized_tensor, \"Comparison of Pixel Distribution Before and After Normalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628c660e-8e96-4113-affd-a732b499152f",
   "metadata": {},
   "source": [
    "##### Calculating Dataset Mean and Standard Deviation\n",
    "\n",
    "When applying the normalization transformation, you used the well known mean and standard deviation from the ImageNet dataset:\n",
    "\n",
    "> `mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]`\n",
    "\n",
    "Using the well-known mean and standard deviation from a large dataset like ImageNet is a common and highly effective practice, especially when fine-tuning a pre-trained model. This approach aligns your new data with the statistical properties the model's weights were originally calibrated to, ensuring a stable and reliable starting point for training.\n",
    "\n",
    "However, for optimal results, particularly when training a model from scratch, it is best to normalize data using its own specific statistics. Every dataset has a unique distribution of colors and brightness, and calculating its precise mean and standard deviation ensures the most accurate normalization possible, which can lead to improved model performance.\n",
    "\n",
    "Think of these as strong recommendations, not strict rules. While using ImageNet stats is the safest approach for fine-tuning, you can use your own dataset's stats. Doing so may require more extensive training for the model to adapt to the new data distribution. For training from scratch, on the other hand, calculating your own stats is essential.\n",
    "\n",
    "* The function, `calculate_mean_std`, iterates through the entire dataset to accumulate the sum and the sum-of-squares of the pixel values for each channel.\n",
    "    * These accumulated values are then used to compute the final mean and standard deviation across all images in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4db5b8f-049f-4af8-99f0-c00c76b02975",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_mean_std(dataset):\n",
    "    \"\"\"\n",
    "    Calculates the mean and standard deviation of a PyTorch dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (torch.utils.data.Dataset): The dataset for which to\n",
    "                                            calculate the stats. It should\n",
    "                                            return image tensors.\n",
    "\n",
    "    Returns:\n",
    "        (torch.Tensor, torch.Tensor): A tuple containing the mean and\n",
    "                                      standard deviation tensors, each of\n",
    "                                      shape (C,).\n",
    "    \"\"\"\n",
    "    # Create a DataLoader to iterate through the dataset in batches for efficiency.\n",
    "    # shuffle=False because the order of images doesn't matter for this calculation.\n",
    "    loader = data.DataLoader(dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Initialize tensors to store the sum of pixel values for each (RGB) channel.\n",
    "    channel_sum = torch.zeros(3)\n",
    "    # Initialize tensors to store the sum of squared pixel values for each channel.\n",
    "    channel_sum_sq = torch.zeros(3)\n",
    "    # Initialize a counter for the total number of pixels.\n",
    "    num_pixels = 0\n",
    "\n",
    "    # Wrap the loader with tqdm to create a progress bar for monitoring.\n",
    "    for images, _ in tqdm(loader, desc=\"Calculating Dataset Stats\"):\n",
    "        # Add the total number of pixels in this batch to the running total.\n",
    "        num_pixels += images.size(0) * images.size(2) * images.size(3)\n",
    "        \n",
    "        # Sum the pixel values across the batch, height, and width dimensions,\n",
    "        # leaving only the channel dimension. Add this to the running total.\n",
    "        channel_sum += images.sum(dim=[0, 2, 3])\n",
    "        \n",
    "        # Square each pixel value, then sum them up similarly to the step above.\n",
    "        channel_sum_sq += (images ** 2).sum(dim=[0, 2, 3])\n",
    "\n",
    "    # Calculate the mean for each channel.\n",
    "    mean = channel_sum / num_pixels\n",
    "    # Calculate the standard deviation using the formula: sqrt(E[X^2] - E[X]^2)\n",
    "    std = (channel_sum_sq / num_pixels - mean ** 2) ** 0.5\n",
    "\n",
    "    # Return the calculated mean and standard deviation.\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217cd620-7a94-4e61-810e-5ec9cba35dcf",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Time to see the `calculate_mean_std function` in action. You'll now compute the specific statistics for the **OxfordIIITPet dataset**, and you'll use these calculated values in your final augmentation pipeline.\n",
    "\n",
    "But before you can calculate the stats, you need to define a simple transformation pipeline.\n",
    "\n",
    "**Why is `simple_transform` needed?**\n",
    "\n",
    "You might wonder why you need to transform the images at all just to calculate their mean. Here's why this is important: \n",
    "\n",
    "* **`transforms.Resize((100, 100))`**: This performs two key functions: \n",
    "    * **Standardization**: It ensures every image has the exact same dimensions. This is important for a fair calculation, as it prevents larger or smaller images from skewing the overall result.\n",
    "    * **Efficiency**: You deliberately use a *small* size (`(100, 100)`) as a practical optimization. Processing `100x100` images (10,000 pixels) is much faster than using a larger size like `224x224` (50,176 pixels), and the resulting stats are still an excellent approximation.\n",
    "* **`transforms.ToTensor()`**: This is essential because your calculation function operates on numerical tensors, not PIL images. This transform converts the images into the required format and scales their pixel values to the `[0.0, 1.0]` range, which is standard for these types of computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca79503-e164-4063-b3c7-2e25493fa9c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a simple transformation\n",
    "simple_transform = transforms.Compose([\n",
    "    transforms.Resize((100, 100)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load the OxfordIIITPet dataset, applying the simple transform to each image\n",
    "my_dataset = datasets.OxfordIIITPet(root=ox3_pet_data_path,\n",
    "                                    split='test',                 # Specify using the test set\n",
    "                                    download=ox3_pet_download,    # Download if not already present\n",
    "                                    transform=simple_transform    # Apply the defined transformations\n",
    "                                   )\n",
    "\n",
    "# Compute the mean and standard deviation for the dataset\n",
    "dataset_mean, dataset_std = calculate_mean_std(my_dataset)\n",
    "\n",
    "print(f\"\\nCalculation Complete.\")\n",
    "print(f\"Dataset Mean: {dataset_mean}\")\n",
    "print(f\"Dataset Std:  {dataset_std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaa2fc3-be33-4bfe-8193-be71f4f7f843",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now that you have the specific mean and standard deviation for the **OxfordIIITPet dataset**, let's address a common question about their reusability.\n",
    "\n",
    "**Are Stats from 100x100 Images Good Enough for a 224x224 Pipeline?**\n",
    "\n",
    "**Yes, absolutely!** This is a pragmatic and widely used approach.\n",
    "\n",
    "For most computer vision tasks, the statistics calculated from a smaller, resized version of an image are a very strong proxy for the original. The core subject (the pets) and color distribution remain the same. While resizing does introduce a tiny amount of statistical change (due to pixel interpolation), this difference is almost always negligible in practice.\n",
    "\n",
    "The benefit is clear: you get a massive speed-up on a one-time calculation, and the resulting stats are far more representative of your dataset than generic values (like those from ImageNet). This is a classic case of a worthwhile trade-off between theoretical perfection and practical efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0640d619-0f59-4073-b148-f22ef469c900",
   "metadata": {},
   "source": [
    "## Composing Transformations for Data Augmentation\n",
    "\n",
    "The real power of transformations comes from chaining them together. `transforms.Compose` creates a single pipeline that applies a sequence of transformations to an image in order. \n",
    "\n",
    "* You will create two pipelines: \n",
    "    * `base_transform`: A simple pipeline with just the essential preprocessing steps (resizing and cropping). Normalization is intentionally skipped so the output images are visually correct and can serve as a clean baseline for comparison.\n",
    "    * `full_augmentation_pipeline`: This includes your random, augmentative transforms and normalization using the stats you just calculated.\n",
    "    \n",
    "This will allow you to directly compare a \"clean\" batch of images with a fully augmented one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9a08f8-1768-4a89-8107-0955b64426e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A simple transform to get a clean, un-augmented version of the images\n",
    "base_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor()\n",
    "    # Skip normalization to keep the image's pixel values in a display-friendly range.\n",
    "])\n",
    "\n",
    "# The full augmentation pipeline with all random transformations\n",
    "full_augmentation_pipeline = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=224),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.05, contrast=0.05),\n",
    "    SaltAndPepperNoise(amount=0.001),\n",
    "    transforms.ToTensor(),\n",
    "    # Using `mean` and `std` values as calculated on the 100x100 images\n",
    "    transforms.Normalize(mean=dataset_mean,\n",
    "                         std=dataset_std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1215ae93-f8a5-42f4-b565-471d3cdcfdc2",
   "metadata": {},
   "source": [
    "### A Dataset Without Augmentations\n",
    "\n",
    "First, take a look at the data in its original form.\n",
    "\n",
    "* Use a `DataLoader` with your `base_transform` pipeline to load a batch of images.\n",
    "    * This applies only the necessary resizing and cropping, giving you a clean, consistent baseline to see what the images look like before any random augmentation is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400f5526-6eb7-4f74-8726-4cd9d5484adf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the dataset with ONLY the base transforms\n",
    "original_dataset = datasets.OxfordIIITPet(root=ox3_pet_data_path, \n",
    "                                          split='test',\n",
    "                                          download=ox3_pet_download,\n",
    "                                          transform=base_transform\n",
    "                                         )\n",
    "\n",
    "# Create a DataLoader for the original images\n",
    "original_loader = data.DataLoader(original_dataset, batch_size=9, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d349430e-9d4d-4373-9f87-605521414075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get one fixed batch of original images\n",
    "original_images, _ = next(iter(original_loader))\n",
    "\n",
    "# Create a grid from the batch of images, arranging them with 3 images per row.\n",
    "grid = vutils.make_grid(original_images, nrow=3, padding=2) \n",
    "\n",
    "print(\"Original Un-augmented Batch:\\n\")\n",
    "helper_utils.display_grid(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3567f224-7570-4238-bcfa-85d19923ed68",
   "metadata": {},
   "source": [
    "### Applying the Augmentation Pipeline\n",
    "\n",
    "Now, you'll see the effect of the `full_augmentation_pipeline`. You will take the same batch of original images from the previous step and apply the full pipeline to each one. You'll do this in a loop to see how the random augmentations change with each run.\n",
    "\n",
    "This demonstrates the core concept of data augmentation. Because the pipeline includes random operations, applying it to an image produces a slightly different result each time. This is precisely what happens during model training when you pass an augmentation pipeline to a `DataLoader`. The transforms are applied on the fly, so with each epoch, your model is fed a unique version of the same original image. This process virtually increases the size and diversity of your training data without you having to collect more images, which helps your model generalize better and reduces overfitting.\n",
    "\n",
    "**Note:** the image colors may appear different because each color channel has been normalized in this transformation. Feel free to comment out the `Normalize` and observe the difference in the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3853cce-05ce-424e-b21b-330da3ee516b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use a loop to apply different random augmentations\n",
    "for i in range(3):\n",
    "    \n",
    "    augmented_batch = []\n",
    "    \n",
    "    # Loop through each original image in the fixed batch\n",
    "    for img_tensor in original_images:\n",
    "        \n",
    "        # Convert tensor back to PIL image to apply random transforms\n",
    "        img_pil = transforms.ToPILImage()(img_tensor)\n",
    "\n",
    "        # Apply the random augmentation pipeline\n",
    "        augmented_tensor = full_augmentation_pipeline(img_pil)\n",
    "\n",
    "        # Add the augmented tensor to the list for display\n",
    "        augmented_batch.append(augmented_tensor)\n",
    "\n",
    "    # Stack the list of augmented tensors into a single batch tensor\n",
    "    final_batch = torch.stack(augmented_batch)\n",
    "\n",
    "    # Create a grid from the batch of images, arranging them with 3 images per row\n",
    "    grid = vutils.make_grid(final_batch, nrow=3, padding=2)\n",
    "    \n",
    "    print(f\"\\nAugmented Batch - Run #{i + 1}\")\n",
    "    helper_utils.display_grid(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77de18e9-7743-40f3-a2bf-03b102bf725d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You've successfully navigated the core components of image preprocessing and augmentation with TorchVision.\n",
    "\n",
    "You saw firsthand how to build a flexible and powerful pipeline for preparing image data. You started with the fundamentals, such as converting images between the `Pillow (PIL)` format and `PyTorch Tensors`, and used essential utilities like `make_grid` for effective visualization.\n",
    "\n",
    "From there, you explored the building blocks of data augmentation, applying individual transformations like `RandomResizedCrop` and `ColorJitter` and even defining a custom noise function from scratch. You also saw the importance of **normalization** and the best practice of calculating the specific mean and standard deviation for your dataset, a key step for stable and efficient training. Finally, you brought all these techniques together using `transforms.Compose` to create a sophisticated augmentation pipeline that transforms images on the fly.\n",
    "\n",
    "These skills are foundational to virtually every computer vision task. A well-designed data pipeline, complete with thoughtful augmentations, not only makes your model more robust but also makes your entire workflow more efficient, reproducible, and reliable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-dlai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
