{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8233f704-9c0e-4863-8a07-ec2f00d0574a",
   "metadata": {},
   "source": [
    "# TorchVision Utility Functions and Models\n",
    "\n",
    "In this lab, you'll move from theory to practice by harnessing the power of the `torchvision` library. In today's AI landscape, the challenge isn't just building models from scratch, but deploying powerful, accurate, and interpretable solutions efficiently. This is where `torchvision` becomes an indispensable part of your toolkit. This lab is designed to take you beyond theory and immerse you in the professional workflow of using state-of-the-art, pre-trained models to solve real-world problems.\n",
    "\n",
    "In this lab, you will see how to:\n",
    "\n",
    "* Use `torchvision` utilities to visualize model predictions with **bounding boxes** and **segmentation masks**.\n",
    "\n",
    "* Inspect pre-trained models to understand their architecture and identify their output classes.\n",
    "\n",
    "* Perform **inference** with pre-trained models for classification, segmentation, and object detection.\n",
    "\n",
    "The lab culminates in a hands-on \"Try it Yourself!\" section where you will run these models on your own uploaded images, solidifying your understanding and preparing you for real-world projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d97885-614d-4620-b94e-30eb03797235",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150df8cd-6ad1-4b22-a4c6-81ef14ea00f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import torchvision.models as tv_models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from IPython.display import Image as DisplayImage\n",
    "from PIL import Image\n",
    "from torchvision.io import decode_image\n",
    "\n",
    "import helper_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073d5b9b-324c-4b17-abe2-bf65cffc5e69",
   "metadata": {},
   "source": [
    "## TorchVision Utilities for Image Annotation\n",
    "\n",
    "A model's output, whether a set of coordinates or a dense map of class scores, is abstract data until you can see it. How do you know if your object detector is working correctly? You draw its predictions directly onto the image. This act of visualization is a fundamental step in any computer vision project, turning raw tensor data into clear, human-understandable results.\n",
    "\n",
    "In this section, you will see two of TorchVision's most practical utilities in action:\n",
    "\n",
    "* `draw_bounding_boxes` for placing boxes around objects a model has identified.\n",
    "\n",
    "* `draw_segmentation_masks` for overlaying detailed, pixel-level masks on specific objects.\n",
    "\n",
    "These functions are your primary tools for debugging, evaluating model performance, and creating compelling visual demonstrations of your work.\n",
    "\n",
    "### Drawing Bounding Boxes\n",
    "\n",
    "An object detection model's main job is to find objects and pinpoint their locations. The classic and most direct way to visualize this location data is by drawing **bounding boxes**, simple rectangles that frame each detected object.\n",
    "\n",
    "You'll use the `draw_bounding_boxes` function to perform this task. It takes your original image and a set of box coordinates to instantly create a clear visual representation of your model's findings. This is the exact technique used in applications ranging from self-driving cars identifying pedestrians to automated checkout systems logging products.\n",
    "\n",
    "* Define hardcoded coordinates for two bounding boxes.\n",
    "    * In practical scenarios, these coordinates are returned by a detection model.\n",
    "* Define a list of strings, `labels`, corresponding to each bounding box.\n",
    "* Use <code>[draw_bounding_boxes()](https://docs.pytorch.org/vision/main/generated/torchvision.utils.draw_bounding_boxes.html)</code> to draw the defined bounding boxes.\n",
    "    * `image`: The `uint8` or `float` tensor of shape (`C, H, W`) on which to draw the boxes.\n",
    "    * `boxes`: A tensor of shape (`N, 4`), where `N` is the number of boxes and `4` corresponds to the (`xmin, ymin, xmax, ymax`) coordinates.\n",
    "    * `labels`: List of strings to display each bounding box.\n",
    "    * `colors`: List of colors used to represent each label or object. Each color corresponds to a specific object detected and is used when drawing its bounding box.\n",
    "    * `width=3`: Sets the pixel line width for the bounding box to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30604dd2-55ce-4c03-8960-c2210922dbe5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the image\n",
    "image = decode_image('./images/dog1.jpg')\n",
    "\n",
    "# Sample bounding boxes, where each box is in (xmin, ymin, xmax, ymax) format\n",
    "# The first box is set to draw around the whole dog, the other to draw around the left eye\n",
    "boxes = torch.tensor([[140, 30, 375, 315], [200, 70, 230, 110]], dtype=torch.float)\n",
    "\n",
    "# Corresponding labels for the detected objects\n",
    "labels = [\"dog\", \"eye\"]\n",
    "\n",
    "# Draw boxes on the image\n",
    "result = vutils.draw_bounding_boxes(image=image, \n",
    "                                    boxes=boxes, \n",
    "                                    labels=labels,           # This is optional\n",
    "                                    colors=[\"red\", \"blue\"],  # This is optional. By default, random colors are generated for boxes.\n",
    "                                    width=3                  # This is optional. The default is width=1\n",
    "                                   )\n",
    "\n",
    "# Display the result\n",
    "helper_utils.display_images(processed_image=result, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1eefc5-bdc0-4509-8296-558b4c83e234",
   "metadata": {},
   "source": [
    "### Drawing Segmentation Masks\n",
    "\n",
    "While bounding boxes tell you *where* an object is, sometimes you need to know its exact **shape**. For tasks requiring a higher level of precision, you must go beyond boxes and classify every single pixel of an object.\n",
    "\n",
    "This is the job of image segmentation, which generates a **segmentation mask**, a pixel-perfect overlay that highlights an object's precise silhouette. This detailed understanding is indispensable in advanced applications. For example, a self-driving car uses segmentation to understand the exact boundary of the road, and medical imaging models rely on it to outline tumors with clinical accuracy.\n",
    "\n",
    "You will use the `draw_segmentation_masks` function to bring these intricate predictions to life on an image.\n",
    "\n",
    "* Load the pre-calculated `object_mask` from a file.\n",
    "    * For demonstration purposes, this mask was pre-calculated only to work on the `dog1.jpg`.\n",
    "    * In practical scenarios, the masks are returned by a segmentation model.\n",
    "* Use <code>[draw_segmentation_masks()](https://docs.pytorch.org/vision/main/generated/torchvision.utils.draw_segmentation_masks.html)</code> to draw the defined segmentation mask.\n",
    "    * `image`: The `uint8` or `float` tensor of shape (`3, H, W`) on which to draw the masks.\n",
    "    * `masks`: A **boolean** tensor of shape (`num_masks, H, W`) or (`H, W`), where `num_masks` is the number of masks and `True` values indicate the pixels to be colored.\n",
    "    * `alpha`: A **float** between `0` (fully transparent) and `1` (fully opaque) that controls the mask's transparency.\n",
    "    * `colors`: List of colors used to represent each object. Each color corresponds to a detected object and is used when drawing its mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c30dca-1f53-4ebf-8611-e71562b2f3dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the pre-saved segmentation mask\n",
    "mask_filename = \"dog_segmentation_mask.pt\"\n",
    "loaded_object_mask = torch.load(mask_filename)\n",
    "\n",
    "# Make it (1, H, W)\n",
    "object_mask = loaded_object_mask.unsqueeze(0)\n",
    "\n",
    "\n",
    "# Draw segmentation mask on the image\n",
    "result  = vutils.draw_segmentation_masks(image=image,\n",
    "                                         masks=object_mask,\n",
    "                                         alpha=0.5,          # This is optional. The default is alpha=0.8\n",
    "                                         colors=[\"blue\"]     # This is optional. By default, random colors are generated for each mask.\n",
    "                                        )\n",
    "\n",
    "# Display the result\n",
    "helper_utils.display_images(processed_image=result, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a14eeb-e977-466a-8dab-0a51d18048ad",
   "metadata": {},
   "source": [
    "## Your Toolkit of Pre-trained Models\n",
    "\n",
    "Training a major computer vision model from scratch is a monumental effort, requiring huge datasets and weeks of GPU time. TorchVision gives you a powerful shortcut: a professional library of **pre-trained models**.\n",
    "\n",
    "Think of these as expert models that have already been trained on massive datasets like ImageNet or COCO. They come with a built-in understanding of visual features, from simple textures to complex objects like faces and vehicles. You can now use this expert knowledge as a powerful starting point for your own projects.\n",
    "\n",
    "**A Tour of Available Architectures**\n",
    "\n",
    "TorchVision organizes its models by the primary task they were designed to solve. Here are the main categories:\n",
    "\n",
    "* **Image Classification**: Answers the basic question: 'What is the main subject of this image?'\n",
    "    * **Models**: [ResNet](https://docs.pytorch.org/vision/main/models/resnet.html), [VGG](https://docs.pytorch.org/vision/main/models/vgg.html), [AlexNet](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.alexnet.html), [SqueezeNet](https://docs.pytorch.org/vision/main/models/squeezenet.html), [MobileNetV3](https://docs.pytorch.org/vision/main/models/mobilenetv3.html), [DenseNet](https://docs.pytorch.org/vision/main/models/densenet.html)\n",
    "* **Image Segmentation**: Goes deeper to ask: 'What is the exact pixel-by-pixel shape of each object?'\n",
    "    * **Models**: [FCN](https://docs.pytorch.org/vision/main/models/fcn.html), [DeepLabV3](https://docs.pytorch.org/vision/main/models/deeplabv3.html)\n",
    "* **Object Detection**: Finds all recognizable objects in an image, draws a box around each one, and classifies them.\n",
    "    * **Models**: [Faster R-CNN](https://docs.pytorch.org/vision/main/models/faster_rcnn.html), [RetinaNet](https://docs.pytorch.org/vision/main/models/retinanet.html), [SSD](https://docs.pytorch.org/vision/main/models/ssd.html)\n",
    "* **Video Classification**: Understands action and movement by classifying entire video clips.\n",
    "    * **Models**: [R(2+1)D 18](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.video.r2plus1d_18.html), [MC3 18](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.video.mc3_18.html), [Video MViT](https://docs.pytorch.org/vision/main/models/video_mvit.html)\n",
    "\n",
    "**Two Strategies for Using These Models**\n",
    "\n",
    "There are two primary strategies for integrating these models into your work:\n",
    "\n",
    "* **Inference (Out-of-the-Box Prediction)**: You use a model's existing knowledge directly to make predictions. This is perfect when your task is very similar to what the model was originally trained on.\n",
    "\n",
    "* **Transfer Learning (Fine-Tuning)**: You adapt a pre-trained model for a new, specialized task by leveraging its existing expert knowledge. This is a highly flexible strategy that gives you precise control over how the model learns your new data. The approach can range from only training a new final layer on top of the frozen base model to selectively fine-tuning deeper layers, or even retraining the entire network on your custom dataset. This powerful technique allows you to achieve excellent results much faster and with less data than training from scratch.\n",
    "\n",
    "In this lab, you will focus entirely on the first strategy: **performing inference**. You will see transfer learning in action in the next lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f70b1b3-4b71-4959-b2b9-a106469d4d15",
   "metadata": {},
   "source": [
    "### Knowing What Your Model Knows: Classes and Capabilities\n",
    "\n",
    "Before using a pre-trained model, you need to perform a quick but vital sanity check. Think of it like reading the label on a tool before you use it: you need to know exactly what it was designed to do. Specifically, you need to answer two questions:\n",
    "\n",
    "* **How many classes** can this model predict?\n",
    "\n",
    "* **What are the names** of those classes?\n",
    "\n",
    "This simple investigation prevents major headaches. For example, if your project requires detecting dogs, you must first confirm that the model you've chosen actually includes 'dog' in its list of recognized classes. Answering this question upfront saves you from wasting time on a model that can't perform your specific task.\n",
    "\n",
    "How you find this information depends on the model's age. Here’s a generic, two-step approach that you can apply to any model.\n",
    "\n",
    "#### Method 1: The Modern Approach (Check for Metadata First)\n",
    "\n",
    "Modern TorchVision models make this easy by embedding this information directly into the model's weights object under a `.meta` attribute. This is the official, most reliable source of truth, and you should **always check this first**.\n",
    "\n",
    "To streamline this process, you will use the provided `get_model_classes_from_weights_meta` helper function, which automatically inspects the weights object and prints the class list for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab35ff8-c95c-46b8-9850-af2a2e12593b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model_classes_from_weights_meta(model, weights_obj=None):\n",
    "    \"\"\"\n",
    "    Inspects a model's weights object to find and return the class names.\n",
    "    \n",
    "    Args:\n",
    "        model: The model instance.\n",
    "        weights_obj: The weights enumeration object (e.g., ResNet50_Weights.DEFAULT).\n",
    "    \n",
    "    Returns:\n",
    "        A tuple containing (number_of_classes, list_of_class_names), or (None, None) if not found.\n",
    "    \"\"\"\n",
    "    num_classes = None\n",
    "    class_names = None\n",
    "\n",
    "    # Check if a weights object was provided and if it has the necessary metadata\n",
    "    if weights_obj and hasattr(weights_obj, 'meta') and \"categories\" in weights_obj.meta:\n",
    "        class_names = weights_obj.meta[\"categories\"]\n",
    "        num_classes = len(class_names)\n",
    "        print(f\"Model is configured for {num_classes} classes based on Weights Metadata. These classes are:\\n\")\n",
    "        # For nice printing, let's display the list\n",
    "        pprint(class_names)\n",
    "            \n",
    "        return num_classes, class_names\n",
    "\n",
    "    else:\n",
    "        print(\"'categories' metadata not found for this model.\")\n",
    "        return num_classes, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8f6108-ea5b-4c1a-a63e-94886eeb2d52",
   "metadata": {},
   "source": [
    "* For this example, you'll inspect a [deeplabv3_resnet50](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.segmentation.deeplabv3_resnet50.html) model, which is used for image segmentation.\n",
    "* From the [weights of this particular model](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.segmentation.deeplabv3_resnet50.html#torchvision.models.segmentation.DeepLabV3_ResNet50_Weights), you’ll use `DeepLabV3_ResNet50_Weights.DEFAULT`. Here, `.DEFAULT` is an alias that automatically points to the best and most current pre-trained weights available for this model.\n",
    "    * Using `.DEFAULT` is considered best practice, as it makes your code more robust and future-proof."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da9f693-a5a6-4377-b029-0675f0ee38e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining the model itself without weights for now\n",
    "seg_model = tv_models.segmentation.deeplabv3_resnet50(weights=None)\n",
    "\n",
    "# Select the specific pre-trained weights you want to inspect for your selected model\n",
    "seg_model_weights = tv_models.segmentation.DeepLabV3_ResNet50_Weights.DEFAULT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8333ad72-4fc4-4c91-8284-e136b97c1357",
   "metadata": {},
   "source": [
    "* Use the helper function to get the class information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d655a5b-03ba-4dc4-bb0b-7ddffadf9045",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call the helper function to inspect the provided weights object.\n",
    "num_classes, class_names_deeplabv3 = get_model_classes_from_weights_meta(\n",
    "    model=seg_model,              # The model architecture.\n",
    "    weights_obj=seg_model_weights # The weights object containing the .meta attribute to inspect.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219aea0d-22c8-487b-90cf-bcb16d570780",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "As you can see, the helper function successfully extracted the 21 class names directly from the weights object. Now you can confidently decide if this model, with these specified weights, is right for your task.\n",
    "\n",
    "Bringing back the earlier example, if your goal was to find a 'dog', you can now scan this list and confirm that `'dog'` is a recognized class. This tells you the model is a suitable choice. If 'dog' hadn't been on the list, you would know immediately that you need to find a different model and weights, saving you valuable time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f36b11-c1ca-46dc-9afb-c211e33bd8a2",
   "metadata": {},
   "source": [
    "#### Method 2: The Manual Approach (When No Metadata Exists)\n",
    "\n",
    "What do you do when a model doesn't have the `.meta` attribute? This is a common scenario you'll encounter with older models loaded via the `pretrained=True` flag, or with custom models that don't have this information packaged with them.\n",
    "\n",
    "In these situations, you need to put on your detective hat. You'll investigate the model's architecture directly to uncover the information you need. This manual inspection is a fundamental skill, ensuring you can work effectively with any model, not just the newest ones.\n",
    "\n",
    "* Let's use an older [Resnet50](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html) model as an example.\n",
    "    * You will load it using the legacy `pretrained=True` method, which automatically fetches its standard weights pre-trained on the ImageNet 1K dataset.\n",
    "\n",
    "First, confirm that this loading method indeed lacks a discoverable `.meta` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967f6f3c-4ec1-42d6-a415-42750ade3e64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the ResNet50 model using the legacy `pretrained=True` method.\n",
    "resnet50_model = tv_models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f5398a-8a75-470b-b703-bfcde6b45871",
   "metadata": {},
   "source": [
    "When using `pretrained=True`, the weights are loaded directly into the model's layers. Unlike the modern approach, this doesn't create a separate weights object that holds the `metadata`. Therefore, you'll explicitly pass `weights_obj=None` to the helper function to demonstrate that no such object is available with this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8b1fb2-91d3-493d-bd83-4048739bdcd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pass `weights_obj=None` because the `pretrained=True` loading method\n",
    "# does not create a separate weights object that has a .meta attribute to inspect.\n",
    "num_classes, class_names = get_model_classes_from_weights_meta(\n",
    "    model=resnet50_model, \n",
    "    weights_obj=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bf8abc-c3e3-44b1-a364-912ec5c5eb70",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "This confirms that this loading method does not provide metadata, which is why you must proceed with manually inspecting the model's architecture.\n",
    "\n",
    "#### Manually Inspecting the Architecture\n",
    "\n",
    "Since the metadata approach was a dead end, your next step is to inspect the model's blueprint directly. Your goal is to find the final layer of the network, as its configuration will tell you how many classes the model predicts.\n",
    "\n",
    "The specific clue you're looking for is the `out_features` parameter in the model's final `Linear` (fully-connected) layer. The most straightforward way to find this is to print the model object itself:\n",
    "\n",
    "```\n",
    "print(resnet50_model)\n",
    "```\n",
    "\n",
    "This will print the entire model architecture, which can be quite long. You only need to focus on the last few lines of the output. For `ResNet50`, you'll find a layer named `fc` that contains the answer:\n",
    "\n",
    "\n",
    "```\n",
    "...\n",
    "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffd2712-28cd-456b-be94-ab74f25de1c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ### Uncomment and execute the line below if you wish print the model's architecture.\n",
    "\n",
    "# print(resnet50_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875869d7-de2a-4067-913f-b743c34e773e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The `out_features=1000` tells you that the model is configured for 1000 classes. You can access this value directly in the code to confirm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4caf1b-3700-4212-9597-75da47c96ce4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the number of output features from the layer named 'fc'\n",
    "num_classes = resnet50_model.fc.out_features\n",
    "\n",
    "print(f\"Inspecting the model's .fc layer: It has {num_classes} output classes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b08e2c-6cb0-4aaa-b383-29dd45974b5c",
   "metadata": {},
   "source": [
    "#### Finding the Class Names\n",
    "\n",
    "Your detective work is almost complete. You've discovered that the model predicts 1000 classes, but a class **index** (like `248`) is meaningless without a class **name** (like `'bull mastiff'`). The final step is to find the legend that maps these indices to human-readable names. As you've learned, this information is stored **externally** from the model file.\n",
    "\n",
    "For a model like `ResNet50` (legacy variant) trained on `ImageNet`, this class list is well-documented. Here are the first places a professional would look:\n",
    "\n",
    "* **Official Documentation: Always start here**. The PyTorch documentation for a model or its weights will almost always describe the dataset it was trained on and provide a link to the class list.\n",
    "\n",
    "* **Community Helper Files**: Code repositories and online tutorials are another excellent source. You can often find helper files (like a `.json` or `.txt`) that contain the direct index-to-name mapping.\n",
    "\n",
    "In the next section, you'll use one such helper file to load the ImageNet class names from a `.json` file and then perform classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046b04f8-b552-432f-9410-e8f1ccbf70c5",
   "metadata": {},
   "source": [
    "## Performing Inference with Pre-trained Models\n",
    "\n",
    "Now that you know how to investigate a model's capabilities, it's time to put that knowledge into practice. You'll perform **inference** using a model \"off-the-shelf\" to get predictions on new images.\n",
    "\n",
    "You will apply this principle to the two models you've already inspected:\n",
    "\n",
    "* **Image Segmentation**: You'll use `DeepLabV3` to find and draw a pixel-perfect mask over an object.\n",
    "\n",
    "* **Image Classification**: You'll use the legacy `ResNet50` to predict the main subject of an image.\n",
    "\n",
    "### Image Segmentation\n",
    "\n",
    "Your first task is to perform **image segmentation** with the `DeepLabV3` model. The goal is straightforward: find a dog in an image and generate a pixel-perfect mask that highlights its exact shape.\n",
    "\n",
    "* You'll begin by loading an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820c5259-a8a6-425a-b76a-01fb3a4c9850",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the file path for the image.\n",
    "image_path = './images/dog2.jpg'\n",
    "\n",
    "# Display the image.\n",
    "DisplayImage(image_path, width=300, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2932a42-2843-4222-afb4-f8e815d25cb3",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "In your earlier investigation, you confirmed that the `DeepLabV3` model's class list includes `'dog'`, making it the right tool for this task.\n",
    "\n",
    "* Now you'll load the model again, this time attaching its pre-trained `.DEFAULT` weights and calling `.eval()` to prepare it for inference.\n",
    "    * **Note on the Download**: A larger file will download now because you're loading the complete model (backbone + head). The class list you checked earlier remains correct, as you're still using the `.DEFAULT` weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92ee3bd-8a18-4961-90a4-8c27c56a9ffc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the model architecture and load the pre-trained weights.\n",
    "seg_model = tv_models.segmentation.deeplabv3_resnet50(weights=seg_model_weights).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181f7cf6-5f58-420f-bb12-5c516993c0d0",
   "metadata": {},
   "source": [
    "With your pre-trained `seg_model` loaded and in evaluation mode, you'll now walk through the complete process of performing image segmentation.\n",
    "\n",
    "* **Prepare Your Tensors**: Start by loading the image. From it, you'll create two tensor versions:\n",
    "\n",
    "    * `original_image_tensor`: A clean, un-normalized tensor that you'll use later for visualization.\n",
    "    \n",
    "    * `input_tensor`: A normalized version of that tensor to be used as the input for the model.\n",
    "    \n",
    "* **Define Your Targets and Colors**: Next, you'll define a `target_class_names` list, setting it to just `['dog']` for this example. You'll also create a corresponding `seg_colors` list to assign a color for each target's mask. Finally, you will get the class indices from the `class_names_deeplabv3` list, which you retrieved in the \"**Modern Approach**\" section.\n",
    "\n",
    "    * **NOTE:** Each class name in your `target_class_names` list must be an exact, **case-sensitive match** to a name in the model's class list, otherwise the lookup will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c863e68-b3d0-42c3-a2e9-83b87854c515",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the base PIL Image\n",
    "img = Image.open(image_path)\n",
    "\n",
    "# Create the clean, un-normalized tensor for visualization later\n",
    "original_image_tensor = transforms.ToTensor()(img)\n",
    "\n",
    "# Define the normalization transform\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# Create the normalized input tensor for the model\n",
    "# .unsqueeze(0) adds a batch dimension, changing the tensor shape \n",
    "# from [C, H, W] to [N, C, H, W], as models expect a batch of images for input\n",
    "input_tensor = normalize(original_image_tensor).unsqueeze(0)\n",
    "\n",
    "# Define a list of target classes you want to find\n",
    "target_class_names = ['dog'] # More classes can be added as well, e.g., ['dog', 'person', ...]\n",
    "\n",
    "# Define a corresponding list of colors for the segmentation masks for each class\n",
    "seg_colors = [\"blue\"]\n",
    "\n",
    "# Use a list comprehension to get a list of corresponding class indices\n",
    "class_indices = [class_names_deeplabv3.index(name) for name in target_class_names]\n",
    "\n",
    "# Print the results for confirmation\n",
    "print(f\"Target Classes:        {target_class_names}\")\n",
    "print(f\"Corresponding Indices: {class_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb97849f-da4f-40ab-b20e-80fbaf261e86",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* **Perform Inference**: Pass the normalized `input_tensor` to the pre-trained `seg_model`. The model returns its predictions in a dictionary, so you'll use `['out'][0]` to access the raw output scores (logits) for the first (and only) image in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dc4282-44a1-4f7a-8f70-466255f28080",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate prediction\n",
    "with torch.no_grad():\n",
    "    output = seg_model(input_tensor)['out'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc30a66-a216-468b-bde6-23431a2e9d11",
   "metadata": {},
   "source": [
    "* **Generate the Final Mask**: Convert the raw scores into a **stack of boolean masks**, one for each of your target classes (in this case, just for the `'dog'` class). This involves three steps:\n",
    "\n",
    "    * First, using `.argmax(0)` to get the model's single best prediction for each pixel.\n",
    "\n",
    "    * Second, creating a list of **individual boolean masks**, where each mask corresponds to one of your target classes.\n",
    "\n",
    "    * Finally, using `torch.stack()` to combine this list of masks into a single tensor. This is the required format for drawing multiple masks with different colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae6c827-ee1c-497b-9d83-47f3955cfdd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the predicted class for each pixel by finding the class with the highest score.\n",
    "output_predictions = output.argmax(0)\n",
    "\n",
    "# Create a separate boolean mask for each of your target classes.\n",
    "# The result is a list of boolean tensors, one for each class index.\n",
    "individual_masks = [(output_predictions == i) for i in class_indices]\n",
    "\n",
    "# Stack the individual masks into a single tensor of shape (num_masks, H, W).\n",
    "stacked_masks = torch.stack(individual_masks, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799d59bc-a2a0-4fae-950e-9e2bc02cabfc",
   "metadata": {},
   "source": [
    "* **Draw Mask**: Use `draw_segmentation_masks` to overlay your generated masks onto the original image, passing these key arguments:\n",
    "\n",
    "    * `image=(original_image_tensor * 255).byte()`: Converts the `original_image_tensor` from a `float` (0.0-1.0 scale) to a `uint8` tensor (0-255 scale), as required by the function.\n",
    "    \n",
    "    * `masks=stacked_masks`: The tensor of shape `(num_masks, H, W)` that you created, where each layer is a boolean mask for one of your target classes.\n",
    "\n",
    "    * `colors=seg_colors`: The list of color strings you defined. The function will use these to draw each corresponding mask in the `stacked_masks` tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae36617-6c05-4bcd-bb08-4f2787ee0994",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply segmentation masks using the stacked_masks tensor.\n",
    "result = vutils.draw_segmentation_masks(image=(original_image_tensor * 255).byte(),\n",
    "                                        masks=stacked_masks,\n",
    "                                        alpha=0.5,\n",
    "                                        colors=seg_colors)\n",
    "# Visualize the mask\n",
    "helper_utils.display_images(processed_image=result, figsize=(7, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c880a5-7eb5-4a4f-9a3f-24ee65e574e5",
   "metadata": {},
   "source": [
    "### Image Classification\n",
    "\n",
    "For your second inference task, you'll switch to **image classification**. Using the same image as before, your goal is to have a model predict the single most likely class for the entire image. For this, you'll use the [Resnet50](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html) model.\n",
    "\n",
    "* Load the model with its legacy weights by setting `pretrained=True`.\n",
    "    * Also set the model to evaluation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc11a43-d737-44ff-ae3f-4dcdaa05fabe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the ResNet50 model, using the legacy weights and set it to .eval()\n",
    "resnet50_model = tv_models.resnet50(pretrained=True).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99efab7-39ec-403c-b7cc-936e8811c22a",
   "metadata": {},
   "source": [
    "Your manual investigation from earlier revealed that the  `ResNet50` model **with these legacy weights (`pretrained=True`)** predicts 1000 classes,. To make sense of these predictions, you need to find the corresponding list of class names.\n",
    "\n",
    "As you've learned, this information isn't packaged with the model itself. For a standard dataset like ImageNet, this \"legend\" is commonly distributed in an external `.json` file. In the next cell, you will:\n",
    "\n",
    "* Use the `load_imagenet_classes` helper function to load the class mappings from the `'./imagenet_class_index.json'` file.\n",
    "\n",
    "* This creates a Python **dictionary** that maps each class index to its human-readable name.\n",
    "\n",
    "**Note**: This manual step of loading an external file is only necessary because you used the legacy `pretrained=True` method. Had you used a modern weights object (e.g., `ResNet50_Weights.IMAGENET1K_V2`), you could have simply used the `.meta` attribute, as you did with `DeepLabV3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a83989-279e-4d43-8b0a-6ef30d0d3aac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the helper function to load the class index-to-name mappings from the JSON file.\n",
    "imagenet_classes = helper_utils.load_imagenet_classes('./imagenet_class_index.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6000a430-7425-432e-9ac2-0a42db797446",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* Instead of printing all 1000 classes, the code cell below is set up to inspect a specific \"slice\" of the dictionary, starting at index `200`. This range contains a number of dog breeds.\n",
    "\n",
    "* When you run the cell, pay attention to **index `207`**. You'll find it corresponds to `'golden_retriever'`, the perfect match for the dog in the image you're about to classify.\n",
    "\n",
    "    * After running it once, feel free to change the `start_index` and `num_to_print` values to explore other sections of the class list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cef677f-7049-4bb1-a6da-ffef8a3ec826",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Total Classes:\", len(imagenet_classes), \"\\n\")\n",
    "\n",
    "# Define the starting index and how many classes you want to print\n",
    "start_index = 200\n",
    "num_to_print = 10\n",
    "\n",
    "print(f\"Printing {num_to_print} classes starting from index {start_index}:\\n\")\n",
    "\n",
    "# Loop through the desired range of indices\n",
    "for i in range(start_index, start_index + num_to_print):\n",
    "    key = str(i)\n",
    "    value = imagenet_classes[key]\n",
    "    print(f\"Index {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a6468c-c32a-47cd-83a9-cf2a5e2f5564",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "With your `ResNet50` model and class names loaded, you'll now walk through the process of classifying the image.\n",
    "\n",
    "* **Prepare the Image**: Define a `transform` pipeline to resize, crop, and normalize the image to match the format the model was trained on. You'll then apply this pipeline to your image and use `.unsqueeze(0)` to create a batch of one for the model input.\n",
    "\n",
    "**Important Note**: When preparing an image for a pre-trained model, it is **highly recommended** that the final input tensor have the **same dimensions** and **normalization** as the data the model was trained on. Models like `ResNet50` expect a specific input size (e.g., 224x224 pixels) and data distribution. A mismatch in either can cause errors or significantly degrade performance. The transformation pipeline below is the standard method for achieving this format for models trained on ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca744bcb-be6e-42b8-bf1e-ec0af67dde75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the base PIL Image\n",
    "img = Image.open(image_path)\n",
    "\n",
    "# Define the transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    # Models like ResNet50 expect a 224x224 input, so you resize to a slightly\n",
    "    # larger image and then take a center crop of the target dimensions.\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize the tensor with the mean and standard deviation from the ImageNet dataset.\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Apply the transforms and add a batch dimension\n",
    "input_tensor = transform(img)\n",
    "# Model's input layer expects a 4D tensor with a specific shape: [N, C, H, W]\n",
    "input_batch = input_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cbc359-07cb-4af7-bb7f-237a1dc31734",
   "metadata": {},
   "source": [
    "* **Perform Inference**: Pass the prepared `input_batch` to the pre-trained `resnet50_model`. The model will output a tensor of raw, unnormalized scores/logits, one score for each of the 1000 ImageNet classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3198a2-649b-4ff2-830a-3519a019a46c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform Inference\n",
    "with torch.no_grad():\n",
    "    output = resnet50_model(input_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b6e84b-caef-4203-8361-b7592b37898c",
   "metadata": {},
   "source": [
    "* **Convert Scores to Probabilities**: Apply the <code>[torch.nn.functional.softmax()](https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html)</code> function to the logits.\n",
    "    * This converts the raw scores into a probability distribution where each value represents the model's confidence for a particular class, and all probabilities sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c65ba-bd2b-4a2a-88dd-9c1e75b7c3d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply softmax for probabilities\n",
    "# Apply it to the first (and only) item in the output batch.\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee2053a-aeb8-427f-9d94-a7cc3df966d3",
   "metadata": {},
   "source": [
    "* **Get the Top Predictions**: Use <code>[torch.topk()](https://docs.pytorch.org/docs/stable/generated/torch.topk.html)</code> to efficiently find the `top` classes with the highest probabilities and get their corresponding class IDs (indices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f1b789-9ab2-492d-9ed9-05bf41c98ff4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the `top` probabilities and their corresponding class IDs\n",
    "top = 5\n",
    "top_prob, top_catid = torch.topk(probabilities, top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b82c29-d7a1-413c-b4db-03ada4a992c4",
   "metadata": {},
   "source": [
    "* **Display the Results**: Loop through the `top` results. For each one, you'll use its class ID to look up the human-readable name in the `imagenet_classes` dictionary and print it alongside its confidence score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f53f0b-1bae-4400-8f0b-42df8658368f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert IDs to class names and print results\n",
    "print(f\"Top {top} predictions:\")\n",
    "for i in range(top_prob.size(0)):\n",
    "    # Get the string representation of the class ID\n",
    "    class_id_str = str(top_catid[i].item())\n",
    "    \n",
    "    # Look up the class name in the dictionary\n",
    "    class_name = imagenet_classes[class_id_str][1]\n",
    "    confidence = top_prob[i].item() * 100\n",
    "    print(f\"\\tTop-{i+1}: {class_name} ({confidence:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb54291a-1cc2-467e-a3ce-9bf99221e145",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "As you can see, the model correctly identified the dog as a **`'golden_retriever'`** with high confidence, successfully concluding the lab's core workflow. This lab has walked you through the essential skills of a computer vision practitioner: investigating a model's capabilities, using pre-trained models for inference, and using `torchvision` utilities to visualize the results.\n",
    "\n",
    "## (Optional) Object Detection\n",
    "\n",
    "With this solid foundation, the final section offers a hands-on opportunity to see these concepts in action on your own images. You will run provided code that uses a powerful `Faster R-CNN` model to perform **object detection** and **DeepLabV3** to generate segmentation masks. This is a chance to experiment with the provided tools and observe their effects on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf49a0d-6860-4003-b4bc-336c4962b896",
   "metadata": {},
   "source": [
    "### Detecting Objects using `fasterrcnn_resnet50_fpn` Model\n",
    "\n",
    "The [fasterrcnn_resnet50_fpn](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html) model is a popular and powerful choice for object detection tasks due to the strengths of its combined components:\n",
    "\n",
    "* `Faster R-CNN (Region-based Convolutional Neural Network)`: This is the core architecture designed specifically for object detection. It efficiently identifies potential object regions (Region Proposal Network) and then classifies these regions and refines their bounding box coordinates. It's known for its good balance between speed and accuracy.\n",
    "\n",
    "* `ResNet-50 (Residual Network 50)`: This serves as the backbone of the model. ResNet-50 is a deep convolutional neural network with 50 layers, renowned for its ability to learn rich, hierarchical features from images. Its \"residual connections\" help in training very deep networks effectively, leading to better performance.\n",
    "\n",
    "* `FPN (Feature Pyramid Network)`: This component enhances the model's ability to detect objects at multiple scales. FPN constructs a pyramid of feature maps with different resolutions, allowing the model to effectively identify both small and large objects within the same image.\n",
    "\n",
    "In essence, `fasterrcnn_resnet50_fpn` leverages a strong feature extractor (ResNet-50), an effective multi-scale detection strategy (FPN), and a robust object detection framework (Faster R-CNN), making it a well-rounded and high-performing model for locating and classifying various objects in an image. Using `FasterRCNN_ResNet50_FPN_Weights.DEFAULT` ensures you're loading the model with pre-trained weights, typically trained on a large dataset like COCO, which allows it to recognize a wide variety of common objects out-of-the-box.\n",
    "\n",
    "* Run the cell below to load the model and set it to evaluation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b773d306-6a6e-4ed0-8a68-6822c59c264d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load a pre-trained object detection model and set to evaluation mode\n",
    "bb_model_weights = tv_models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "bb_model = tv_models.detection.fasterrcnn_resnet50_fpn(weights=bb_model_weights).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8efde9-8ab2-4826-8f35-7a9aecadda79",
   "metadata": {},
   "source": [
    "#### How many classes can the model detect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a79d0c-b814-4185-908e-55bb749af6bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the helper function to inspect the weights object of the object detection model.\n",
    "num_classes, classes = get_model_classes_from_weights_meta(\n",
    "    model=bb_model, \n",
    "    weights_obj=bb_model_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4c7203-962c-42ab-8607-1d6dcc0072ff",
   "metadata": {},
   "source": [
    "* As you might have **noticed**, some class indices have `N/A` next to them.\n",
    "* The model's `91` output classes include `81` recognizable COCO dataset classes, plus 10 additional entries marked as `N/A` (the specific nature of which is not defined in this list).\n",
    "* If you were to use a standard list of 81 COCO classes instead of this model-specific list, you would encounter index mismatches with the model's output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31eae41-0eb6-4015-a36d-0d630b97ece7",
   "metadata": {},
   "source": [
    "* Your goal for this example is to detect cars and traffic lights. First, you'll set the path to an image that contains several of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cb90ff-a97e-444c-9cd2-f5d54f52a8d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the file path to the image you'll use for car/traffic light detection.\n",
    "image_path = './images/cars.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0240ed40-0b28-452d-82e6-57ccb865f4ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display the image\n",
    "DisplayImage(image_path, width=600, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c33c5f-d81a-4a64-a786-8335a806f1ca",
   "metadata": {},
   "source": [
    "* Define the `target_class_names` as a list of strings specifying the objects you want to detect, in this case, `['car', 'traffic light']`.\n",
    "\n",
    "* Create a `bbox_colors` list with a corresponding color string for each target class's bounding box.\n",
    "\n",
    "* Generate the `object_indices` list from the target names using a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220a7903-fa41-4c2c-be88-d5d4204d1179",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a list of target classes to detect\n",
    "target_class_names = ['car', 'traffic light']\n",
    "\n",
    "# Define a corresponding list of colors for each class's bounding box\n",
    "bbox_colors = ['red', 'blue']\n",
    "\n",
    "# Use a list comprehension to get a list of all target indices\n",
    "object_indices = [classes.index(name) for name in target_class_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e943504b-d1d8-4480-8bad-748c88d08ef5",
   "metadata": {},
   "source": [
    "The function `detect_and_draw_bboxes` is provided to you. It takes the detection model, image path, lists of target objects and their colors, and a confidence threshold. It then returns the image with bounding boxes drawn for all detected target objects.\n",
    "\n",
    "The core functionality of this function is as follows:\n",
    "\n",
    "* **Using a Confidence `Threshold`**: A minimum confidence score is used to filter out uncertain detections.\n",
    "\n",
    "* **Model Prediction & Scoring**: The input image is processed by the model, which outputs potential bounding boxes for all detected objects and assigns confidence scores to each.\n",
    "\n",
    "* **Filtering Qualifying Boxes**: The function loops through your list of targets and selects only those boxes that:\n",
    "\n",
    "    * Match one of the **target object classes** you provided.\n",
    "    \n",
    "    * Have a confidence score greater than the set `threshold`.\n",
    "\n",
    "* **Drawing and Returning**: The function collects all qualifying boxes from all your target classes and then uses the `draw_bounding_boxes` utility to draw them onto the image, each with its specified label and color.  The final modified image (or original, if no boxes qualified) is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fdf86d-1a4a-45ee-81c5-1c66c89e61b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detect_and_draw_bboxes(model, image_path, object_indices, labels, bbox_colors, threshold, bbox_width=3):\n",
    "    \"\"\"\n",
    "    Detects and draws labeled bounding boxes for multiple specified object classes on an image.\n",
    "\n",
    "    Args:\n",
    "        model: Pre-trained object detection model.\n",
    "        image_path (str): Path to the image file.\n",
    "        object_indices (list): List of indices for the target classes to detect.\n",
    "        labels (list): List of text labels for each target class.\n",
    "        bbox_colors (list): List of colors for each target class's bounding boxes.\n",
    "        threshold (float): Confidence threshold for detections.\n",
    "        bbox_width (int, optional): The line width for the bounding boxes. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Image tensor with all detected boxes drawn.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open and transform the image, and prepare the result tensor\n",
    "    pil_image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform_to_tensor = transforms.Compose([transforms.ToTensor()])\n",
    "    tensor_image_batch = transform_to_tensor(pil_image).unsqueeze(0)\n",
    "    result_image_tensor = (tensor_image_batch.squeeze(0) * 255).byte()\n",
    "\n",
    "    # Perform inference to get predictions for all possible objects\n",
    "    with torch.no_grad():\n",
    "        prediction = model(tensor_image_batch)[0]\n",
    "\n",
    "    # Initialize lists to collect all boxes, labels, and colors that meet the criteria\n",
    "    all_boxes_to_draw = []\n",
    "    all_labels_to_draw = []\n",
    "    all_colors_to_draw = []\n",
    "\n",
    "    # Loop through each target class to find its boxes\n",
    "    for index, label, color in zip(object_indices, labels, bbox_colors):\n",
    "        # Filter predictions for the current class index and confidence threshold\n",
    "        class_mask = (prediction['labels'] == index) & (prediction['scores'] > threshold)\n",
    "        \n",
    "        # Get the boxes for the current class\n",
    "        boxes_for_this_class = prediction['boxes'][class_mask]\n",
    "\n",
    "        if boxes_for_this_class.nelement() > 0:\n",
    "            # Add the found boxes to our master list\n",
    "            all_boxes_to_draw.extend(boxes_for_this_class.tolist())\n",
    "            # Create and add corresponding labels and colors\n",
    "            all_labels_to_draw.extend([label] * len(boxes_for_this_class))\n",
    "            all_colors_to_draw.extend([color] * len(boxes_for_this_class))\n",
    "\n",
    "    # After checking all classes, draw all collected boxes at once if any were found\n",
    "    if all_boxes_to_draw:\n",
    "        result_image_tensor = vutils.draw_bounding_boxes(\n",
    "            result_image_tensor,\n",
    "            torch.tensor(all_boxes_to_draw),\n",
    "            labels=all_labels_to_draw,\n",
    "            colors=all_colors_to_draw,\n",
    "            width=bbox_width\n",
    "        )\n",
    "    else:\n",
    "        # If the list of boxes to draw is empty, print this information.\n",
    "        print(f\"No objects from the list {labels} were found with a confidence score above {threshold}.\\n\")\n",
    "    \n",
    "    return result_image_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e8d290-27b1-4b4c-a2f7-55175c5ae5b1",
   "metadata": {},
   "source": [
    "* Set a confidence threshold. This value (from 0.0 to 1.0) is the minimum score a detection must have to be considered valid.\n",
    "    * Feel free to experiment with this value. A lower threshold (e.g., `0.3`) might reveal more objects, while a higher one (e.g., `0.8`) will only show the most confident detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf50e61c-49d4-4a9b-9e36-72d9038adf57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "confidence_threshold = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f594c00d-2920-4e1a-8e5d-d05fd6949212",
   "metadata": {},
   "source": [
    "* Execute the function to detect bounding boxes on `car` and `traffic light` in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fb486b-0a9c-4ae0-8309-79e5a5259d91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Execute the main detection function\n",
    "result_image_tensor = detect_and_draw_bboxes(\n",
    "    model=bb_model,                  # The pre-trained object detection model.\n",
    "    image_path=image_path,           # The path to the input image.\n",
    "    object_indices=object_indices,   # The list of integer indices for the target classes.\n",
    "    labels=target_class_names,       # The list of string names for the box labels.\n",
    "    bbox_colors=bbox_colors,         # The list of colors for the bounding boxes.\n",
    "    threshold=confidence_threshold,  # The minimum confidence score for a detection.\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "helper_utils.display_images(processed_image=result_image_tensor, figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87a529e-a1fb-444d-a0bd-cb13a4b87bc2",
   "metadata": {},
   "source": [
    "### Try it Yourself!\n",
    "\n",
    "This is where you can see everything you've learned come together. In this section, you will use the powerful models and functions from this lab, to perform **object detection** and **image segmentation** on your own images. You'll get to upload an image, select your target objects by changing the input variables, and then run the pre-trained models to see them in action on your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fcd5db-413e-44d2-bcdd-62a635fa4f20",
   "metadata": {},
   "source": [
    "### 1 - Find Bounding Boxes on Objects on your Own Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcf5e37-634c-47cb-8866-47bc5327216a",
   "metadata": {},
   "source": [
    "Running the function `helper_utils.upload_jpg_widget()` will display a widget that allows you to upload your own images into the workspace.\n",
    "\n",
    "* You can only upload images that have a `.jpg` extension.\n",
    "* Each image should not exceed **5 MB** in **file size**.\n",
    "* Once an image is successfully uploaded, you'll see its file path dsiplayed, which you can directly copy and paste into the cell below.\n",
    "\n",
    "Also, once the widget is displayed, you can use it multiple times to upload images; you don't have to re-run the `helper_utils.upload_jpg_widget()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b31868-fefe-4101-bd71-85976648ae7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "helper_utils.upload_jpg_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f721e2-183d-43bb-abbf-3063ed587a2f",
   "metadata": {},
   "source": [
    "* Set the path to your image (as displayed above).\n",
    "\n",
    "* Alternatively, you can use these images that are already present in the workspace:\n",
    "  ```\n",
    "  image_path = './images/birds_sheep_dog.jpg'\n",
    "  image_path = './images/car_bus_tram.jpg'\n",
    "  image_path = './images/person_and_bicycle.jpg'\n",
    "  ```\n",
    "<br>  \n",
    "* A default path has been set for you, but feel free to change it to a different one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fd3232-c679-437d-8ebe-08fc0235499c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_path = './images/car_bus_tram.jpg' ### Add your image path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cfad8a-a8a1-4417-a32d-ee8a98e24dfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display the image\n",
    "DisplayImage(image_path, width=500, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edacb125-dea2-4e8f-b878-94b3055dcdb5",
   "metadata": {},
   "source": [
    "* As a reminder, following are the classes the model can detect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a11b17-eae1-4198-a2b5-02e631ed2565",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "detection_classes = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A',\n",
    "    'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
    "    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n",
    "    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f148125a-2e71-4d78-90e8-11233ce3fd87",
   "metadata": {},
   "source": [
    "* In the code cell below, you'll define the parameters for your object detection task:\n",
    "\n",
    "    * `target_class_names`: A list of strings for all the objects you want to detect. Each name must be an exact match from the `detection_classes` list above.\n",
    "\n",
    "    * `bbox_colors`: A corresponding list of color strings for each target class's bounding box.\n",
    "\n",
    "    * `confidence_threshold`: A value between 0.0 and 1.0 that sets the minimum confidence for a detection to be shown.\n",
    "\n",
    "* Example values have been set for you. Feel free to change these values to detect different objects in your image or to adjust the detection sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6f9c30-4a33-4373-83b1-0b70ba40dbe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a list of target classes to detect\n",
    "target_class_names = ['car', 'bus', 'train'] ### Add your target class names here\n",
    "\n",
    "# Define a corresponding list of colors for each class's bounding box\n",
    "bbox_colors = ['red', 'purple', 'green'] ### Add your target class names here\n",
    "\n",
    "# Set the confidence_threshold\n",
    "confidence_threshold = 0.7 ### Set your threshold here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414899e9-dc33-4895-91ae-1f8d1e6db805",
   "metadata": {},
   "source": [
    "* Check out the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9838b466-0b38-4318-9e50-2b16ef5d9d3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use a list comprehension to get a list of all target indices\n",
    "object_indices = [detection_classes.index(name) for name in target_class_names]\n",
    "\n",
    "# Define the label for the object same as the target_class_names\n",
    "labels = target_class_names\n",
    "\n",
    "# Execute the main detection function\n",
    "result_image_tensor = detect_and_draw_bboxes(\n",
    "    model=bb_model,                  # The pre-trained object detection model.\n",
    "    image_path=image_path,           # The path to the input image.\n",
    "    object_indices=object_indices,   # The list of integer indices for the target classes.\n",
    "    labels=labels,                   # The list of string names for the box labels.\n",
    "    bbox_colors=bbox_colors,         # The list of colors for the bounding boxes.\n",
    "    threshold=confidence_threshold,  # The minimum confidence score for a detection.\n",
    "    bbox_width=5                     # The line thickness for the bounding boxes.\n",
    ")\n",
    "\n",
    "# Display the results (feel free to set a different `figsize`)\n",
    "helper_utils.display_images(processed_image=result_image_tensor, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0e9ef4-2583-4163-9941-0fdb999d533c",
   "metadata": {},
   "source": [
    "### 2 - Generating Segmentation on Objects on your Own Images\n",
    "\n",
    "Run the function `helper_utils.upload_jpg_widget()` to display the image upload a widget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc4b4e6-bcc0-4f5c-be04-8a2c992eff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.upload_jpg_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989750be-21ca-4d9c-829c-3c7d42e08e33",
   "metadata": {},
   "source": [
    "* Set the path to your image (as displayed above).\n",
    "\n",
    "* Alternatively, you can use these images that are already present in the workspace:\n",
    "  ```\n",
    "  image_path = './images/birds_sheep_dog.jpg'\n",
    "  image_path = './images/car_bus_tram.jpg'\n",
    "  image_path = './images/person_and_bicycle.jpg'\n",
    "  ```\n",
    "<br>  \n",
    "* A default path has been set for you, but feel free to change it to a different one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25f69f4-520a-423c-b13b-e9273abe1e96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_path = './images/person_and_bicycle.jpg' ### Add your image path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf93680-02dd-4023-895c-f3f02c3ca564",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display the image\n",
    "DisplayImage(image_path, width=500, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8166a8-fff2-4277-bdb3-b5d72fb803fd",
   "metadata": {},
   "source": [
    "* As a reminder, following are the classes the segmentation model can detect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d199250f-70f7-4354-abe1-a779e12dc5ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "segmentation_classes = [\n",
    "    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car',\n",
    "    'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', \n",
    "    'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb657dcd-32dd-494c-b5b7-25a4258c51f9",
   "metadata": {},
   "source": [
    "In the code cell below, you'll define the parameters for your image segmentation task:\n",
    "\n",
    "* `target_class_names`: A list of strings for all the objects you want to segment. Each name must be an exact match from the `segmentation_classes` list above.\n",
    "\n",
    "* `seg_colors`: A corresponding list of color strings for each target class's segmentation mask.\n",
    "\n",
    "* Example values have been set for you. Feel free to change these values to segment different objects in your image or to use different colors for the masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f92d8a-60b1-4adf-80e9-b21639755e01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a list of target classes you want to find\n",
    "target_class_names = ['person', 'bicycle'] ### Add your target class name here\n",
    "\n",
    "# Define a corresponding list of colors for the segmentation masks for each class\n",
    "seg_colors = [\"pink\", 'yellow'] ### Add your segmentation masks for colors each class here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e109f197-70d7-4e5a-b679-3b88c63dd6ef",
   "metadata": {},
   "source": [
    "* Check out the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f9b7a8-5acf-46a5-9d6d-126826eb7f1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load pre-trained segmentation model\n",
    "seg_model_weights = tv_models.segmentation.DeepLabV3_ResNet50_Weights.DEFAULT\n",
    "seg_model = tv_models.segmentation.deeplabv3_resnet50(weights=seg_model_weights).eval()\n",
    "\n",
    "# Load the base PIL Image\n",
    "img = Image.open(image_path)\n",
    "\n",
    "# Create the clean, un-normalized tensor for visualization later\n",
    "original_image_tensor = transforms.ToTensor()(img)\n",
    "\n",
    "# Define the normalization transform\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# Create the normalized input tensor for the model\n",
    "input_tensor = normalize(original_image_tensor).unsqueeze(0)\n",
    "\n",
    "# Use a list comprehension to get a list of corresponding class indices\n",
    "class_indices = [segmentation_classes.index(name) for name in target_class_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee26388d-1797-48c7-a3a6-0796ced800ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate prediction\n",
    "with torch.no_grad():\n",
    "    output = seg_model(input_tensor)['out'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f958eb5-d00a-4140-b5d8-f340695dfea3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the predicted class for each pixel by finding the class with the highest score.\n",
    "output_predictions = output.argmax(0)\n",
    "\n",
    "# Create a separate boolean mask for each of your target classes.\n",
    "individual_masks = [(output_predictions == i) for i in class_indices]\n",
    "\n",
    "# Stack the individual masks into a single tensor of shape (num_masks, H, W).\n",
    "stacked_masks = torch.stack(individual_masks, dim=0)\n",
    "\n",
    "# Apply segmentation masks using the stacked_masks tensor.\n",
    "result = vutils.draw_segmentation_masks(image=(original_image_tensor * 255).byte(),\n",
    "                                        masks=stacked_masks,\n",
    "                                        alpha=0.5,\n",
    "                                        colors=seg_colors)\n",
    "\n",
    "# Visualize the mask  (feel free to set a different `figsize`)\n",
    "helper_utils.display_images(processed_image=result, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704225e4-d64b-4ec9-83b1-130d01acec71",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You've now seen a complete, practical workflow for using `torchvision` to solve common computer vision problems. This lab demonstrated the essential steps of selecting a pre-trained model, understanding its capabilities, preparing your data, and using the model for inference to get tangible results. You saw firsthand how critical visualization is, not just for final outputs, but for understanding and validating each step of the process.\n",
    "\n",
    "The most significant takeaway is the strategic advantage of leveraging pre-trained models. By building on the knowledge of state-of-the-art architectures, you are effectively harnessing thousands of hours of computation and research. This allows you to rapidly prototype and deploy highly accurate solutions without the prohibitive cost and time of training from scratch. This isn't just a shortcut; it's the standard, efficient way to build powerful vision applications.\n",
    "\n",
    "Mastering inference with pre-trained models is the essential foundation for any computer vision practitioner. With these skills, you are now perfectly positioned for the next step in your journey: learning how to adapt and customize these powerful models for your own unique datasets and specialized tasks through techniques like transfer learning and fine-tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-dlai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
