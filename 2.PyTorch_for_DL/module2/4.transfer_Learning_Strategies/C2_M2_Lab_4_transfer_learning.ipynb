{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8233f704-9c0e-4863-8a07-ec2f00d0574a",
   "metadata": {},
   "source": [
    "# Transfer Learning Strategies\n",
    "\n",
    "You have seen how to load and use pre-trained models from TorchVision for straightforward image classification. These models, trained on vast datasets like ImageNet, are experts in their domain. But what happens when you need to solve a problem outside of that domain? An expert in identifying animals and cars may be completely lost when asked to recognize handwritten digits. This lab confronts that exact challenge, showing how a powerful model can fail on a seemingly simple, out-of-domain task.\n",
    "\n",
    "This scenario highlights the need for **transfer learning**, a powerful technique for adapting a model's existing knowledge to your specific needs. Instead of training a network from scratch, you'll leverage a model's pre-trained **feature extractor**, the layers that have learned to recognize universal patterns like edges and textures. You'll then replace the model's original **classifier head** with a new one tailored to your task. The key decision is determining how much of the original model to retrain, which can range from updating only the new classifier to fine-tuning the entire network. This flexible approach saves significant training time and computational resources, making it a vital skill for any deep learning practitioner.\n",
    "\n",
    "In this lab, you will put theory into practice. You will:\n",
    "\n",
    "* Witness the limitations of a pre-trained model when faced with a new, specialized dataset.\n",
    "\n",
    "* Learn to modify the architecture of pre-trained models by replacing their final classification layer, exploring two common patterns: Direct Attribute access and Modular Block access.\n",
    "\n",
    "* Implement and compare three distinct transfer learning strategies, each offering a different balance of speed and performance:\n",
    "    * **Feature Extraction**: Freezing the model's backbone and training only the new classifier head.\n",
    "    \n",
    "    * **Fine-Tuning**: Unfreezing and training the top layers of the backbone for better adaptation.\n",
    "    \n",
    "    * **Full Retraining**: Training the entire model end-to-end for maximum performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d97885-614d-4620-b94e-30eb03797235",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150df8cd-6ad1-4b22-a4c6-81ef14ea00f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as tv_models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import helper_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4446bab4-c882-43b6-bae4-43fe3720df99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219aea0d-22c8-487b-90cf-bcb16d570780",
   "metadata": {},
   "source": [
    "## When Expertise Is Not Enough: The Limits of Pre-trained Models\n",
    "\n",
    "As you saw in the previous lab, pre-trained models are incredibly powerful. They offer a phenomenal head start, providing expert level knowledge learned from massive datasets like ImageNet. For tasks that align with their original training, such as identifying common objects in photographs, they perform brilliantly right out of the box.\n",
    "\n",
    "Let's explore this with a simple but revealing experiment. A model like [MobileNetV3](https://docs.pytorch.org/vision/main/models/mobilenetv3.html) is a certified expert, trained to identify a 1000 different classes of objects with high accuracy. It can distinguish between a 'sports car' and a 'race car'. But what happens when you ask this expert to perform a task that seems much simpler, like recognizing basic handwritten numbers and letters?\n",
    "\n",
    "To find out, you will challenge the model with the `EMNIST` dataset of handwritten digits. Although this is a straightforward dataset, it perfectly simulates a common and vital real world problem: **what do you do when a pre-trained model, for all its power, has no knowledge of the specific classes your project requires?** This is a situation you will frequently encounter, and the principle you are about to see applies to any custom task, no matter how complex.\n",
    "\n",
    "* Define the transformation pipeline for the `EMNIST` dataset. Since the dataset is simple, you can use the same for both, training and validation datasets. \n",
    "\n",
    "* Remember, when using a pre-trained model, it is an essential practice to format your input data to match the configuration the model was originally trained on. A mismatch can lead to poor performance or errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec187286-5904-405d-a5ea-5d07fe4b5fd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the transformation pipeline\n",
    "emnist_transformation = transforms.Compose([\n",
    "    # Convert grayscale image to 3 channels to match MobileNetV2's input\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    # Resize the image to 224x224, the standard input size for MobileNetV2\n",
    "    transforms.Resize((224, 224)),\n",
    "    # Apply the 90-degree rotation augmentation\n",
    "    transforms.RandomRotation(degrees=(90, 90)),\n",
    "    # Apply the vertical flip augmentation\n",
    "    transforms.RandomVerticalFlip(p=1.0),\n",
    "    # Convert the image to a PyTorch Tensor\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize the tensor using ImageNet's mean and standard deviation\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe17209-f575-47b5-8765-199e7f393971",
   "metadata": {},
   "source": [
    "* Next, create the training and validation data loaders from the `EMNIST` dataset's `digits` split, applying the transformation pipeline you just defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a97c6e0-5001-44d6-bf2b-c22b211ce9bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the training and validation DataLoaders.\n",
    "train_loader, val_loader = helper_utils.create_emnist_dataloaders(\n",
    "    batch_size=32,\n",
    "    transform=emnist_transformation  # Apply the defined transformations\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b67903e-b575-471c-93b5-a33e29a41345",
   "metadata": {},
   "source": [
    "* Load the `MobileNetV3` Small model with its `'IMAGENET1K_V1'` pre-trained weights and set it to evaluation mode using `.eval()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99363172-d3ee-4643-9d3e-6f49a6387dda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the pre-trained MobileNetV3 model and set it to evaluation mode for inference\n",
    "mobilenet_model = tv_models.mobilenet_v3_small(weights='IMAGENET1K_V1').eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee38b5f-eb89-425a-a808-34271537fbcc",
   "metadata": {},
   "source": [
    "* Since you already know this model was trained on the standard ImageNet dataset, you can take a shortcut and load the 1,000 class names directly from `./imagenet_class_index.json`, the same file you used in the previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca64c3e-08ec-49f8-a817-9d878a0777cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mapping of class indices to human-readable names from the JSON file\n",
    "class_names = helper_utils.load_imagenet_classes('./imagenet_class_index.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4b71e2-b6c3-4d89-a407-f7acecec2372",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* Now for the moment of truth: pass the model and data loader to the helper function to visualize the model's predictions on these unfamiliar images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236919af-b855-45b8-b3f2-83576d2a6ba6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the model's predictions on the validation images\n",
    "helper_utils.show_predictions(mobilenet_model, val_loader, device, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74450a8a-cafa-4a52-bafe-3b1005300580",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Well, that's surprising. A model smart enough to identify 1,000 different objects looks at these numbers and sees a spatula or a nematode, among other things. It seems that the model has a very active imagination! \n",
    "\n",
    "As you can see, the predictions are not just slightly off; they are completely out of context. This happens because the model is doing its best to match these unfamiliar shapes to the closest visual patterns it learned from its ImageNet training. It has no concept of numbers, so it finds the next best thing in its vocabulary.\n",
    "\n",
    "This perfectly illustrates the core problem: a pre-trained model's knowledge is powerful but limited to its original domain. So, how do you teach an expert model a new skill? The solution is a powerful technique called **transfer learning**. In the next section, you'll learn how to take this model's existing knowledge and adapt it to correctly identify the very numbers it just failed to recognize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec91c04-921c-4193-aa38-a791dd24fc44",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "Using a pre-trained model for inference is powerful, but its true potential is unlocked when you adapt it for a new, custom task. This process, called **transfer learning**, allows you to leverage a model's expert knowledge of general shapes and patterns to achieve excellent results on your specific dataset with significantly less data and training time.\n",
    "\n",
    "The core idea is to treat the pre-trained model as two distinct parts:\n",
    "\n",
    "* The **Feature Extractor** (or \"backbone\"): These are the early layers of the network that have learned to recognize universal visual features like edges, textures, and shapes.\n",
    "\n",
    "* The **Classifier Head**: This is the final layer (or layers) that takes the features from the backbone and makes the final prediction.\n",
    "\n",
    "In transfer learning, you will always replace the original classifier head with a new one designed for your specific number of classes. The main question then becomes: **how much of the feature extractor should you train?** This choice gives rise to three primary strategies, each offering a different trade off between training time and performance. You will see these strategies in action.\n",
    "\n",
    "### Strategy 1: Feature Extraction (Train Only the New Head)\n",
    "\n",
    "This is the fastest and most computationally efficient approach. You **freeze** the entire backbone and only train the new classifier head. You are effectively using the pre-trained model as a fixed feature extractor, trusting its existing knowledge completely. This is an excellent starting point, especially when your dataset is small.\n",
    "\n",
    "While this strategy focuses on training *only* the classifier head, the practical skill of identifying and replacing it is a foundational step for **any** transfer learning approach you will use. The exact method depends on the model's architecture, which typically follows one of two patterns: a **Direct Attribute** (like in ResNet) or a **Modular Block** (like in MobileNet). You will now explore both, starting with the Direct Attribute pattern.\n",
    "\n",
    "#### Direct Attribute\n",
    "\n",
    "Let's start with the Direct Attribute pattern. For this, you'll use [ResNet18](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html).\n",
    "\n",
    "* From the [weights of this particular model](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html#torchvision.models.ResNet18_Weights), you’ll use `IMAGENET1K_V1`. \n",
    "* Next, you load the `ResNet18` model with these specified weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dce6436-2e8c-474d-b894-b459bab790ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the ResNet18 model architecture and load the selected weights\n",
    "resnet18_model = tv_models.resnet18(weights='IMAGENET1K_V1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e626c5-005b-483c-ad68-1a7479d7dc4a",
   "metadata": {},
   "source": [
    "* Now, inspect the model's architecture.\n",
    "* To do this, you can print the entire model. The output will be verbose as it lists every layer in the network. \n",
    "\n",
    "```\n",
    "ResNet(\n",
    "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (relu): ReLU(inplace=True)\n",
    "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
    "  ...\n",
    "    \n",
    "  )\n",
    "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3374d005-6244-4c84-88d2-d698f1d6a6df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ### Uncomment and execute the line below if you wish print the model's architecture.\n",
    "\n",
    "# print(resnet18_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfe7cd2-2eba-4c43-ba66-a2e508b68b10",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Note these key architectural details that inform how you'll modify the model.**\n",
    "\n",
    "* Notice how all the main components (`conv1`, `layer1`, `fc`, etc.) are direct attributes of the main `ResNet` model. There isn't a single, high-level block called `features` that groups all the convolutional layers together. This \"flat\" structure tells you that the simplest approach to freeze all parameters at once is with a single loop over the model's parameters (`resnet18_model.parameters()`).\n",
    "\n",
    "* In transfer learning, you always replace the model's final **classifier head**. By inspecting the end of the output, you can identify the final layer, which is a `Linear` layer named `fc`.\n",
    "\n",
    "* `out_features=1000`: Corresponds to the original ImageNet classes. You will replace this layer to match the number of classes for your new task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e25656f-1aa8-4eed-b617-9ccf85f20ea6",
   "metadata": {},
   "source": [
    "Now that you understand the model's architecture, begin by **freezing** the layers of the pre-trained model. This step ensures that the model retains the valuable features it learned from its initial training.\n",
    "\n",
    "* Iterate over each parameter in `resnet18_model` using `resnet18_model.parameters()`.\n",
    "* For each `param`, set `param.requires_grad=False`. This prevents gradients from being computed for these parameters during fine-tuning the model for your specific task, effectively freezing their weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f23e797-5f34-4ac5-ab3c-a4310afc1308",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Iterate over each parameter in the resnet18_model\n",
    "for param in resnet18_model.parameters():\n",
    "    # Set the requires_grad attribute of each parameter to False to freeze it\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4404f8-9a51-41ab-9477-299fd3ee4828",
   "metadata": {},
   "source": [
    "* After freezing the earlier layers, the next step is to modify the **classifier head** to suit your new task. As identified earlier, for `ResNet18`, this is the `fc` (fully connected) layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde4b9c4-a5db-438f-a015-9a02baa869eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Access the fully connected layer (fc) of the resnet18_model\n",
    "original_fc_layer = resnet18_model.fc\n",
    "\n",
    "print(\"Model's Original Fully Connected Layer:\")\n",
    "print(original_fc_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f009ca-8360-445d-9ba9-4a6133b92794",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* The final step is to replace model's original classifier head with a new one tailored for your specific classification task. Let's assume, for your new task, the number of classes you want is `5`.\n",
    "    \n",
    "    * `num_features = original_fc_layer.in_features`: This retrieves the **input feature dimension** of the original `fc` layer (`in_features=512`). This value represents the output size of the pre-trained feature extractor layers, and your new classifier **must** be able to accept inputs of this size.\n",
    "    \n",
    "    * `num_classes=5`: Here, you define the number of classes for your new task.\n",
    "    \n",
    "    * `new_fc_layer = nn.Linear(in_features=num_features, out_features=num_classes)`: A new `Linear` (fully connected) layer is created. It's configured to accept the `num_features` from the frozen backbone and output a prediction for each of your `num_classes`.\n",
    "    \n",
    "    * `resnet18_model.fc = new_fc_layer`: This is where the replacement happens! You directly assign your `new_fc_layer` to the `fc` attribute of the `resnet18_model`, effectively swapping out the old ImageNet classifier for your custom one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe07bf0-d21e-4ceb-8f5c-d07bcac19ecc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the number of input features to the original fully connected layer\n",
    "# This is stored in the 'in_features' attribute of the linear layer\n",
    "num_features = original_fc_layer.in_features\n",
    "\n",
    "# Define the number of output classes for the new classification task\n",
    "num_classes = 5\n",
    "\n",
    "# Create a new fully connected layer (Linear layer)\n",
    "new_fc_layer = nn.Linear(in_features=num_features, out_features=num_classes)\n",
    "\n",
    "# Replace the original fully connected layer of resnet18_model with the new_fc_layer\n",
    "resnet18_model.fc = new_fc_layer\n",
    "\n",
    "print(\"Model's New Fully Connected Layer:\")\n",
    "print(resnet18_model.fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7431258d-965f-49ab-8a3c-755d0a9eadb1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "And with that, you have successfully reconfigured the `ResNet` model for a new task! This process, freezing the feature extractor and swapping the classifier, is a cornerstone of transfer learning. You are now ready to apply this same core principle to the next common architectural pattern: the Modular Block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f18609-b2b6-4e13-94b5-9a5634c6fe38",
   "metadata": {},
   "source": [
    "#### Modular Block\n",
    "\n",
    "Now let's look at the Modular Block pattern. For this, you'll use [MobileNetV3](https://docs.pytorch.org/vision/main/models/mobilenetv3.html) Small.\n",
    "\n",
    "* From the [weights of this particular model](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.mobilenet_v3_small.html#torchvision.models.MobileNet_V3_Small_Weights), you’ll use `IMAGENET1K_V1`.\n",
    "* Next, you load the `MobileNetV3` Small model with these specified weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cf163b-cf88-4031-9fb0-f94633cdf085",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the MobileNetV3 Small architecture and load the selected weights\n",
    "mobilenet_model = tv_models.mobilenet_v3_small(weights='IMAGENET1K_V1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a63a64-ab94-4dec-8593-00fddbf0f6c7",
   "metadata": {},
   "source": [
    "* Now, inspect the model's architecture.\n",
    "* To do this, you can print the entire model. The output will be verbose as it lists every layer in the network. \n",
    "\n",
    "```\n",
    "MobileNetV3(\n",
    "  (features): Sequential(\n",
    "    (0): Conv2dNormActivation(\n",
    "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (2): Hardswish()\n",
    "    )\n",
    " ...\n",
    "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
    "  (classifier): Sequential(\n",
    "    (0): Linear(in_features=576, out_features=1024, bias=True)\n",
    "    (1): Hardswish()\n",
    "    (2): Dropout(p=0.2, inplace=True)\n",
    "    (3): Linear(in_features=1024, out_features=1000, bias=True)\n",
    "  )\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543b2ace-a2d7-4e42-bd09-6c46b50f6b60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ### Uncomment and execute the line below if you wish to print the model's architecture.\n",
    "\n",
    "# print(mobilenet_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba44fffd-8076-4578-9948-66b45f970a8c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Note these key architectural details that inform how you'll modify the model.**\n",
    "\n",
    "* **Notice the modular structure**: The convolutional layers are grouped within a `Sequential` block named `features`, and the final prediction layers are contained within **another** `Sequential` block named `classifier`. This modularity means you'll freeze parameters by iterating over the parameters within the features (`mobilenet_model.features.parameters()`) for the backbone and specifically target the classifier (`mobilenet_model.classifier`) for modification.\n",
    "\n",
    "* By inspecting the end of the output, you can identify the final classification layer as the **last** `Linear` layer (labeled `(3)`) inside the `classifier` `Sequential` block.\n",
    "\n",
    "* `out_features=1000`: Corresponds to the original ImageNet classes. You will replace this layer to match the number of classes for your new task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b374d7-e692-40b7-a2c7-5c4855af1980",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Same as before, you'll **freeze** the earlier layers. However, due to `MobileNetV3` Small's modular structure, you'll specifically target its `features` block.\n",
    "\n",
    "* Iterate over each parameter in `mobilenet_model.features` using `mobilenet_model.features.parameters()`.\n",
    "* For each `feature_parameter`, `set feature_parameter.requires_grad = False`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2792f4-a30b-471e-abbb-6b620c36a58b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Iterate through each parameter in model.features.parameters()\n",
    "for feature_parameter in mobilenet_model.features.parameters():\n",
    "    # Set the requires_grad attribute of each feature_parameter to False\n",
    "    feature_parameter.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc8ff46-bf88-4a03-989f-1de040b28f5b",
   "metadata": {},
   "source": [
    "As before, the next step is to modify the classifier head. Unlike ResNet's direct `fc` attribute, `MobileNetV3`'s classifier is a `Sequential` block. \n",
    "\n",
    "* To access the final linear layer within this block (which performs the actual classification), use `mobilenet_model.classifier[-1]` to directly select the last element in the `classifier`'s sequence of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eab81e-520c-4d03-996c-1cdd07c21f74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Access the final classification layer of the model\n",
    "last_classifier_layer = mobilenet_model.classifier[-1]\n",
    "\n",
    "print(\"Model's Original Output Layer:\")\n",
    "print(last_classifier_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6fc740-07dd-4148-bd2f-f13a20b7c162",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Same as before, the final step is to replace the model's original classifier head with a new one tailored for your specific classification task. Let's assume your new task requires `10` classes. Everything happens the same, except:\n",
    "\n",
    "* `mobilenet_model.classifier[-1]=new_classifier`: Since the classifier is a Sequential block, you directly assign your `new_classifier` to the last element (`[-1]`) of `mobilenet_model.classifier`, effectively swapping out the old ImageNet classifier for your custom one. This new layer will be the only part of the model that learns during fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8fcde3-7472-4b3d-992e-1d4ab0343b0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Access the in_features attribute of last_classifier_layer\n",
    "num_features = last_classifier_layer.in_features\n",
    "\n",
    "# Define the number of output classes for the new classification task\n",
    "num_classes = 10\n",
    "\n",
    "# Create a new Linear layer for classification\n",
    "new_classifier = nn.Linear(in_features=num_features, out_features=num_classes)\n",
    "\n",
    "# Replace the original last classification layer with the newly created layer\n",
    "mobilenet_model.classifier[-1] = new_classifier\n",
    "\n",
    "print(\"Model's New Output Layer:\")\n",
    "print(mobilenet_model.classifier[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a846fc5-d067-429e-b6ac-f94d004cee8c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "And that's how you prepare a model for transfer learning when it has a modular architecture!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32285cae-af39-4017-8df6-fdb1c2349f76",
   "metadata": {},
   "source": [
    "#### Training the Classifier Head\n",
    "\n",
    "It's time to put **Strategy 1 (Feature Extraction)** into practice by solving the initial `EMNIST` challenge. You will use the `mobilenet_model` you already prepared; its new 10-class classifier head is a perfect match for the 10 digit classes in the `EMNIST` dataset. Since the model and data loaders are ready, the first step is to define the loss function and optimizer.\n",
    "\n",
    "* Define the loss function and optimizer.\n",
    "* `filter(lambda p: p.requires_grad, mobilenet_model.parameters()`: Notice that you are **only** passing the parameters of the new *classifier head* to the optimizer.\n",
    "    * This selects only the model parameters that require gradients (i.e., are not frozen). This ensures that you exclusively train the new, final layer for your specific task, which is the central principle of this feature extraction approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d316b051-ddb0-4982-b1b9-7384913b774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "# Only optimize the parameters that require gradients for mobilenet_model\n",
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, \n",
    "                                   mobilenet_model.parameters()),\n",
    "                            lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eb68c8-c01d-4759-87c5-e0a998dcef36",
   "metadata": {},
   "source": [
    "* Finally, pass all the components, the model, data loaders, loss function, and optimizer to the `training_loop` function to start the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad22d16-0391-4f0c-a4ad-c470e9dd2d48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of epochs for new classifier head training\n",
    "num_epochs = 1\n",
    "\n",
    "# Start the training.\n",
    "trained_model = helper_utils.training_loop(\n",
    "    model=mobilenet_model, \n",
    "    trainloader=train_loader, \n",
    "    valloader=val_loader, \n",
    "    loss_function=loss_function, \n",
    "    optimizer=optimizer, \n",
    "    num_epochs=num_epochs, \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53f72c6-9e51-4869-8322-8dcf0230e783",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "With just a single epoch of training, the model achieved an impressive validation accuracy of over 80%.\n",
    "\n",
    "* Now, visualize the model's new predictions to see this improved performance in action.\n",
    "    * Since you replaced the model's original classifier head with one trained on `EMNIST` digits, the old ImageNet class names are no longer valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3da018-1138-4c71-86fa-cf0e8e37dbcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a list of class names for the EMNIST digits (0-9).\n",
    "emnist_class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "# Visualize the model's predictions on the validation images\n",
    "helper_utils.show_predictions(trained_model, val_loader, device, emnist_class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9aad09-0212-4ba2-9ecc-2468df50e051",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "As you've seen, training only the classifier head is a fast and effective way to get strong initial results. However, to push your model's performance even further, you can move beyond simple feature extraction and begin to fine-tune the backbone itself. This leads to the second, more powerful strategy.\n",
    "\n",
    "### Strategy 2: Fine-Tuning (Train the Head and Top Layers)\n",
    "\n",
    "This highly effective and popular strategy serves as a middle ground, offering better performance than feature extraction without the cost of retraining the entire model. When you decide to fine-tune, you have two primary options:\n",
    "\n",
    "* **Single Stage Training**: A straightforward approach where you unfreeze the desired top layers of the backbone and train them simultaneously with the new classifier head, typically using a single learning rate.\n",
    "\n",
    "* **Two Stage Training**: A more careful and often more effective approach that separates the training process into two distinct stages to gently adapt the model.\n",
    "\n",
    "    The two stage strategy works as follows:\n",
    "\n",
    "    * **Stage 1 (Train the Head)**: You begin exactly as you did in Strategy 1 by training *only* the new classifier head while the backbone remains frozen. This quickly teaches the new head how to make decisions based on the general features the backbone provides.\n",
    "\n",
    "    * **Stage 2 (Unfreeze and Fine-Tune)**: After the head is trained, you **unfreeze** the last few layers of the backbone. Then, you continue training both the unfrozen layers and the head together, but with a much lower learning rate. This critical step allows the model's more specialized, high level feature detectors to gently adapt to the specifics of your new dataset without drastically altering their valuable pre trained knowledge.\n",
    "\n",
    "Now, you will demonstrate the powerful **two stage strategy**. Since you have already completed Stage 1 by training the classifier head on your `trained_model` (originally `mobilenet_model`), you are perfectly set up to proceed. You will now take that model and perform Stage 2: unfreezing the top layers of the backbone and fine-tuning it with a lower learning rate.\n",
    "\n",
    "#### Unfreezing the Top Layers\n",
    "\n",
    "The first step of Stage 2 is to unfreeze the last few layers of the model's backbone. The earlier layers of the backbone learned to detect general patterns like edges and colors, while the later, \"top\" layers learned more complex, high-level features. By unfreezing only these top layers, you allow the model to adapt its most specialized feature detectors to the specifics of the `EMNIST` dataset.\n",
    "\n",
    "##### How to Identify the Top Layers to Unfreeze\n",
    "\n",
    "Just like finding the classifier head, identifying the top layers requires you to inspect the model's architecture by printing it (`print(model)`). You are looking for the last few major convolutional blocks just before the pooling layer and the final classifier. The approach varies slightly depending on the model's structure.\n",
    "\n",
    "* **For Modular Models (like MobileNetV3)**: You'll look inside the main backbone block, which in this case is `features`. Since `features` is a `Sequential` module, the \"top layers\" are simply the last blocks in that sequence. Looking at the `MobileNetV3` architecture, the `features` block ends like this:\n",
    "    ```\n",
    "      ...\n",
    "      (11): InvertedResidual(...)\n",
    "      (12): Conv2dNormActivation(...)\n",
    "    )\n",
    "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
    "    (classifier): Sequential(...)\n",
    "    ```\n",
    "    <br>\n",
    "    The last block, indexed at <code>[12]</code>, is the topmost layer of the feature extractor. A common fine-tuning strategy is to unfreeze just this block, or perhaps the last two or three. For this demonstration, you will unfreeze the final block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd605889-e5c0-4f43-9155-26efa7d4ccba",
   "metadata": {},
   "source": [
    "* **For Flat Models (like ResNet)**: In an architecture like `ResNet`, you would look for the last named layer blocks. These are typically `layer1`, `layer2`, `layer3`, and `layer4`. The top layers would be `layer4`, or you might choose to unfreeze both `layer3` and `layer4`. The investigation process is the same: print the model, identify the final feature-extracting blocks by name (`model.layer4`), and then target their parameters for unfreezing.\n",
    "\n",
    "Now that you know how to identify the target layers, you will proceed with unfreezing the last block of your `trained_model` (originally `mobilenet_model`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297dd73a-9fc5-4575-bba9-1d86dcf08f9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ### Uncomment and execute the line below if you wish to print the model's architecture.\n",
    "\n",
    "# print(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae49cec7-0f86-4c0f-9b49-3d6dbe80bbf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The model from the previous training stage\n",
    "fine_tune_model = trained_model\n",
    "\n",
    "# Unfreeze the parameters of the last block in the 'features' section\n",
    "for param in fine_tune_model.features[12].parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f219a70-d2be-4ac1-aebd-134c5dc4d619",
   "metadata": {},
   "source": [
    "* It's an excellent practice to verify that the correct layers have been frozen and unfrozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16521b97-8164-4745-8e1d-3066ad1bf137",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify that the parameters of an early block (e.g., features[0]) are frozen\n",
    "print(f\"Parameters in features[0] are frozen:       {not fine_tune_model.features[0][0].weight.requires_grad}\")\n",
    "\n",
    "# Verify that the parameters of a late block (e.g., features[12]) are now unfrozen\n",
    "print(f\"Parameters in features[12] are unfrozen:    {fine_tune_model.features[12][0].weight.requires_grad}\")\n",
    "\n",
    "# Verify that the classifier head remains unfrozen and trainable\n",
    "print(f\"Parameters in the classifier are unfrozen:  {fine_tune_model.classifier[-1].weight.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e6f5fa-3e94-4eb4-a5dd-baea9b6bf7c3",
   "metadata": {},
   "source": [
    "#### Continuing the Training (Stage 2)\n",
    "\n",
    "* Now that you have more trainable layers, you must create a new optimizer. A lower learning rate is essential for this stage. Using a small learning rate ensures that the updates to the pre-trained weights are small and careful, refining their knowledge rather than destroying it.\n",
    "    * The optimizer will again be configured to only update the parameters where `requires_grad` is `True`, but this now includes both the classifier head and the unfrozen top layers of the backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b10b3a-f916-483b-9fc8-c20662a0d9e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new optimizer that targets all trainable parameters\n",
    "optimizer = torch.optim.SGD(\n",
    "    filter(lambda p: p.requires_grad, fine_tune_model.parameters()),\n",
    "    lr=1e-5  # A new, lower learning rate for fine-tuning\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1acca7-1e7b-44fa-b9f7-34ed3f66da0b",
   "metadata": {},
   "source": [
    "* Finally, pass all the components, the model, data loaders, loss function, and optimizer to the `training_loop` function to start the training.\n",
    "    * This time, the gradients will flow through and update not only the classifier head but also the top layers of the feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10964d49-1ee5-40c2-a322-21a1409b8d3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of epochs for the fine-tuning stage\n",
    "num_epochs_fine_tune = 1\n",
    "\n",
    "# Continue training the model\n",
    "fine_tune_trained_model = helper_utils.training_loop(\n",
    "    model=fine_tune_model,\n",
    "    trainloader=train_loader,\n",
    "    valloader=val_loader,\n",
    "    loss_function=loss_function,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs_fine_tune,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e922cbd2-caa4-439a-8b37-6a8cf342dc2d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "After fine-tuning, depending on how many epochs you ran the training for, you should see a boost in validation accuracy, surpassing the performance from the first stage. You likely also noticed that each epoch took longer; this is expected as the optimizer now updates the weights for both the head and the unfrozen backbone layers. This highlights the essential trade-off of fine-tuning: investing more computation time to achieve higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5d5df2-6d36-40f5-b83d-7b5e88bebdd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ### Uncomment and execute the line below if you wish to visualize predictions\n",
    "\n",
    "# # Visualize the model's predictions on the validation images\n",
    "# helper_utils.show_predictions(fine_tune_trained_model, val_loader, device, emnist_class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06205b0e-25fe-479c-8421-5dcb8e077cba",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "You have seen how to freeze the entire backbone and how to fine-tune just the top layers. The final strategy gives you complete control by allowing the entire model to adapt to your new dataset.\n",
    "\n",
    "### Strategy 3: Full Retraining (Train the Entire Model)\n",
    "\n",
    "This is the most computationally intensive approach and is essentially a continuation of fine-tuning. After replacing the classifier head, you unfreeze **all** layers in the backbone and retrain the entire model end to end on your new data. This strategy is most effective when you have a large dataset and want to adapt every pre-trained parameter to the specific patterns of your new task.\n",
    "\n",
    "For full retraining, the entire model must be trainable. When you load a pre-trained model and replace its classifier head, **all layers are in a trainable state by default**. You only need to freeze layers if you specifically choose to, as you did in Strategies 1 and 2.\n",
    "\n",
    "Since you are continuing with the model from the previous fine-tuning stage, which still has frozen layers, you must first unfreeze the entire backbone. \n",
    "\n",
    "* You'll iterate through all of the model's parameters and set their `requires_grad` attribute back to `True`, ensuring the entire model will be updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9065b5-b12e-49ca-9a92-aace9a174208",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The model from the previous fine-tuning stage\n",
    "full_retrain_model = fine_tune_trained_model\n",
    "\n",
    "# Unfreeze all parameters in the model\n",
    "for param in full_retrain_model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf418b9-b4c1-4c6d-8360-eb4b68d60c3d",
   "metadata": {},
   "source": [
    "* As a quick check, you can verify that a parameter from an early layer, which was previously frozen, is now trainable and ready for updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383429a1-0ae3-4628-8e76-28f045d325d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify that an early layer is now unfrozen and trainable\n",
    "print(f\"Parameters in features[0] are unfrozen: {full_retrain_model.features[0][0].weight.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408aa730-e79f-4a64-8fef-fb6337744244",
   "metadata": {},
   "source": [
    "#### Full Training\n",
    "\n",
    "* For this final stage, you will create another optimizer. Unlike the previous stages where you selectively passed only the trainable parameters to the optimizer, this time you will pass `full_retrain_model.parameters()`. This simple change instructs the optimizer to calculate gradients and update the weights for **every single parameter** in the entire model, which is the core of the full retraining strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0485b08-1d3e-4238-ac56-89d5b9248b6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The optimizer now targets all parameters in the model\n",
    "optimizer = torch.optim.SGD(\n",
    "    full_retrain_model.parameters(),\n",
    "    lr=1e-4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5329bf-c531-4e7a-9d80-6af90510bdf1",
   "metadata": {},
   "source": [
    "* Finally, pass all the components to the `training_loop` function to start the final stage of training.\n",
    "    * During this process, the gradients will flow through and update the weights of **every single layer**, adapting the entire model from the earliest feature detectors to the final classifier head for your new task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bffff3-d77b-43e2-91c3-f0ca7a34aa6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of epochs for the full retraining stage\n",
    "num_epochs_full_retrain = 1\n",
    "\n",
    "# Continue training the entire model\n",
    "final_model = helper_utils.training_loop(\n",
    "    model=full_retrain_model,\n",
    "    trainloader=train_loader,\n",
    "    valloader=val_loader,\n",
    "    loss_function=loss_function,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs_full_retrain,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfe718b-f066-4d68-b67d-f0ce8cb201a5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "After this final training stage, you may see another boost in validation accuracy, potentially achieving the best performance of all three strategies. This stage was also the most computationally intensive; with every layer unfrozen, the optimizer had to calculate gradients and update weights for the entire model.\n",
    "\n",
    "This highlights the core trade off of the full retraining approach. By investing the maximum amount of computation, you give the model complete freedom to adapt its entire learned representation to your new dataset, which can unlock the highest level of accuracy for complex tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9206d90e-dce1-4a35-a26b-0a753b4fa72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Uncomment and execute the line below if you wish to visualize predictions\n",
    "\n",
    "# # Visualize the model's predictions on the validation images\n",
    "# helper_utils.show_predictions(final_model, val_loader, device, emnist_class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3483e52-61fe-4dc6-b835-55d24ed3c4f9",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Remember, the goal of demonstration of these different stategies was not to declare one strategy superior to another. Instead, view these three approaches, feature extraction, fine-tuning, and full retraining, as powerful and distinct tools in your professional toolkit. The best choice always depends on the unique demands of your project, including dataset size, available computing power, and performance requirements. You are now equipped to make that strategic decision and apply the right technique to your own work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5a3a9c-c80e-4614-aff6-c448aa0cb747",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You have successfully navigated the complete workflow of adapting a pre-trained model for a custom task. Starting with a model that produced nonsensical predictions, you applied a series of increasingly powerful techniques to teach it a new skill. You have not only learned the theory but have now implemented the three primary strategies in transfer learning. You started with **feature extraction**, the fastest method, by freezing the backbone and training only a new classifier head. Then, you moved to **fine-tuning**, a balanced approach where you unfroze and carefully trained the top layers of the backbone for a boost in performance. Finally, you explored **full retraining**, the most computationally intensive strategy, which allows the entire model to adapt to the new data.\n",
    "\n",
    "The journey through these strategies demonstrates an essential lesson: there is no single \"best\" approach. Your choice will always depend on the unique demands of your project. For a small dataset or quick prototyping, feature extraction is an excellent starting point. When performance is paramount and you have sufficient data, fine-tuning or full retraining are more suitable options. You are now equipped with a versatile and powerful toolkit, ready to make strategic decisions and effectively apply transfer learning to your own computer vision challenges."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-dlai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
