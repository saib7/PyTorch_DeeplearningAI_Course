{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment - FakeFinder: Upgrading the Expedition with Transfer Learning\n",
    "\n",
    "Welcome back, AI Explorer! In your first expedition, you successfully built a **FakeFinder** from the ground up, navigating the complex hyperparameter landscape with Optuna to engineer a custom CNN architecture. You learned the fundamental principles of model construction and optimization, an essential skill for any deep learning practitioner.\n",
    "\n",
    "Now, it's time to upgrade your toolkit for the next leg of your journey. In the real world, explorers don't always build their vehicles from scratch; they often retrofit a powerful, pre-existing engine to achieve superior performance with greater efficiency. That is your mission in this assignment: to leverage the power of **transfer learning**.\n",
    "\n",
    "You will harness the knowledge of a veteran model, **MobileNetV3-Large**, which has already been trained on the vast ImageNet dataset. By adapting this pre-trained model to our specific `AIvsReal` dataset, you will see a dramatic improvement in performance and speed.\n",
    "\n",
    "Throughout this upgraded expedition, you will:\n",
    "\n",
    "* Prepare your familiar image dataset with transformation pipelines optimized for pre-trained models.\n",
    "* Understand the core principles of transfer learning and its pivotal role in modern computer vision.\n",
    "* Load a powerful pre-trained model and adapt it for a new task using **feature extraction**.\n",
    "* Freeze the convolutional base and replace the final classifier layer to specialize the model for fake image detection.\n",
    "* Train your adapted model and compare its remarkable results to the custom model from your first assignment.\n",
    "\n",
    "Get ready to see how leveraging pre-trained wisdom can accelerate your path to building highly effective, state of the art models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='submission'></a>\n",
    "\n",
    "<h4 style=\"color:green; font-weight:bold;\">TIPS FOR SUCCESSFUL GRADING OF YOUR ASSIGNMENT:</h4>\n",
    "\n",
    "* All cells are frozen except for the ones where you need to submit your solutions or when explicitly mentioned you can interact with it.\n",
    "\n",
    "* In each exercise cell, look for comments `### START CODE HERE ###` and `### END CODE HERE ###`. These show you where to write the solution code. **Do not add or change any code that is outside these comments**.\n",
    "\n",
    "* You can add new cells to experiment but these will be omitted by the grader, so don't rely on newly created cells to host your solution code, use the provided places for this.\n",
    "\n",
    "* Avoid using global variables unless you absolutely have to. The grader tests your code in an isolated environment without running all cells from the top. As a result, global variables may be unavailable when scoring your submission. Global variables that are meant to be used will be defined in UPPERCASE.\n",
    "\n",
    "* To submit your notebook for grading, first save it by clicking the ðŸ’¾ icon on the top left of the page and then click on the `Submit assignment` button on the top right of the page.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Imports](#0)\n",
    "- [1 - Preparing for the Next Expedition: A Quick Reconnaissance](#1)\n",
    "    - [1.1 - Scouting the Archives: Loading Data with ImageFolder](#1-1)\n",
    "        - **[Exercise 1 - create_dataset_splits](#ex-1)**    \n",
    "    - [1.2 - Specialized Augmentations: Protocols for Training and Validation](#1-2)\n",
    "        - **[Exercise 2 - define_transformations](#ex-2)**\n",
    "    - [1.3 - Preparing the Data Launchpad: Applying Transformations and Batching Data](#1-3)\n",
    "        - **[Exercise 3 - create_data_loaders](#ex-3)**    \n",
    "- [2 - Advanced Expedition: Leveraging Pre-trained Model Wisdom](#2)\n",
    "    - [2.1 - Selecting Your Champion: Introducing MobileNetV3-Large](#2-1)\n",
    "        - **[Exercise 4.1 - load_mobilenetv3_model](#ex-41)**\n",
    "    - [2.2 - Tailoring Your Champion: Adapting MobileNetV3 for Fake Detection](#2-2)\n",
    "        - **[Exercise 4.2 - update_model_last_layer](#ex-42)**\n",
    "- [3 - Final Expedition: Training and Evaluating Your Adapted Champion](#3)\n",
    "    - [3.1 - Setting the Launchpad: Preparing DataLoaders and the Adapted Model](#3-1)\n",
    "    - [3.2 - Training the Adapted Model](#3-2)\n",
    "    - [3.3 - Performance Review: Visualizing Predictions](#3-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='0'></a>\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "7q46o9gkZW5l",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as tv_models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import helper_utils\n",
    "import unittests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Preparing for the Next Expedition: A Quick Reconnaissance\n",
    "\n",
    "This assignment continues from the previous one, using the same curated collection of images from the [AI-Generated Images vs Real Images](https://www.kaggle.com/datasets/tristanzhang32/ai-generated-images-vs-real-images) dataset. \n",
    "\n",
    "As a reminder, this dataset is a sample from a larger collection of 60,000 images. Half are generated by advanced AI models (Stable Diffusion, MidJourney, and DALLÂ·E), and the other half are genuine images from sources like Pexels and Unsplash. You will work with the same carefully selected subset of **5,000 images for training** and **1,000 for testing/validation**.\n",
    "\n",
    "The data organization is the same as before. Images are sorted into `train` and `test` folders, each containing `real` and `fake` subdirectories. This logical structure is fundamental to understand, as it directly influences how you will load the data for the upcoming tasks.\n",
    "\n",
    "Hereâ€™s a reminder of the data layout:\n",
    "\n",
    "```\n",
    "./AIvsReal_sampled/\n",
    "â””â”€â”€â”€train/\n",
    "|   â”œâ”€â”€â”€fake/\n",
    "|   â”‚   â”œâ”€â”€â”€0046.jpg\n",
    "|   â”‚   â””â”€â”€â”€...\n",
    "|   â””â”€â”€â”€real/\n",
    "|       â”œâ”€â”€â”€0046.jpg\n",
    "|       â””â”€â”€â”€...\n",
    "â””â”€â”€â”€test/\n",
    "    â”œâ”€â”€â”€fake/\n",
    "    â”‚   â”œâ”€â”€â”€0046.jpg\n",
    "    â”‚   â””â”€â”€â”€...\n",
    "    â””â”€â”€â”€real/\n",
    "        â”œâ”€â”€â”€0046.jpg\n",
    "        â””â”€â”€â”€...\n",
    "```\n",
    "\n",
    "* Run the next two cells to define the path to the dataset and view some samples from the training folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "GvFE4XN1LAPW",
    "outputId": "8b17c55f-fbc5-4b86-90a5-4eb5cab9bcb7"
   },
   "outputs": [],
   "source": [
    "# Load the dataset path\n",
    "dataset_path = \"./AIvsReal_sampled\"  \n",
    "\n",
    "# Analyzes the dataset splits at the given path and prints a count of images for each class.\n",
    "helper_utils.dataset_images_per_class(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: For the remainder of this assignment, you will use the `test` directory as your **validation set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Randomly select and display a grid of sample images from the 'train' folder.\n",
    "helper_utils.display_train_images(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-1'></a>\n",
    "### 1.1 - Scouting the Archives: Loading Data with ImageFolder\n",
    "\n",
    "Now that you've reviewed the dataset, your next objective is to load the images into a structure that PyTorch can use for training. For datasets that are neatly organized into subdirectories for each class, such as your `AIvsReal_sampled` data, `ImageFolder` is an exceptionally convenient and standard tool.\n",
    "\n",
    "This tool works by automatically inferring class labels from the names of the subdirectories. It expects your data to be arranged such that `dataset_path/class_name/image_name.jpg` would be an image belonging to `class_name`. This feature aligns perfectly with your dataset's organized structure, saving you significant preparation time.\n",
    "\n",
    "A pivotal advantage of `ImageFolder` is its `transform` attribute. This allows you to apply a sequence of image transformations, giving you precise control over how your data is prepared. You have the flexibility to set these transformations when you first load the data or apply them later. This adaptability ensures your data loading pipeline is highly effective and reusable for future projects.\n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### Exercise 1 - create_dataset_splits\n",
    "\n",
    "Your first exercise is to implement the `create_dataset_splits` function. This function will load your image data from the disk and create two distinct PyTorch `Dataset` objects.\n",
    "\n",
    "**Your Task**:\n",
    "\n",
    "* Use the [ImageFolder](https://docs.pytorch.org/vision/main/generated/torchvision.datasets.ImageFolder.html) class to create a dataset object for your training data and another for your validation data.\n",
    "* The paths for the training and validation directories (`train_path` and `val_path`) have already been defined for you. You'll need to pass the correct path to each `ImageFolder` instance.\n",
    "* The function should return the two datasets you create: `train_dataset` and `val_dataset`.\n",
    "\n",
    "<details>\n",
    "<summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
    "\n",
    "If you need some help, here is a more detailed guide.\n",
    "\n",
    "You need to create two instances of the `ImageFolder` class. The main argument you need to provide is `root`, which tells `ImageFolder` where to find the images.\n",
    "\n",
    "* The first line for the training dataset will look like this:\n",
    "    > `train_dataset = ImageFolder(root=train_path)`\n",
    "* Now, apply the same logic to create the `val_dataset`. Make sure you use the `val_path` variable for its `root`.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: create_dataset_splits\n",
    "\n",
    "def create_dataset_splits(data_path):\n",
    "    \"\"\"\n",
    "    Creates training and validation datasets from a directory structure using ImageFolder.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): The root path to the dataset directory, which should\n",
    "                         contain 'train' and 'validation/test' subdirectories.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the train_dataset and validation_dataset\n",
    "               (train_dataset, validation_dataset).\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct the full path to the training data directory.\n",
    "    train_path = data_path + \"/train\"\n",
    "    # Construct the full path to the validation data directory.\n",
    "    val_path = data_path + \"/test\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Create the train dataset using ImageFolder\n",
    "    train_dataset = ImageFolder(\n",
    "        # Set the root to train dataset path\n",
    "        root=train_path,\n",
    "    ) \n",
    "\n",
    "    # Create the validation dataset using ImageFolder\n",
    "    val_dataset = ImageFolder(\n",
    "        # Set the root to validation dataset path\n",
    "        root=val_path,\n",
    "    ) \n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Verify that the function loads the datasets\n",
    "temp_train, temp_val = create_dataset_splits(dataset_path)\n",
    "\n",
    "print(\"--- Training Dataset ---\")\n",
    "print(temp_train)\n",
    "print(\"\\n--- Validation Dataset ---\")\n",
    "print(temp_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output:\n",
    "```\n",
    "--- Training Dataset ---\n",
    "Dataset ImageFolder\n",
    "    Number of datapoints: 5000\n",
    "    Root location: ../AIvsReal_sampled/train\n",
    "\n",
    "--- Validation Dataset ---\n",
    "Dataset ImageFolder\n",
    "    Number of datapoints: 1000\n",
    "    Root location: ../AIvsReal_sampled/test\n",
    "```    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test your code! \n",
    "unittests.exercise_1(create_dataset_splits)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-2'></a>\n",
    "### 1.2 - Specialized Augmentations: Protocols for Training and Validation\n",
    "\n",
    "Now it's time to define the transformations that will prepare your images for the neural network. These preprocessing steps are pivotal for standardizing your data and for augmenting the training set to make your model more robust.\n",
    "\n",
    "When using pre-trained architectures as you will later, it's standard practice to apply *different* sets of transformations to your training and validation datasets. When leveraging a pre-trained model, it is **essential** that the core transformations applied to both your training and validation data are identical to the ones used to train the original model. This is particularly important for resizing the images and using the same normalization values (mean and standard deviation). While data augmentation techniques can vary between your training and validation data, these fundamental steps ensure your data is processed in a way the pre-trained model already understands.\n",
    "\n",
    "* Before jumping into the transformations, you'll first define the standard normalization values from the ImageNet dataset, on which many pre-trained models were trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# Define the standard mean values for the ImageNet dataset\n",
    "imagenet_mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "\n",
    "# Define the standard standard deviation values for the ImageNet dataset\n",
    "imagenet_std = torch.tensor([0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-2'></a>\n",
    "### Exercise 2 - define_transformations\n",
    "\n",
    "In this exercise, you'll implement the `define_transformations` function to create two distinct image transformation pipelines: one for augmenting the training data and another for preparing the validation data.\n",
    "\n",
    "**Your Task**:\n",
    "\n",
    "* **For the `train_transform` pipeline:**\n",
    "    * You need to chain together a series of transformations using [transforms.Compose](https://docs.pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html).\n",
    "    * This pipeline should include operations for:\n",
    "        *  [Random resizing](https://docs.pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html).\n",
    "        *  [Random horizontal flipping](https://docs.pytorch.org/vision/main/generated/torchvision.transforms.RandomHorizontalFlip.html).\n",
    "        *  [Color adjustments](https://docs.pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html) to augment the data.\n",
    "        *  [Tensor conversion](https://docs.pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html).\n",
    "        *  [Normalization](https://docs.pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html).\n",
    ">\n",
    "* **For the `val_transform` pipeline:**\n",
    "    * Create a second `transforms.Compose` pipeline.\n",
    "    * This one will be simpler and should only include the essential steps:\n",
    "        * [Resizing](https://docs.pytorch.org/vision/stable/generated/torchvision.transforms.Resize.html) the image.\n",
    "        * [Tensor conversion](https://docs.pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html).\n",
    "        * [Normalization](https://docs.pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html).\n",
    "> \n",
    "* Both pipelines should use the `mean` and `std` values passed into the function for normalization.\n",
    "\n",
    "<details>\n",
    "<summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
    "\n",
    "If you're looking for a more detailed guide, follow these steps.\n",
    "\n",
    "**For `train_transform`:**\n",
    "\n",
    "Your `transforms.Compose` list should contain five transformation steps in this order:\n",
    "1.  Add the `RandomResizedCrop` transform, setting the output size to `(224, 224)`.\n",
    "2.  Add the `RandomHorizontalFlip` transform.\n",
    "3.  Add the `ColorJitter` transform, setting `brightness` and `contrast` to `0.2`.\n",
    "4.  Add the `ToTensor` transform to convert images to tensors.\n",
    "5.  Add the `Normalize` transform, using the `mean` and `std` variables from the function's arguments.\n",
    "\n",
    "**For `val_transform`:**\n",
    "\n",
    "Your `transforms.Compose` list for validation is simpler and has three steps:\n",
    "1.  The first transform is for resizing:\n",
    "    > `transforms.Resize((224, 224))`\n",
    "2.  Next, add the `ToTensor` transform.\n",
    "3.  Finally, add the `Normalize` transform, again using the `mean` and `std` variables.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: define_transformations\n",
    "\n",
    "def define_transformations(mean=imagenet_mean, std=imagenet_std):\n",
    "    \"\"\"\n",
    "    Defines separate series of image transformations for training and validation datasets.\n",
    "\n",
    "    Args:\n",
    "        mean (list or tuple): The mean values (for each channel, e.g., RGB) calculated from ImageNet.\n",
    "        std (list or tuple): The standard deviation values (for each channel) calculated from ImageNet.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two `torchvision.transforms.Compose` objects:\n",
    "               - The first for training transformations.\n",
    "               - The second for validation transformations.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Create a Compose object to chain multiple transformations together for the training set\n",
    "    \n",
    "    # Initialize 'train_transform' using transforms.Compose to apply a sequence of transforms\n",
    "    train_transform = transforms.Compose([\n",
    "        # Randomly resize and crop the input image to 224x224 pixels\n",
    "        transforms.RandomResizedCrop(224),\n",
    "\n",
    "        # Apply a random horizontal flip to the image for data augmentation\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "\n",
    "        # Randomly change the brightness and contrast of the image for data augmentation\n",
    "        # Set `brightness=0.2` and `contrast=0.2`\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        # Convert the PIL Image to a PyTorch Tensor\n",
    "        transforms.ToTensor(),\n",
    "\n",
    "        # Normalize the tensor image with the provided 'mean' and 'std' to normalize the tensor\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "    ]) \n",
    "\n",
    "    # Create a Compose object to chain multiple transformations together for the validation set\n",
    "    \n",
    "    # Initialize 'val_transform' using transforms.Compose to apply a sequence of transforms\n",
    "    val_transform = transforms.Compose([\n",
    "        # Resize the input image to to 224x224 pixels\n",
    "        transforms.Resize((224, 224)),\n",
    "\n",
    "        # Convert the PIL Image to a PyTorch Tensor\n",
    "        transforms.ToTensor(),\n",
    "\n",
    "        # Normalize the tensor image with the provided 'mean' and 'std' to normalize the tensor\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "    ]) \n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return train_transform, val_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Create the composed transformations\n",
    "combined_transformations = define_transformations()\n",
    "\n",
    "# Print the composed transformations to verify the sequence of operations\n",
    "print(\"Augmented Training Transformations:\\n\")\n",
    "print(combined_transformations[0])\n",
    "print(\"\\nValidation Transformations:\\n\")\n",
    "print(combined_transformations[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output:\n",
    "```\n",
    "Augmented Training Transformations:\n",
    "\n",
    "Compose(\n",
    "    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=True)\n",
    "    RandomHorizontalFlip(p=0.5)\n",
    "    ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=None, hue=None)\n",
    "    ToTensor()\n",
    "    Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
    ")\n",
    "\n",
    "Validation Transformations:\n",
    "\n",
    "Compose(\n",
    "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
    "    ToTensor()\n",
    "    Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test your code! \n",
    "unittests.exercise_2(define_transformations)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-3'></a>\n",
    "### 1.3 - Preparing the Data Launchpad: Applying Transformations and Batching Data\n",
    "\n",
    "You have now handled the creation of your training and validation dataset objects and separately defined the transformation pipelines for each. A powerful feature of the `ImageFolder` objects is their built-in `.transform` attribute. You'll now apply your defined transformations by assigning each pipeline to its respective dataset. This provides a clean and modular way to prepare each data subset for its specific purpose. After this, you'll use `DataLoader` to create batched and shuffled data loaders for your training and validation data, which is an essential step to prepare it for training.\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### Exercise 3 - create_data_loaders\n",
    "\n",
    "Your task is to implement the `create_data_loaders` function. This function brings everything together by applying the transformations to your datasets and then wrapping them in `DataLoader` objects to prepare them for training.\n",
    "\n",
    "**Your Task**:\n",
    "\n",
    "* **Retrieve Transformations**:\n",
    "    * Call the `define_transformations()` function you implemented earlier to get the `train_transform` and `val_transform` pipelines.\n",
    ">\n",
    "* **Apply Transformations**:\n",
    "    * Assign the `train_transform` to the `.transform` attribute of the input `trainset`.\n",
    "    * Assign the `val_transform` to the `.transform` attribute of the input `valset`.\n",
    ">\n",
    "* **Create DataLoaders**:\n",
    "    * Instantiate a [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for the training set. Remember to shuffle the training data.\n",
    "    * Instantiate a second `DataLoader` for the validation set. This one should not be shuffled.\n",
    ">\n",
    "* Both `DataLoader` instances should use the `batch_size` provided to the function.\n",
    "\n",
    "<details>\n",
    "<summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
    "\n",
    "If you need a more detailed guide, follow these steps.\n",
    "\n",
    "1.  **Get the transformations**: This is a single function call with two return values.\n",
    "    > `train_transform, val_transform = define_transformations()`\n",
    "\n",
    "2.  **Apply the transformations**: You just need to assign the transform objects to the correct dataset attribute. The first one is:\n",
    "    > `trainset.transform = train_transform`\n",
    "    > Now do the same for the `valset` using the `val_transform`.\n",
    "\n",
    "3.  **Create the training `DataLoader`**: The `DataLoader` needs the dataset, `batch_size`, and shuffle status.\n",
    "    > `train_loader = create a new DataLoader using (the trainset, the function's batch_size, and set shuffle to True)`\n",
    "\n",
    "4.  **Create the validation `DataLoader`**: This is almost identical to the training loader.\n",
    "    > `val_loader = create a new DataLoader using (the valset, the function's batch_size, and set shuffle to False)`\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "name": "exercise_3",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: create_data_loaders\n",
    "\n",
    "def create_data_loaders(trainset, valset, batch_size):\n",
    "    \"\"\"\n",
    "    Creates DataLoader instances for training and validation datasets with respective transformations.\n",
    "\n",
    "    Args:\n",
    "        trainset (torch.utils.data.Dataset): The training dataset.\n",
    "        valset (torch.utils.data.Dataset): The validation dataset.\n",
    "        batch_size (int): The number of samples to load in each batch.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - train_loader (torch.utils.data.DataLoader): DataLoader for the training set.\n",
    "            - val_loader (torch.utils.data.DataLoader): DataLoader for the validation set.\n",
    "            - trainset (torch.utils.data.Dataset): The original training dataset with transformations now applied.\n",
    "            - valset (torch.utils.data.Dataset): The original validation dataset with transformations now applied.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Define separate transformations for the training and validation datasets\n",
    "    # Use define_transformations() to get train_transform and val_transform\n",
    "    train_transform, val_transform = define_transformations()\n",
    "    \n",
    "    # Apply the train transformations directly to the train dataset by setting the .transform attribute\n",
    "    trainset.transform = train_transform\n",
    "    # Apply the val transformations directly to the val dataset by setting the .transform attribute\n",
    "    valset.transform = val_transform\n",
    "    \n",
    "    # Create a DataLoader for the training dataset\n",
    "    # Use the transformed train dataset\n",
    "    # Set batch_size to the input batch_size\n",
    "    # Set shuffle=True\n",
    "    train_loader = DataLoader(\n",
    "        dataset=trainset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Create a DataLoader for the validation dataset\n",
    "    # Use the transformed validation dataset\n",
    "    # Set batch_size to the input batch_size\n",
    "    # Set shuffle=False\n",
    "    val_loader  = DataLoader(\n",
    "        dataset=valset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return train_loader, val_loader, trainset, valset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "dataloaders = create_data_loaders(temp_train, temp_val, batch_size=16)\n",
    "\n",
    "print(\"--- Train Loader ---\")\n",
    "helper_utils.display_data_loader_contents(dataloaders[0])\n",
    "print(\"\\n--- Val Loader ---\")\n",
    "helper_utils.display_data_loader_contents(dataloaders[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output:\n",
    "```\n",
    "--- Train Loader ---\n",
    "Length: 313\n",
    "--- Batch 1 ---\n",
    "Data shape: torch.Size([16, 3, 224, 224])\n",
    "Labels shape: torch.Size([16])\n",
    "\n",
    "--- Val Loader ---\n",
    "Length: 63\n",
    "--- Batch 1 ---\n",
    "Data shape: torch.Size([16, 3, 224, 224])\n",
    "Labels shape: torch.Size([16])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test your code! \n",
    "unittests.exercise_3(create_data_loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Advanced Expedition: Leveraging Pre-trained Model Wisdom\n",
    "\n",
    "Youâ€™ve prepared your data. Now, it's time for the heart of this expedition: selecting and working with a **pre-trained model**. This approach, known as **transfer learning**, is a cornerstone of modern computer vision.\n",
    "\n",
    "Instead of building a model from scratch, you can leverage a model that has already learned from a massive and diverse dataset, such as ImageNet. Think of it as using a powerful, veteran tool already equipped with a deep understanding of general visual patterns.\n",
    "\n",
    "The benefits of this approach are substantial:\n",
    "\n",
    "* **Accelerated Discovery**: You get a significant head start, as the model has already learned a rich set of features.\n",
    "* **Superior Performance**: Pre-trained models often achieve higher accuracy, especially when your dataset is small.\n",
    "\n",
    "<a name='2-1'></a>\n",
    "### 2.1 - Selecting Your Champion: Introducing MobileNetV3-Large\n",
    "\n",
    "Your first task in this phase is to choose a suitable pre-trained architecture. For this assignment, you'll be working with [MobileNetV3-Large](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.mobilenet_v3_large.html), an efficient and powerful model.\n",
    "\n",
    "**Why MobileNetV3-Large?**\n",
    "\n",
    "Itâ€™s an excellent choice for a task like identifying AI-generated images due to its:\n",
    "\n",
    "* **High Efficiency**: It offers an outstanding trade-off between accuracy and resource usage, making it highly suitable for performance on resource-constrained environments.\n",
    "* **Strong Classification Accuracy**: Despite its focus on efficiency, MobileNetV3-Large delivers robust performance, incorporating advanced design elements optimized through Neural Architecture Search (NAS).\n",
    "* **Effective Feature Extraction**: Having been pre-trained on the massive ImageNet dataset, the model has already learned a rich hierarchy of visual features, a strong foundation you can adapt to distinguish between real and fake images.\n",
    "\n",
    "<a name='ex-41'></a>\n",
    "### Exercise 4.1 - load_mobilenetv3_model\n",
    "\n",
    "Your task is to implement the `load_mobilenetv3_model` function. In this assignment, instead of downloading weights from the internet, you will load them from a local file that has been provided for you.\n",
    "\n",
    "**Your Task**:\n",
    "\n",
    "* **Load the Model Architecture**:\n",
    "    * Instantiate a [**`mobilenet_v3_large`**](https://pytorch.org/vision/main/models/generated/torchvision.models.mobilenet_v3_large.html) model from `torchvision.models`.\n",
    "    * It's essential to initialize it **without** its pre-trained weights (`weights=None`)  for now (you'll load them in the next step).\n",
    ">\n",
    "* **Load the Weights**:\n",
    "    * Use **`torch.load()`** to load the model's state dictionary (its weights) from the `weights_path` provided to the function.\n",
    ">\n",
    "* The rest of the function, which applies these loaded weights to the model architecture, is already completed for you.\n",
    "\n",
    "<details>\n",
    "<summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
    "\n",
    "If you require a more detailed guide, here are the steps.\n",
    "\n",
    "1.  **Load the model architecture**: You need to call the `mobilenet_v3_large` function. To ensure it does not download any weights, you must set the `weights` argument to `None`.\n",
    "    > `model = tv_models.mobilenet_v3_large(weights=None)`\n",
    "\n",
    "2.  **Load the local weights file**: Use the `torch.load()` function to load the weights. You need to provide the file path and a `map_location` to ensure it works on any device (CPU or GPU).\n",
    "    > `state_dict = load the torch model from (the weights_path, and set the map_location to a cpu torch device)`\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: load_mobilenetv3_model\n",
    "\n",
    "def load_mobilenetv3_model(weights_path):\n",
    "    \"\"\"\n",
    "    Loads a pre-trained MobileNetV3-Large model from torchvision.\n",
    "\n",
    "    Args:\n",
    "        weights_path (str): The file path to the saved .pth model weights.\n",
    "        \n",
    "    Returns:\n",
    "        torch.nn.Module: A pre-trained MobileNetV3-Large model.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Load the pre-trained MobileNetV3-Large model without pre-trained weights.\n",
    "    model = tv_models.mobilenet_v3_large(weights=None)\n",
    "\n",
    "    # Load the state dictionary (weights) from the local file.\n",
    "    state_dict = torch.load(weights_path, map_location=torch.device('cpu'))\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Load the pre-trained MobileNetV3-Large model using weights from the local file.\n",
    "local_weights = \"./mobilenet_weights/mobilenet_v3_large-8738ca79.pth\"\n",
    "test_model = load_mobilenetv3_model(local_weights)\n",
    "\n",
    "# Print the last layer of the classifier of the loaded model\n",
    "print(test_model.classifier[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output:\n",
    "```\n",
    "Linear(in_features=1280, out_features=1000, bias=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test your code! \n",
    "unittests.exercise_4_1(load_mobilenetv3_model)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-2'></a>\n",
    "### 2.2 - Tailoring Your Champion: Adapting MobileNetV3 for Fake Detection\n",
    "\n",
    "You've selected your champion: the pre-trained `MobileNetV3-Large` model. While it's a veteran at classifying images from the vast ImageNet dataset (with 1,000 classes), it needs to be tailored for your specific mission of identifying AI-generated images, which has only two classes. This process is called **feature extraction**.\n",
    "\n",
    "To adapt the model, you will follow two pivotal steps:\n",
    "\n",
    "* **Freezing the Core Layers**: The early layers of the model, which are responsible for extracting general visual patterns like edges, textures, and shapes, contain invaluable knowledge. By **freezing** these layers, you prevent their weights from being updated during training. This preserves the model's powerful foundation, significantly reduces the number of trainable parameters, and accelerates training.\n",
    "\n",
    "* **Replacing the Classifier Head**: The original model's final classifier is configured for 1,000 classes and is not suitable for your two-class task. You'll replace it with a new, custom-built classifier. This new layer will take the high-level features from the frozen core and output a prediction for your two target classes, preparing your model to become a specialized FakeFinder.\n",
    "\n",
    "<a name='ex-42'></a>\n",
    "### Exercise 4.2 - update_model_last_layer\n",
    "\n",
    "Your final graded task is to implement the `update_model_last_layer` function. This is the core of the **feature extraction** technique, where you will adapt the pre-trained model for your specific `AIvsReal` classification task.\n",
    "\n",
    "**Your Task**:\n",
    "\n",
    "* **Freeze Feature Layers**:\n",
    "    * The pre-trained knowledge of the model is in its feature layers (`model.features`). You need to **freeze** these layers to prevent them from being updated during training.\n",
    "    * To do this, you will iterate through all the parameters in `model.features.parameters()` and set their `requires_grad` attribute to `False`.\n",
    ">\n",
    "* **Replace the Classifier Head**:\n",
    "    * The original classifier (`model.classifier[-1]`) is designed for 1,000 classes. You need to replace it with a new one suited for your task.\n",
    "    * You will need to get the number of input features from the original classifier.\n",
    "    * Then, create a new [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layer with the correct number of input features and the new `num_classes` as the output.\n",
    "    * Finally, replace the old classifier with your new one.\n",
    ">\n",
    "\n",
    "<details>\n",
    "<summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
    "\n",
    "If you're looking for more detailed guidance, follow these steps.\n",
    "\n",
    "**1. Freezing the Layers**:\n",
    "This requires a `for` loop.\n",
    "> `for each parameter in the model's feature parameters:`\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;`set the parameter's requires_grad attribute to False`\n",
    "\n",
    "**2. Replacing the Classifier**:\n",
    "This is a three step process for the second half of the function.\n",
    "\n",
    "* First, get the number of input features from the model's original last classifier layer.\n",
    "    > `num_features = get the in_features from model.classifier[-1]`\n",
    "\n",
    "* Next, create the new linear layer. It needs the `num_features` you just retrieved and the `num_classes` from the function's arguments.\n",
    "    > `new_classifier = create a new nn.Linear layer with (in_features set to num_features, and out_features set to num_classes)`\n",
    "\n",
    "* Finally, replace the old layer with your new one.\n",
    "    > `model.classifier[-1] = your new_classifier`\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "id": "Y8nSQCYqEgHf",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_model_last_layer\n",
    "\n",
    "def update_model_last_layer(model, num_classes):\n",
    "    \"\"\"\n",
    "    Freezes the feature layers of a pre-trained model and replaces its final\n",
    "    classification layer with a new one adapted to the specified number of classes.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The pre-trained model to be modified.\n",
    "        num_classes (int): The number of output classes for the new classification layer.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: The modified model with frozen feature layers and a new\n",
    "                         classification layer.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Freeze the parameters of the feature layers of the model\n",
    "    # Iterate through each parameter in model.features.parameters()\n",
    "    for feature_parameter in model.features.parameters():\n",
    "        # Set the requires_grad attribute of each feature_parameter to False\n",
    "        feature_parameter.requires_grad = False\n",
    "\n",
    "    # Access the final classification layer of the model\n",
    "    last_classifier_layer = model.classifier[-1] \n",
    "    \n",
    "    # Access the in_features attribute of last_classifier_layer\n",
    "    num_features = last_classifier_layer.in_features\n",
    "    \n",
    "    # # Use nn.Linear to create a new Linear layer for classification with the original number of\n",
    "    # input features and the specified number of output classes\n",
    "    new_classifier = nn.Linear(in_features=num_features, out_features=num_classes)\n",
    "    \n",
    "    # Replace the original last classification layer with the newly created layer\n",
    "    model.classifier[-1] = new_classifier\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Modify the last layer of the MobileNetV3-Large model\n",
    "test_model = update_model_last_layer(test_model, num_classes=5)\n",
    "\n",
    "# Print the last layer of the classifier of the modified model\n",
    "print(test_model.classifier[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output:\n",
    "```\n",
    "Linear(in_features=1280, out_features=5, bias=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test your code! \n",
    "unittests.exercise_4_2(update_model_last_layer)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Submission Note\n",
    "\n",
    "Congratulations! You've completed the final graded exercise of this assignment.\n",
    "\n",
    "If you've successfully passed all the unit tests above, you've completed the core requirements of this assignment. Feel free to [submit](#submission) your work now. The grading process runs in the background, so it will not disrupt your progress and you can continue on with the rest of the material.\n",
    "\n",
    "**ðŸš¨ IMPORTANT NOTE** If you have passed all tests within the notebook, but the autograder shows a system error after you submit your work:\n",
    "\n",
    "<div style=\"background-color: #1C1C1E; border: 1px solid #444444; color: #FFFFFF; padding: 15px; border-radius: 5px;\">\n",
    "    <p><strong>Grader Error: Grader feedback not found</strong></p>\n",
    "    <p>Autograder failed to produce the feedback...</p>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "This is typically a temporary system glitch. The most common solution is to resubmit your assignment, as this often resolves the problem. Occasionally, it may be necessary to resubmit more than once. \n",
    ">\n",
    "If the error persists, please reach out for support in the [DeepLearning.AI Community Forum](https://community.deeplearning.ai/c/course-q-a/pytorch-for-developers/pytorch-techniques-and-ecosystem-tools/561).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Final Expedition: Training and Evaluating Your Adapted Champion\n",
    "\n",
    "This section allows you to take your adapted MobileNetV3 model through a training cycle and observe its performance. It's a chance to see how the harnessed knowledge from a pre-trained model, combined with your specialized techniques, leads to a powerful FakeFinder.\n",
    "\n",
    "<a name='3-1'></a>\n",
    "### 3.1 - Setting the Launchpad: Preparing DataLoaders and the Adapted Model\n",
    "\n",
    "Before you can begin the actual training, you first need to set up your environment by executing three core steps:\n",
    "\n",
    "* **Initializing the Datasets**: You'll first load your training and validation data from their respective directories.\n",
    "\n",
    "* **Initializing the DataLoaders**: Next, you'll create the `train_loader` and `val_loader` to feed your transformed images to the model in manageable batches.\n",
    "\n",
    "* **Preparing the Model**: You'll load the pre-trained MobileNetV3-Large model and then adapt its final classification layer for your two specific classes. This readies your champion model for its specialized mission.\n",
    "\n",
    "The following code cell will execute these preparatory steps, ensuring both your data and your model are primed for the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize the training and validation datasets\n",
    "train_dataset, val_dataset = create_dataset_splits(dataset_path)\n",
    "\n",
    "# Initialize the dataloaders for training and validation\n",
    "train_loader, val_loader, _, __ = create_data_loaders(train_dataset, val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "DWHMOru1bpbM",
    "outputId": "efb73b65-5b26-47eb-e403-32a95a1f43f9"
   },
   "outputs": [],
   "source": [
    "# Load the pre-trained MobileNetV3-Large model and modify its last layer\n",
    "local_weights = \"./mobilenet_weights/mobilenet_v3_large-8738ca79.pth\"\n",
    "mobilenet_model = load_mobilenetv3_model(local_weights)\n",
    "mobilenet_model = update_model_last_layer(mobilenet_model, num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-2'></a>\n",
    "### 3.2 - Training the Adapted Model\n",
    "\n",
    "With your data prepared and the MobileNetV3 model adapted, it's time to fine-tune its layers on your AI vs. Real image dataset. This training step will prepare the model to become a specialized FakeFinder.\n",
    "\n",
    "* Define the loss function and the optimizer.\n",
    "    * The optimizer will only update the unfrozen layers of the model, which in this case, is just the new classifier head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Define the loss function to compute the difference between the model's output and the true labels\n",
    "loss_fcn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer to update the model's weights during training\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, \n",
    "                             mobilenet_model.parameters()), \n",
    "                       lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define the number of epochs for training.\n",
    "    * You can experiment by adjusting this number to observe different training results, but for now, run it for just one epoch to see the powerful results of a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# EDITABLE CELL:\n",
    "\n",
    "# Set the number of epochs\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use the `training_loop` helper function to manage the standard training and validation cycle, a process that should be familiar to you by now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trained_model = helper_utils.training_loop(\n",
    "    mobilenet_model, \n",
    "    train_loader, \n",
    "    val_loader,\n",
    "    loss_fcn,\n",
    "    optimizer,\n",
    "    DEVICE, \n",
    "    num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "With the training complete, itâ€™s now time to reflect on the remarkable improvements you've just observed.\n",
    "\n",
    "In your previous assignment, you navigated a complex hyperparameter landscape with Optuna to find a custom model configuration. Your best-performing model achieved a strong validation accuracy of over **70%** after **3 epochs** of training. That was an excellent result born from a meticulous and time consuming search process.\n",
    "\n",
    "Now, by simply leveraging a pre-trained MobileNetV3-Large model and doing feature extraction, you have reached a validation accuracy and precision of over **80%** in just **1 epoch**! This is a dramatic performance gain in a fraction of the time.\n",
    "\n",
    "This difference powerfully illustrates a fundamental lesson in machine learning: a pre-trained model's extensive knowledge base provides an accelerated path to superior performance. While building custom architectures is pivotal for understanding how deep learning works, knowing when and how to apply the wisdom of pre-trained models is how you conquer complex, real world missions with unparalleled speed and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-3'></a>\n",
    "### 3.3 - Performance Review: Visualizing Predictions\n",
    "\n",
    "Your adapted model has completed its training. While the final accuracy score gives you a quantitative measure of its performance, it's often more insightful to see the model in action. In this final section, you'll conduct a qualitative review of your upgraded FakeFinder.\n",
    "\n",
    "First, you will visualize the model's predictions on a random batch of images from the validation set. This will help you get a feel for where the model succeeds and where it might make errors, providing a clearer picture than numbers alone.\n",
    "\n",
    "* Run the cell below to display a grid of images from the validation set, showing the true label and the model's prediction for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Get the list of class names ('fake', 'real') from the validation dataset.\n",
    "class_names = val_dataset.classes\n",
    "\n",
    "# Visualise predictions made by the trained model\n",
    "helper_utils.visualize_predictions(trained_model, val_loader, DEVICE, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Your FakeFinder on a New Image\n",
    "\n",
    "Now it's time to put your FakeFinder to the test with a new piece of data. You can upload your own image to see how the model classifies it. This is a great way to test its performance on data it has never seen.\n",
    "\n",
    "Run the `helper_utils.upload_jpg_widget()` function, which displays a widget for uploading images directly into the workspace. Please note the following:\n",
    "\n",
    "* You can only upload images that have a `.jpg` extension.\n",
    "* The file size for the image should not exceed **5 MB**.\n",
    "* After a successful upload, the image's file path will be displayed. You should copy this path for the next step.\n",
    "* Once the widget is displayed, you can use it multiple times to upload images; you don't have to re-run the `helper_utils.upload_jpg_widget()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "helper_utils.upload_jpg_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set the path to your image (as displayed above).\n",
    "\n",
    "Alternatively, you can use these images that are already present in the workspace:\n",
    "> * `image_path = './images/fake/birds_sheep_dog.jpg'`\n",
    "> * `image_path = './images/fake/car_bus_tram.jpg'`\n",
    "> * `image_path = './images/fake/person_and_bicycle.jpg'`\n",
    "> * `image_path = './images/real/eiffel_tower.jpg'`\n",
    "> * `image_path = './images/real/minar.jpg'`\n",
    "> * `image_path = './images/real/statue_liberty.jpg'`\n",
    "\n",
    "\n",
    "* A default path has been set for you, but feel free to change it to a different one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "id": "F85BDzt4TEKL"
   },
   "outputs": [],
   "source": [
    "# EDITABLE CELL:\n",
    "\n",
    "image_path = './images/fake/birds_sheep_dog.jpg' ### Add your image path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Display a prediction for the single uploaded image.\n",
    "helper_utils.make_predictions(trained_model, image_path, DEVICE, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations, AI Explorer! You have successfully completed your second expedition and significantly upgraded your FakeFinder. By masterfully applying transfer learning, you have not only improved your model's performance but also added one of the most practical and powerful techniques in modern AI to your skillset. In this assignment, you successfully loaded a pre-trained MobileNetV3 model, preserved its knowledge by freezing its core layers, and skillfully replaced its classifier to specialize it for your task. You then trained the adapted model and witnessed a dramatic performance increase, achieving a higher validation accuracy in a fraction of the training time compared to your custom built model.\n",
    "\n",
    "Your journey through these two assignments illustrates a fundamental lesson in applied AI. In the first expedition, you learned the essential principles of building and optimizing a model from scratch. In this second expedition, you learned how to stand on the shoulders of giants by leveraging pre-trained models to solve problems with incredible speed and efficiency. A truly skilled AI practitioner knows when to build from the ground up and when to adapt existing tools. You are now equipped with both skill sets.\n",
    "\n",
    "Well done on a successful mission. May your future expeditions into the world of AI be even more ambitious and rewarding."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "grader_version": "1",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
