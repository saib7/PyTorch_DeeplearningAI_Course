{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6cbfb9a-a851-43bb-a54b-9a2046e86709",
   "metadata": {},
   "source": [
    "# Optimizing DataLoaders for Performance\n",
    "\n",
    "Slow model training is a frequent obstacle in machine learning projects. While it's easy to assume that hardware is the limiting factor, performance issues often stem from a data bottleneck, where the powerful GPU is left waiting for data to process. This inefficiency can significantly prolong training cycles. Addressing how data is loaded and delivered to the accelerator is a vital step toward faster and more effective model development.\n",
    "\n",
    "\n",
    "This lab serves as a hands on guide to overcoming these challenges by effectively utilizing the PyTorch DataLoader. You'll explore how its various parameters can be configured to build an efficient data pipeline that keeps your hardware fully engaged. By the end of this session, you'll have a practical understanding of how to diagnose and resolve common data loading issues.\n",
    "\n",
    "In this notebook, you will:\n",
    "\n",
    "* Investigate how enabling parallel data processing can dramatically reduce GPU idle time.\n",
    "\n",
    "* Measure the impact of batching on processing speed and discover its relationship with hardware memory limits.\n",
    "\n",
    "* Experiment with fine tuning parameters that can accelerate data transfer and manage data buffering for additional performance gains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77e6dac-cd39-4d58-97a1-e68ab9897a3f",
   "metadata": {},
   "source": [
    "## üö® The Goal: Learning the Process, Not the Numbers üö®\n",
    "\n",
    "This notebook demonstrates how key `DataLoader` parameters affect performance. \n",
    "Because this lab operates in a **restrictive environment** with limited GPU and shared memory, a **controlled methodology is adopted** to isolate the impact of each setting. This involves aggressively clearing memory between runs to ensure every test is independent. *(Note: This cleanup is for demonstration purposes and is not a typical practice in projects where caching is beneficial).*\n",
    "\n",
    "\n",
    "The results and optimal values shown are tailor-made to demonstrate the concepts **within this specific lab**. \n",
    "In real-world projects, the ideal configuration will always **differ depending on the unique hardware (CPU, GPU, RAM) and dataset (size, complexity)**.\n",
    "\n",
    "Therefore, the key takeaway is the **process of experimentation** itself. This process provides a blueprint for how to test these parameters systematically and find the optimal configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb31a782-d89d-46e2-9745-0daa8a94b399",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1837f248-b13c-4737-a597-bc63dab667e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import helper_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8243f72e-f555-4b74-ba9b-3deadd7dae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797eb643",
   "metadata": {},
   "source": [
    "## Measuring Performance\n",
    "\n",
    "In the upcoming experiments, you will measure the **total time** it takes to load the entire dataset for one full epoch using the `measure_average_epoch_time` helper function.\n",
    "\n",
    "This metric is the primary indicator of `DataLoader` performance in this lab. By minimizing the total epoch time, you can improve the overall efficiency of the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7199b4e",
   "metadata": {},
   "source": [
    "## Tuning `num_workers` for Parallel Loading\n",
    "\n",
    "When training deep learning models, you can face a **data loading bottleneck** when your powerful GPU sits idle waiting for the CPU to prepare data, which slows down the entire process. \n",
    "To solve this, PyTorch's `DataLoader` has a `num_workers` parameter. By default it is `0`, meaning a single process loads data, but increasing this value enables multiple processes to prepare data in parallel. \n",
    "Think of this as having several cooks preparing meals at once instead of just one, ensuring a constant flow of food and significantly speeding up the pipeline.\n",
    "\n",
    "Finding the optimal number of workers is an important optimization step. \n",
    "To determine how many parallel processes your CPU can run to support the GPU's operations, a good starting point is to check the number of available CPU cores. \n",
    "\n",
    "**Note:**  Ultimately, the ideal number balances the capabilities of your specific hardware (CPU, GPU, and disk speed) with your **dataset's complexity**, which you can find only through experimentation.\n",
    "\n",
    "In the following examples, you will experiment with several different worker settings to measure their impact on performance using the CIFAR10 dataset.\n",
    "\n",
    "* Load the *CIFAR10* dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d14ed85-3071-4c56-8d90-c4062dcefb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = helper_utils.download_and_load_cifar10()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c3f9f5-13d5-48df-9f4b-db0c6e4ce1cb",
   "metadata": {},
   "source": [
    "* Before you begin experimenting, you can find out exactly how many cores your environment has by running the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27485e7c-9643-4102-9b19-130a39bdfb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_cores = os.cpu_count()\n",
    "print(f\"Number of available CPU cores: {cpu_cores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba1aa64-7a4d-4442-a67b-0c9536cc8501",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**The Point of Diminishing Returns**\n",
    "\n",
    "Just because you can set a high number of workers, sometimes even more than your available CPU cores, doesn't mean you should. Adding more workers is subject to a law of **diminishing returns**, where you eventually reach a point that adding more provides little benefit or can even slow things down. This is often due to two main factors:\n",
    "\n",
    "* **System Limits**: Each worker requires system resources, including **shared memory**, to operate. Creating too many workers can exhaust this memory, which may cause your program to fail.\n",
    "\n",
    "\n",
    "* **Resource Competition**: After a certain point, adding more workers creates its own overhead. It can lead to competition for other resources, like your disk's reading speed or the main Python process itself, creating a new bottleneck.\n",
    "\n",
    "*Your goal is to find the \"sweet spot\", the **smallest number** of workers that keeps your GPU constantly supplied with data.*\n",
    "\n",
    "\n",
    "To begin your experiment, you'll define a list to test six different `num_workers` values: `0`, `2`, `4`, `6`, `8`, and `10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac85d7f-005e-49c6-8653-7e6fe1d1b219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of num_workers values to test\n",
    "workers_to_test = [0, 2, 4, 6, 8, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a188aa5-95ea-4d72-a5e3-12422115b7a0",
   "metadata": {},
   "source": [
    "* Execute the cell below to measure the time it takes to load the entire dataset, based on each `num_workers` value, while keeping a fixed `batch_size=32`.\n",
    "* For each setting, the timing runs for five epochs. The first two are for **warm-up**, which allows system processes to stabilize, while the **final three** are averaged to calculate the stable loading time.\n",
    "\n",
    "<div style=\"background-color: #FFD2D2; border: 1px solid #D8000C; color: black; padding: 15px; border-radius: 5px;\">\n",
    "    <p>üö® <b>IMPORTANT NOTE:</b> Pay attention to any <code>RuntimeError</code> messages. These can occur with higher number of workers count on systems with limited <b>shared memory</b>. If you have modified the code and encounter this error, your <strong>first</strong> step should be to undo your modifications, then restart the kernel and run the original code to continue.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cabec62-5ead-4adb-aa2a-825173cf5a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_workers(workers_to_test, trainset, device):\n",
    "    \"\"\"\n",
    "    Measures the data loading time for different numbers of workers.\n",
    "\n",
    "    Args:\n",
    "        workers_to_test: A list of integers representing the number of workers to test.\n",
    "        trainset: The dataset to be loaded.\n",
    "        device: The device to which the data will be moved (e.g., 'cpu' or 'cuda').\n",
    "    \"\"\"\n",
    "    # Initialize a dictionary to store the results\n",
    "    worker_times = {}\n",
    "\n",
    "    # Loop through each worker number you want to test\n",
    "    for nw in workers_to_test:\n",
    "        print(f\"--- Testing Number of Workers = {nw} ---\")\n",
    "        \n",
    "        # Create a new DataLoader instance for each specific test.\n",
    "        loader = DataLoader(trainset, \n",
    "                            batch_size=32, \n",
    "                            shuffle=True,\n",
    "                            # The 'num_workers' is set to the current value in the loop.\n",
    "                            num_workers=nw\n",
    "                        )\n",
    "        \n",
    "        # Handle potential runtime errors\n",
    "        try:\n",
    "            # Time the data loading for one epoch and save it to the dictionary\n",
    "            worker_times[nw] = helper_utils.measure_average_epoch_time(loader, device)\n",
    "        except RuntimeError as e:\n",
    "            # If an error occurs (often from running out of shared memory)\n",
    "            print(f\"\\n‚ùå ERROR with {nw} workers. Likely a shared memory issue.\")\n",
    "            worker_times[nw] = float('inf')\n",
    "            \n",
    "        # Clean up the loader and call the garbage collector to free up memory\n",
    "        del loader\n",
    "        gc.collect()\n",
    "\n",
    "        # Clear the PyTorch CUDA cache to free up GPU memory\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return worker_times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccbe21c",
   "metadata": {},
   "source": [
    "Most experiments in this lab are executed using the helper function `run_experiment`. The purpose of this function is to **prevent re-running already successful experiments** in case of a crash or interruption.\n",
    "\n",
    "By default, the function first checks if a result file for a given experiment already exists. If it does not find one, which is the case on the very first run, it will execute the computation and save the result. If a result file *is* found on a subsequent run, it will load the data from that file instead of computing it again.\n",
    "\n",
    "Each experiment‚Äôs results are stored in the `checkpoint_experiments` folder, with one file per experiment (named after the experiment) in JSON format.\n",
    "\n",
    "**Note**: If you need to force a specific experiment to run again, you can pass the `rerun=True` flag when calling the `run_experiment` function. For example: `run_experiment(..., rerun=True)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c524a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment to measure the data loading time for different numbers of workers.\n",
    "worker_times = helper_utils.run_experiment(\n",
    "    # A unique name for this experiment, used as the filename for the cached results.\n",
    "    experiment_name='worker_times', \n",
    "    # The actual function that contains the experiment's logic.\n",
    "    experiment_fcn=experiment_workers, \n",
    "    # The parameters to iterate over; in this case, a list of worker counts.\n",
    "    cases=workers_to_test, \n",
    "    # The dataset required by the experiment function.\n",
    "    trainset=trainset, \n",
    "    # The computation device (e.g., 'cpu' or 'cuda') to be used.\n",
    "    device=device,\n",
    "    # If False, the function will load results from the cache if they exist.\n",
    "    # If True, it will force the experiment to run again and overwrite any old results.\n",
    "    rerun=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147801db-73c9-4bfa-bbd8-c560dc46c56a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* With the performance data collected, you can now plot the results to visualize the impact of `num_workers` on loading time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c66a3a-97f7-4d74-a77a-dc81e68b93fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.plot_performance_summary(\n",
    "    worker_times, \n",
    "    title=\"DataLoader Performance vs. num_workers\", \n",
    "    xlabel=\"Number of Workers\", \n",
    "    ylabel=\"Average Time per Epoch (milliseconds)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae64ee02-7bc2-4e53-bf77-40c127d8bbb2",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Interpreting the Results**\n",
    "\n",
    "The exact times you see can fluctuate slightly with each execution, depending on the current system load of the notebook environment. However, the general trend you observe should be clear:\n",
    "\n",
    "* **The Initial Jump**: You will almost certainly see the most significant performance improvement in the first few workers. This demonstrates the powerful and immediate benefit of enabling parallel data loading.\n",
    "\n",
    "* **Diminishing Returns**: Beyond 2 or 4 workers, the results often become less predictable. The time might plateau, fluctuate, or even increase slightly with more workers. This is a classic example of the law of diminishing returns, where adding more resources eventually stops providing a clear benefit and can create new overhead.\n",
    "\n",
    "The goal is to find a reliable setting that is consistently fast. \n",
    "**For this environment**, it can be observed that `num_workers=6` is frequently a good and decent choice that balances speed with resource usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bec795-88b4-474e-95a9-14e682c0285c",
   "metadata": {},
   "source": [
    "### Visualizing DataLoader Efficiency\n",
    "\n",
    "The previous graph showed the total loading time per epoch. \n",
    "To better understand *why* some configurations are faster, you can visualize the **efficiency breakdown** of your DataLoader pipeline.\n",
    "\n",
    "By using the `visualize_dataloader_efficiency` function, you can get this breakdown. \n",
    "Instead of measuring the entire epoch, it performs a **per-batch analysis** to reveal the pipeline's efficiency. \n",
    "\n",
    "You will obtain a normalized bar chart.\n",
    "Each bar represents 100% of the total time spent per batch for different `num_workers` settings, composed of:\n",
    "\n",
    "* The **blue** part of the bar represents **GPU Active Time** - the percentage of time spent on productive work (moving data to the GPU device).\n",
    "\n",
    "* The **yellow** part represents **GPU Idle/Waiting Time** - the percentage of time the GPU spends waiting for the CPU to prepare and deliver the next batch. This idle time represents the pipeline bottleneck you want to minimize.\n",
    "\n",
    "\n",
    "Think of the GPU as a customer at a restaurant and the CPU workers as chefs. \n",
    "The blue portion shows which proportion of the total time the customer spends eating (productive work), while the yellow portion shows how much time they spend waiting for the next dish to arrive (wasted time). \n",
    "*More efficient configurations will have larger blue sections and smaller yellow sections.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e2be35-dc5f-40fe-8b12-322ee9ca5c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This cell will take a few seconds to run\n",
    "\n",
    "# Create the dictionary of loaders iteratively using a dictionary comprehension\n",
    "# for each number in the 'workers_to_test' list.\n",
    "loaders_to_compare = {\n",
    "    f\"{nw} Workers\": DataLoader(trainset, batch_size=32, num_workers=nw) \n",
    "    for nw in workers_to_test\n",
    "}\n",
    "\n",
    "# Pass the generated dictionary to the plotting function.\n",
    "helper_utils.visualize_dataloader_efficiency(loaders_to_compare, device)\n",
    "\n",
    "# Clean up and release memory.\n",
    "del loaders_to_compare\n",
    "gc.collect()\n",
    "\n",
    "# Clear the PyTorch CUDA cache to free up GPU memory.\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac257b7c-7570-4237-bb48-b75eb201b49a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**How to Interpret the Chart**\n",
    "\n",
    "The bar chart helps you diagnose **why** some settings are faster by visualizing the **efficiency** of each `num_workers` configuration. \n",
    "It shows the proportion of time the GPU spends doing useful work versus waiting for data.\n",
    "\n",
    "Because the performance *can fluctuate slightly with each execution* **in this shared environment**, your goal is to find a setting that is consistently both fast and efficient. \n",
    "For this experiment, you will see a significant efficiency improvement when moving from `0` to `2` workers, as the yellow \"waiting\" bar shrinks dramatically. After that, you will likely observe diminishing returns. \n",
    "\n",
    "As a general rule, you are looking for **GPU Efficiency**: high blue percentages and low yellow percentages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fd73b6-c2ec-4935-b460-38562824d0cb",
   "metadata": {},
   "source": [
    "## Exploring the Effect of `batch_size`\n",
    "\n",
    "Along with the number of workers, the `batch_size` is another fundamental parameter for optimizing your data pipeline. Batching allows your model to process multiple data samples at the same time, which is essential for making efficient use of parallel hardware like a GPU. \n",
    "Think of it like a bus instead of a car; a bus (large batch size) transports many people at once, leading to a more efficient trip than sending each person in their own individual car (small batch size).\n",
    "\n",
    "While a larger batch size can often lead to faster training by maximizing hardware utilization, it's not without its limits. \n",
    "The most significant constraint is **GPU memory**. \n",
    "As you increase the batch size, you'll see loading times improve and then plateau, until the batch becomes too large and causes an out of memory error. In the next experiment, you will hold the number of workers constant and test different batch sizes to observe their effect on performance.\n",
    "\n",
    "* For this experiment, you'll define a list to test six different `batch_size` values: `16`, `32`, `64`, `128`, `256`, and `512`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f1fa59-71df-4983-947f-8ecceeb08d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of batch_size values to test\n",
    "batch_sizes_to_test = [16, 32, 64, 128, 256, 512]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c59b49-8257-46c7-af98-ead6503e019f",
   "metadata": {},
   "source": [
    "* Execute the cell below to measure the time it takes to load the entire dataset, based on each `batch_size` value, while keeping a fixed `num_workers=6`.\n",
    "* For each setting, the timing runs for five epochs. The first two are for **warm-up**, which allows system processes to stabilize, while the **final three** are averaged to calculate the stable loading time.\n",
    "\n",
    "<div style=\"background-color: #FFD2D2; border: 1px solid #D8000C; color: black; padding: 15px; border-radius: 5px;\">\n",
    "    <p>üö® <b>IMPORTANT NOTE:</b> Pay attention to any <code>RuntimeError</code> messages. These often indicate that you have exceeded a system resource limit, such as running out of <b>shared memory</b> (common with a high number of workers) or <b>GPU memory</b> (common with a large batch size). If you have modified the code and encounter this error, your <strong>first</strong> step should be to undo your modifications, then restart the kernel and run the original code to continue.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0232228-9ffb-43e8-ab72-f1f8f85bace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_batch_sizes(batch_sizes_to_test, trainset, device):\n",
    "    \"\"\"\n",
    "    Measures the data loading time for different batch sizes.\n",
    "\n",
    "    Args:\n",
    "        batch_sizes_to_test: A list of integers representing the batch sizes to test.\n",
    "        trainset: The dataset to be loaded.\n",
    "        device: The device to which the data will be moved (e.g., 'cpu' or 'cuda').\n",
    "    \"\"\"\n",
    "    # Initialize a dictionary to store the results\n",
    "    batch_size_times = {}\n",
    "\n",
    "    # Loop through each batch size you want to test\n",
    "    for bs in batch_sizes_to_test:\n",
    "        print(f\"--- Testing Batch Size = {bs} ---\")\n",
    "        \n",
    "        # Create a new DataLoader instance for each specific test.\n",
    "        loader = DataLoader(trainset, \n",
    "                            # The 'batch_size' is set to the current value in the loop.\n",
    "                            batch_size=bs, \n",
    "                            shuffle=True,\n",
    "                            num_workers=6\n",
    "                        )\n",
    "        \n",
    "        # Handle potential runtime errors, especially out-of-memory\n",
    "        try:\n",
    "            # Time the data loading for one epoch and save it to the dictionary\n",
    "            batch_size_times[bs] = helper_utils.measure_average_epoch_time(loader, device)\n",
    "        except RuntimeError as e:\n",
    "            # If an error occurs (often from running out of GPU memory),\n",
    "            print(f\"\\n‚ùå ERROR with batch size {bs}. Likely a GPU memory issue.\")\n",
    "            batch_size_times[bs] = float('inf')\n",
    "            \n",
    "        # Clean up the loader and call the garbage collector to free up memory\n",
    "        # ensuring each test runs in a clean environment.\n",
    "        del loader\n",
    "        gc.collect()\n",
    "\n",
    "        # Clear the PyTorch CUDA cache to free up GPU memory.\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "    return batch_size_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a2603b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment to measure the data loading time for different batch sizes.\n",
    "batch_size_times = helper_utils.run_experiment(\n",
    "    # A unique name for this experiment, used as the filename for the cached results.\n",
    "    experiment_name=\"batch_size_times\", \n",
    "    # The actual function that contains the experiment's logic.\n",
    "    experiment_fcn=experiment_batch_sizes,\n",
    "    # The parameters to iterate over; in this case, a list of different batch sizes.\n",
    "    cases=batch_sizes_to_test,\n",
    "    # The dataset required by the experiment function.\n",
    "    trainset=trainset,\n",
    "    # The computation device (e.g., 'cpu' or 'cuda') to be used.\n",
    "    device=device,\n",
    "    # If False, the function will load results from the cache if they exist.\n",
    "    # If True, it will force the experiment to run again and overwrite any old results.\n",
    "    rerun=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1734818f-d08a-4599-a7ed-3b54d4748f47",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* With the performance data collected, you can now plot the results to visualize the impact of `batch_size` on loading time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019f01d5-af0d-42b3-9d4f-bbc67514ebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.plot_performance_summary(\n",
    "    batch_size_times, \n",
    "    title=\"DataLoader Performance vs. batch_size\", \n",
    "    xlabel=\"Batch Sizes\", \n",
    "    ylabel=\"Average Time per Epoch (milliseconds)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38bd03d-ea57-46e1-8ad9-ae14cbc1e7d1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Interpreting the Results**\n",
    "\n",
    "Again, the exact times can fluctuate with each run, but the overall trend should be very clear:\n",
    "\n",
    "* **Initial Speedup**: You will notice a significant drop in loading time as you increase the `batch_size` from smaller values (e.g., 16 to 128). This is because larger batches make much more efficient use of the GPU's parallel processing capabilities, reducing the overhead per sample.\n",
    "\n",
    "* **Diminishing Returns**: You can also see the law of diminishing returns in action. The performance gain from doubling the batch size from 256 to 512 is much smaller than the gain from doubling it from 16 to 32. This happens because the GPU is already becoming saturated with work.\n",
    "\n",
    "The goal is to find the largest batch size that gives good performance without exceeding your system's limits. While larger is often faster, there is a hard limit. In this notebook environment, for example, increasing the `batch_size` to 1024 will cause a GPU out of memory error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597fd19a-d748-408e-b3ac-061d901e0c90",
   "metadata": {},
   "source": [
    "## Optimizing with `pin_memory`\n",
    "\n",
    "Now that you have optimized `num_workers` and `batch_size`, you can explore another optimization to speed up the data transfer from the CPU to the GPU. While it can provide a significant speed boost, this feature comes with a trade-off: **it consumes more of your system's RAM**.\n",
    "\n",
    "By default, data loaded by the CPU is in \"pageable\" memory, which requires an extra copy step before the GPU can access it. By setting `pin_memory=True` in the `DataLoader`, you instruct it to use a special \"pinned\" memory region. \n",
    "This allows for faster and more direct memory transfer to the GPU. \n",
    "\n",
    "This is similar to airport security; standard memory requires you to stop and unpack your bags into separate bins, while pinned memory is like having your bags prepped for the express lane, allowing for a much faster trip through the scanner.\n",
    "\n",
    "In this experiment, you will take your best performing configuration from the previous steps and see how much additional speed you can gain by enabling `pin_memory`.\n",
    "\n",
    "* Define a list with boolean values for `pin_memory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d60e13-2fa4-43ea-a4ef-5b94e1d93eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pin_memory_settings = [False, True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cde118-7881-4dd4-80d2-5d0c0b34605c",
   "metadata": {},
   "source": [
    "* Execute the cell below to measure the time it takes to load the entire dataset for each `pin_memory` setting (`False` and `True`), while keeping a fixed `num_workers=6` and `batch_size=256`.\n",
    "\n",
    "* For each setting, the timing runs for five epochs. The first two are for warm-up, which allows system processes to stabilize, while the final three are averaged to calculate the stable loading time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996dbd3b-95a2-4fb4-bbbc-a11d863fd7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_pin_memory(pin_memory_settings, trainset, device):\n",
    "    \"\"\"\n",
    "    Measures the data loading time with and without pinned memory.\n",
    "\n",
    "    Args:\n",
    "        pin_memory_settings: A list of boolean values to test for pin_memory.\n",
    "        trainset: The dataset to be loaded.\n",
    "        device: The device to which the data will be moved (e.g., 'cpu' or 'cuda').\n",
    "    \"\"\"\n",
    "    # Initialize a dictionary to store the results\n",
    "    pin_memory_times = {}\n",
    "\n",
    "    # Loop through each pin_memory setting\n",
    "    for setting in pin_memory_settings:\n",
    "        print(f\"--- Testing with pin_memory = {setting} ---\")\n",
    "        \n",
    "        # Create a DataLoader with the current pin_memory setting\n",
    "        loader = DataLoader(trainset,\n",
    "                            batch_size=256,\n",
    "                            num_workers=6,\n",
    "                            shuffle=True,\n",
    "                            # The 'pin_memory' is set to the current boolean value in the loop.\n",
    "                            pin_memory=setting\n",
    "                        )\n",
    "        \n",
    "        try:\n",
    "            # Measure performance and store the result in the dictionary\n",
    "            pin_memory_times[setting] = helper_utils.measure_average_epoch_time(loader, device)\n",
    "        except RuntimeError as e:\n",
    "            # Print an error message if an exception occurs\n",
    "            print(f\"\\n‚ùå An error occurred with pin_memory = {setting}: {e}\")\n",
    "            pin_memory_times[setting] = float('inf')\n",
    "            \n",
    "        # --- Memory Cleanup for each iteration ---\n",
    "        del loader\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return pin_memory_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67096b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment to measure the data loading time for pin memory either set as False or True.\n",
    "pin_memory_times = helper_utils.run_experiment(\n",
    "    # A unique name for this experiment, used as the filename for the cached results.\n",
    "    experiment_name=\"pin_memory_times\",\n",
    "    # The actual function that contains the experiment's logic.\n",
    "    experiment_fcn=experiment_pin_memory,\n",
    "    # The parameters to iterate over; in this case, a list of boolean values for pin memory.\n",
    "    cases=pin_memory_settings,\n",
    "    # The dataset required by the experiment function.\n",
    "    trainset=trainset, \n",
    "    # The computation device (e.g., 'cpu' or 'cuda') to be used.\n",
    "    device=device,\n",
    "    # If False, the function will load results from the cache if they exist.\n",
    "    # If True, it will force the experiment to run again and overwrite any old results.\n",
    "    rerun=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067a8417-92d4-4efa-ade2-b11ca2eaf6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.plot_performance_summary(\n",
    "    pin_memory_times, \n",
    "    title=\"DataLoader Performance vs. pin_memory\", \n",
    "    xlabel=\"Pin Memory\", \n",
    "    ylabel=\"Average Time per Epoch (milliseconds)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d506e2-ea92-4f16-87c1-f4c9e7cf5b7c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Interpreting the Results**\n",
    "\n",
    "While `pin_memory` is often recommended as a straightforward performance boost, the results in this specific environment show the opposite. \n",
    "This highlights that **optimizations are not always universal and must be tested**.\n",
    "\n",
    "* **The Counter-Intuitive Result**: As seen in the chart, enabling `pin_memory=True` (represented by 1 on the x-axis) actually resulted in a slightly slower time per epoch compared to leaving it disabled (0).  \n",
    "**Note**: In the rare case that your specific run shows a slight improvement with `pin_memory=True`, you will likely notice that the difference is marginal.\n",
    "\n",
    "\n",
    "* **The Cost of Overhead**: This demonstrates an important concept in performance tuning: every optimization has a potential overhead.  \n",
    "While pinning memory can speed up the final data transfer step to the GPU, the process of allocating that special memory itself has a small cost. In this highly optimized scenario, where \"6 workers\" and a \"large batch size\" are already keeping the GPU well-fed, the overhead cost of pinning memory outweighed its transfer speed benefit.\n",
    "\n",
    "\n",
    "**The Takeaway on Experimentation**: This is a good example that achieving the best performance doesn't mean enabling every available feature. \n",
    "Optimization is an experimental process to *find the right combination of settings that work best for your specific hardware, software, and dataset*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fefd11-477d-41c6-8f4f-99feb2e410a8",
   "metadata": {},
   "source": [
    "## Fine-Tuning with `prefetch_factor`\n",
    "\n",
    "You've now optimized the number of parallel workers and found the ideal batch size and `pin_memory` setting. The final knob you can turn for the `DataLoader` is the `prefetch_factor`. This parameter controls how many batches are pre-loaded into memory *per worker*.\n",
    "\n",
    "By default, `prefetch_factor=2`. This means each worker will always try to have two batches ready and waiting in the background. For most use cases, this is enough to hide the data loading latency and keep the GPU fed.\n",
    "\n",
    "The trade-off is straightforward: increasing the `prefetch_factor` can sometimes smooth out data loading stalls at the cost of using more **system RAM**, as more pre-fetched batches will be held in memory. Think of it as a larger buffer, it can help if data loading speeds are inconsistent, but it consumes more resources.\n",
    "\n",
    "In this final experiment, you'll test different `prefetch_factor` values to see if moving away from the default provides any final performance gain for your already optimized pipeline.\n",
    "\n",
    "* For this experiment, you'll define a list to test six different `prefetch_factor` values: `2`, `4`, `6`, `8`, `10`, and `12`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8c53a5-096e-4d71-8e28-bd663b589374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of prefetch_factor values to test\n",
    "prefetch_factors_to_test = [2, 4, 6, 8, 10, 12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823bf489-203b-409b-8151-273f1dc8ee33",
   "metadata": {},
   "source": [
    "* Execute the cell below to measure the time it takes to load the entire dataset for each `prefetch_factor` value, while using the optimal settings determined in previous experiments (`num_workers=6`, `batch_size=256`, and `pin_memory=False`).\n",
    "* For each setting, the timing runs for five epochs. The first two are for warm-up, which allows system processes to stabilize, while the final three are averaged to calculate the stable loading time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e982391-2d41-4436-8d86-de3cb154145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_prefetch_factor(prefetch_factors_to_test, trainset, device):\n",
    "    \"\"\"\n",
    "    Measures the data loading time for different prefetch factor settings.\n",
    "\n",
    "    Args:\n",
    "        prefetch_factors_to_test: A list of integers representing the prefetch factors to test.\n",
    "        trainset: The dataset to be loaded.\n",
    "        device: The device to which the data will be moved (e.g., 'cpu' or 'cuda').\n",
    "    \"\"\"\n",
    "    # Initialize a dictionary to store the results\n",
    "    prefetch_factor_times = {}\n",
    "\n",
    "    # Loop through each prefetch factor you want to test\n",
    "    for pf in prefetch_factors_to_test:\n",
    "        print(f\"--- Testing prefetch_factor = {pf} ---\")\n",
    "        \n",
    "        # Create a new DataLoader instance for each specific test, using the optimal settings\n",
    "        loader = DataLoader(trainset, \n",
    "                            batch_size=256, \n",
    "                            shuffle=True,\n",
    "                            num_workers=6,\n",
    "                            pin_memory=False,\n",
    "                            # The 'prefetch_factor' is set to the current value in the loop.\n",
    "                            prefetch_factor=pf\n",
    "                        )\n",
    "        \n",
    "        # Handle potential runtime errors\n",
    "        try:\n",
    "            # Time the data loading for one epoch and save it to the dictionary\n",
    "            prefetch_factor_times[pf] = helper_utils.measure_average_epoch_time(loader, device)\n",
    "        except RuntimeError as e:\n",
    "            # If an error occurs, record it.\n",
    "            print(f\"\\n‚ùå ERROR with prefetch_factor {pf}: {e}\")\n",
    "            prefetch_factor_times[pf] = float('inf')\n",
    "            \n",
    "        # Clean up the loader and call the garbage collector to free up memory\n",
    "        # ensuring each test runs in a clean environment.\n",
    "        del loader\n",
    "        gc.collect()\n",
    "\n",
    "        # Clear the PyTorch CUDA cache to free up GPU memory.\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return prefetch_factor_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4560bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment to measure the data loading time for different prefetch factor.\n",
    "prefetch_factor_times = helper_utils.run_experiment(\n",
    "    # A unique name for this experiment, used as the filename for the cached results.\n",
    "    experiment_name=\"prefetch_factor_times\", \n",
    "    # The actual function that contains the experiment's logic.\n",
    "    experiment_fcn=experiment_prefetch_factor,\n",
    "    # The parameters to iterate over; in this case, a list of different prefetch factor.\n",
    "    cases=prefetch_factors_to_test,\n",
    "    # The dataset required by the experiment function.\n",
    "    trainset=trainset, \n",
    "    # The computation device (e.g., 'cpu' or 'cuda') to be used.\n",
    "    device=device,\n",
    "    # If False, the function will load results from the cache if they exist.\n",
    "    # If True, it will force the experiment to run again and overwrite any old results.\n",
    "    rerun=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6c09df-f239-4d58-822e-f71be5b2fe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.plot_performance_summary(\n",
    "    prefetch_factor_times, \n",
    "    title=\"DataLoader Performance vs. prefetch_factor\", \n",
    "    xlabel=\"Prefetch Factor\", \n",
    "    ylabel=\"Average Time per Epoch (milliseconds)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951e7d86-9b4f-4591-a783-48fbc5aa3d62",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Interpreting the Results**\n",
    "\n",
    "This final experiment explores the `prefetch_factor` and perfectly illustrates the principle of diminishing returns in performance tuning.\n",
    "\n",
    "* **The Point of Diminishing Returns**: After achieving significant speed-ups with `num_workers` and `batch_size`, you've reached the fine-tuning stage where improvements become marginal or non-existent. When you experiment with `prefetch_factor`, you should not expect to see a clear, repeatable winner. The performance will likely be \"noisy,\" with minor fluctuations that don't point to a definitive improvement over the default.\n",
    "\n",
    "* **The RAM vs. Stability Trade-Off**: Increasing the `prefetch_factor` creates a larger data buffer, which consumes more system RAM. While a larger buffer could theoretically guard against data loading stalls, an already efficient pipeline rarely benefits. You're essentially spending more memory for a larger safety net that your fast-moving workers likely don't need.\n",
    "\n",
    "**Know When to Stop**: The most important lesson here is learning to recognize when a system is sufficiently optimized.  \n",
    "Instead of chasing tiny, unstable gains with advanced parameters, it's often more practical and reliable to stick with a well-chosen default like `prefetch_factor=2`. It provides an excellent balance of performance and resource usage without adding unnecessary complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19eee9a-6b9e-40c4-94dc-b51f3386ed5e",
   "metadata": {},
   "source": [
    "## Try Yourself!\n",
    "\n",
    "You explored four key `DataLoader` parameters for performance optimization: `num_workers`, `batch_size`, `pin_memory`, and `prefetch_factor`.\n",
    "\n",
    "An important observation was that the final two parameters, `pin_memory` and `prefetch_factor`, had a negligible impact on performance. \n",
    "This outcome occurred because the data pipeline, with the CIFAR10 dataset, was already highly optimized for this specific lab environment by the primary settings of `num_workers=6` and `batch_size=256`.\n",
    "\n",
    "An interesting challenge for you now is to experiment and see if `pin_memory` and `prefetch_factor` can be made to have a more significant impact. \n",
    "For instance, consider creating a scenario where the pipeline is intentionally sub-optimal. \n",
    "What happens to the `pin_memory` benchmark if `num_workers` is set to 0 or 2? Does `prefetch_factor` become more relevant then?\n",
    "\n",
    "This is an opportunity to play around with different combinations to build a deeper intuition for how these parameters interact.\n",
    "\n",
    "<div style=\"background-color: #FFD2D2; border: 1px solid #D8000C; color: black; padding: 15px; border-radius: 5px;\">\n",
    "<p>üö® <b>A Friendly Reminder:</b> Remember to work within the constraints of this lab environment to avoid memory errors. If an error does occur, the simplest solution is to restart the kernel from the menu above, try a different combination of settings, and then run the cells below again.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfd8665-eefa-4eb3-a4d0-c39896dd295c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import helper_utils\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af65b875-832c-4e82-a56a-5222124cf0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = helper_utils.download_and_load_cifar10()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b517b9-f0b7-40da-bbd2-8f872b644f89",
   "metadata": {},
   "source": [
    "In the code cell, implement the logic where the `### Add your code here` placeholder is shown.\n",
    "\n",
    "* `parameter_name`: For this variable, assign a string that describes the `DataLoader` parameter being tested (e.g., `'pin_memory'` or `'batch_size'`).\n",
    "\n",
    "* `list_of_values_to_test`: For this variable, create a Python list containing the different values to experiment with (e.g., `[True, False]` or `[16, 32, 64]`).\n",
    "\n",
    "* `loader`: For this variable, instantiate a `DataLoader`. Configure it with the desired settings and ensure the parameter being tested is set using the `current_value` variable from the loop.\n",
    "\n",
    "**Important Note: üö®** When creating the `DataLoader`, the variable must be named exactly `loader`. This is necessary because the memory cleanup code that follows depends on this specific variable name. **Do not modify the cleanup code**, as it is essential for ensuring each experiment runs independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e794f410-087d-45e6-b00b-4fe03dda8e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_experiment(trainset, device):\n",
    "    \"\"\"\n",
    "    Runs a custom experiment to measure DataLoader performance.\n",
    "\n",
    "    Args:\n",
    "        trainset: The dataset to be used for the experiment.\n",
    "        device: The device (e.g., 'cpu' or 'cuda') on which to run the test.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            - A dictionary with the performance results.\n",
    "            - The name of the parameter that was tested.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Specify the name of the DataLoader parameter to be tested.\n",
    "    # For example: parameter_name = 'prefetch_factor'\n",
    "\n",
    "    ### Add your code here\n",
    "    parameter_name = \"prefetch_factor\"\n",
    "\n",
    "    # Provide a list of values to iterate through for the specified parameter.\n",
    "    # For example: list_of_values_to_test = [6, 8]\n",
    "    ### Add your code \n",
    "    list_of_values_to_test = [2, 4, 6, 8, 10, 12]\n",
    "\n",
    "    # Initialize an empty dictionary to store the performance results.\n",
    "    results_dictionary = {}\n",
    "\n",
    "    # Iterate over each value in the test list.\n",
    "    for current_value in list_of_values_to_test:\n",
    "        print(f\"--- Testing {parameter_name} = {current_value} ---\")\n",
    "        \n",
    "        # Configure and instantiate the DataLoader for the current test iteration.\n",
    "        # For example: loader = DataLoader(trainset, \n",
    "                                       #  batch_size=64, \n",
    "                                       #  shuffle=True, \n",
    "                                       #  num_workers=2, \n",
    "                                       #  pin_memory=False,\n",
    "                                       #  prefetch_factor=current_value\n",
    "                                       # )\n",
    "        \n",
    "        loader = DataLoader(trainset,\n",
    "                            batch_size=256,\n",
    "                            shuffle=True,\n",
    "                            num_workers=6,\n",
    "                            pin_memory=False,\n",
    "                            prefetch_factor=current_value\n",
    "                        )\n",
    "        \n",
    "        # Measure the performance and handle potential runtime errors.\n",
    "        try:\n",
    "            # Calculate the average epoch time and store it in the results dictionary.\n",
    "            results_dictionary[current_value] = helper_utils.measure_average_epoch_time(loader, device)\n",
    "        except RuntimeError as e:\n",
    "            # Handle cases where a runtime error occurs, such as an out-of-memory issue.\n",
    "            print(f\"\\n‚ùå ERROR with {parameter_name} = {current_value}: {e}\")\n",
    "            results_dictionary[current_value] = float('inf')\n",
    "            \n",
    "        # Ensure each test run is independent by cleaning up memory.\n",
    "        # Clean up the DataLoader instance to free up resources.\n",
    "        del loader\n",
    "        # Invoke the garbage collector to release unreferenced memory.\n",
    "        gc.collect()\n",
    "\n",
    "        # Clear the CUDA cache if a GPU is available.\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Return the dictionary of results and the name of the tested parameter.\n",
    "    return results_dictionary, parameter_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2ec7e1-e86b-44d1-9118-9557fc5fce23",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dictionary, parameter_name = custom_experiment(trainset=trainset, device=device)\n",
    "\n",
    "helper_utils.plot_performance_summary(\n",
    "    results_dictionary, \n",
    "    title=f\"DataLoader Performance vs. {parameter_name}\", \n",
    "    xlabel=parameter_name.replace('_', ' ').title(), \n",
    "    ylabel=\"Average Time per Epoch (milliseconds)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8920321-5d0e-4ed7-8016-0608c071132e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This lab demonstrated the process of systematically optimizing PyTorch `DataLoader` parameters to improve data loading performance. \n",
    "Through a series of controlled experiments, you investigated the impact of `num_workers`, `batch_size`, `pin_memory`, and `prefetch_factor` on the total time required to load the CIFAR10 dataset for one epoch.\n",
    "\n",
    "The most significant performance gain was achieved by increasing num_workers from 0 to 2, highlighting the immediate benefit of parallel data loading. Further increases in num_workers and batch_size showed a clear pattern of diminishing returns, where performance gains plateaued and eventually could even degrade due to resource competition and system overhead.\n",
    "\n",
    "Ultimately, this investigation confirms that there is no single \"best\" configuration for a DataLoader. \n",
    "*The optimal settings are highly dependent on the specific interplay between the hardware, dataset complexity, and the parameters themselves*. \n",
    "The key takeaway is not the specific values found in this environment, but rather the experimental methodology itself. \n",
    "By systematically isolating and testing each parameter, one can effectively identify and eliminate data loading bottlenecks, thereby finding the most efficient configuration for any given training scenario."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
