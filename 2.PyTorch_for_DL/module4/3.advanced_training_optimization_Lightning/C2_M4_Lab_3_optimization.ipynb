{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdb66ac9",
   "metadata": {},
   "source": [
    "# Advanced Training Optimization using Lightning\n",
    "\n",
    "Your team has been tasked with optimizing the memory footprint of your deep learning models. With GPU memory being one of the most expensive resources in your infrastructure budget, your manager has requested solutions to reduce peak memory usage without compromising model performance.\n",
    "\n",
    "This lab guides you through implementing and evaluating two powerful memory optimization techniques: **Mixed Precision Training** and **Gradient Accumulation**. These methods can significantly reduce GPU memory consumption while maintaining model accuracy, potentially leading to substantial cost savings in your ML infrastructure.\n",
    "\n",
    "Beyond the optimizations themselves, this notebook will deepen your understanding of the Lightning framework. You will see how to use core components like the `Trainer` and `Callbacks` to structure and control your training process.\n",
    "\n",
    "By the end of this lab, you will have built a complete framework to:\n",
    "* **Build a custom `Callback`** to reliably measure key performance metrics like peak GPU memory and validation accuracy.\n",
    "* **Encapsulate the core training logic** by configuring the Lightning `Trainer` with memory-efficient parameters.\n",
    "* **Systematically benchmark** the different techniques against a baseline to gather empirical data.\n",
    "* **Analyze the trade-offs** between memory usage and model performance to make data-driven decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412755e5-f714-4c75-97cd-f035bb1c8d85",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1b6edb-cd85-4f27-b925-cb9a243afb1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Redirect stderr to a black hole to catch other potential messages\n",
    "class BlackHole:\n",
    "    def write(self, message):\n",
    "        pass\n",
    "    def flush(self):\n",
    "        pass\n",
    "sys.stderr = BlackHole()\n",
    "\n",
    "# Ignore Python-level UserWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdd95af-313a-4ffa-a813-fcbd836bb726",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from lightning.pytorch.callbacks import Callback\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Accuracy\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import helper_utils\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65186287",
   "metadata": {},
   "source": [
    "## Defining the Data and Model with Lightning\n",
    "\n",
    "Before you can benchmark different optimization techniques, you need to define the data pipeline and model architecture. As in the previous lab, you'll use `LightningDataModule` and `LightningModule` to set up these necessary components.\n",
    "\n",
    "### `LightningDataModule` for CIFAR-10\n",
    "\n",
    "* Define the `CIFAR10DataModule` class that handles downloading, preparing, and loading the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c315624-18ac-4093-a958-34f06fbd45b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CIFAR10DataModule(pl.LightningDataModule):\n",
    "    \"\"\"A LightningDataModule for the CIFAR10 dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir='./data', batch_size=128, num_workers=0):\n",
    "        \"\"\"\n",
    "        Initializes the DataModule.\n",
    "\n",
    "        Args:\n",
    "            data_dir (str): Directory to store the data.\n",
    "            batch_size (int): Number of samples per batch.\n",
    "            num_workers (int): Number of subprocesses for data loading.\n",
    "        \"\"\"\n",
    "        # Call the constructor of the parent class (LightningDataModule).\n",
    "        super().__init__()\n",
    "        # Store the data directory path.\n",
    "        self.data_dir = data_dir\n",
    "        # Store the batch size for the DataLoaders.\n",
    "        self.batch_size = batch_size\n",
    "        # Store the number of worker processes for data loading.\n",
    "        self.num_workers = num_workers\n",
    "        # Define a sequence of transformations to be applied to the images.\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"Downloads the CIFAR10 dataset if not already present.\"\"\"\n",
    "        \n",
    "        # Download the training split of CIFAR10.\n",
    "        datasets.CIFAR10(self.data_dir, train=True, download=True)\n",
    "        # Download the testing split of CIFAR10.\n",
    "        datasets.CIFAR10(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"\n",
    "        Assigns train/val datasets for use in dataloaders.\n",
    "\n",
    "        Args:\n",
    "            stage (str, optional): The stage of training (e.g., 'fit', 'test').\n",
    "                               The Lightning Trainer requires this argument, but it is not\n",
    "                               utilized in this implementation as the setup logic is the\n",
    "                               same for all stages. Defaults to None.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create the training dataset instance and apply the transformations.\n",
    "        self.cifar_train = datasets.CIFAR10(self.data_dir, train=True, transform=self.transform)\n",
    "        # Create the validation dataset instance (using the test set) and apply transformations.\n",
    "        self.cifar_val = datasets.CIFAR10(self.data_dir, train=False, transform=self.transform)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        \"\"\"Returns the DataLoader for the training set.\"\"\"\n",
    "        # The DataLoader handles batching, shuffling, and parallel data loading.\n",
    "        return DataLoader(self.cifar_train, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \"\"\"Returns the DataLoader for the validation set.\"\"\"\n",
    "        # Shuffling is not necessary for the validation set.\n",
    "        return DataLoader(self.cifar_val, batch_size=self.batch_size, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f2a427-6966-4ad7-931b-006693992a30",
   "metadata": {},
   "source": [
    "### `LightningModule` for the CNN Model\n",
    "\n",
    "* Define the `CIFAR10LightningModule` class.\n",
    "    * You will use the *efficient* model configuration identified in the profiling lab (`conv_channels=(32, 64, 128)` and `linear_features=512`), as it provides a much better performance baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd0ee9f-499f-4ad1-9fe4-d3f21ac7b919",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CIFAR10LightningModule(pl.LightningModule):\n",
    "    \"\"\"A flexible LightningModule for CIFAR10 image classification.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 learning_rate=1e-3, \n",
    "                 weight_decay=0.01,\n",
    "                 conv_channels=(32, 64, 128),\n",
    "                 linear_features=512,\n",
    "                 num_classes=10):\n",
    "        \"\"\"\n",
    "        Initializes the LightningModule with configurable layer parameters.\n",
    "\n",
    "        Args:\n",
    "            learning_rate: The learning rate for the optimizer.\n",
    "            weight_decay: The weight decay (L2 penalty) for the optimizer.\n",
    "            conv_channels: A tuple specifying the output channels for each\n",
    "                           convolutional block.\n",
    "            linear_features: The number of features in the hidden fully\n",
    "                             connected layer.\n",
    "            num_classes: The number of output classes for the classification task.\n",
    "        \"\"\"\n",
    "        # Call the constructor of the parent class.\n",
    "        super().__init__()\n",
    "        # Save the hyperparameters passed to the constructor.\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Calculate the flattened size of the feature maps after the final\n",
    "        # pooling layer. This is needed to define the input size of the\n",
    "        # first fully connected layer.\n",
    "        flattened_size = self.hparams.conv_channels[-1] * 4 * 4\n",
    "        \n",
    "        # Define the model's architecture using a sequential container.\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, self.hparams.conv_channels[0], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(self.hparams.conv_channels[0], self.hparams.conv_channels[1], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(self.hparams.conv_channels[1], self.hparams.conv_channels[2], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flattened_size, self.hparams.linear_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hparams.linear_features, self.hparams.num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize the loss function.\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Initialize metrics to track accuracy for training and validation.\n",
    "        self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=self.hparams.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x: The input tensor containing a batch of images.\n",
    "\n",
    "        Returns:\n",
    "            The output tensor (logits) from the model.\n",
    "        \"\"\"\n",
    "        # Pass the input through the sequential model.\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx=None):\n",
    "        \"\"\"\n",
    "        Performs a single training step.\n",
    "    \n",
    "        Args:\n",
    "            batch (Any): The data batch from the dataloader.\n",
    "            batch_idx (int, optional): The index of the current batch. The Lightning Trainer\n",
    "                                     requires this argument, but it is not utilized in this\n",
    "                                     implementation. Defaults to None.\n",
    "        \"\"\"\n",
    "        # Unpack the batch into inputs (images) and labels.\n",
    "        inputs, labels = batch\n",
    "        # Perform a forward pass to get the model's predictions (logits).\n",
    "        outputs = self(inputs)\n",
    "        # Calculate the loss.\n",
    "        loss = self.loss_fn(outputs, labels)\n",
    "\n",
    "        # Log the training loss\n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "        # Return the loss to Lightning for backpropagation.\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx=None):\n",
    "        \"\"\"\n",
    "        Performs a single validation step.\n",
    "    \n",
    "        Args:\n",
    "            batch (Any): The data batch from the dataloader.\n",
    "            batch_idx (int, optional): The index of the current batch. The Lightning Trainer\n",
    "                                     requires this argument, but it is not utilized in this\n",
    "                                     implementation. Defaults to None.\n",
    "        \"\"\"\n",
    "        # Unpack the batch into inputs (images) and labels.\n",
    "        inputs, labels = batch\n",
    "        # Perform a forward pass to get the model's predictions (logits).\n",
    "        outputs = self(inputs)\n",
    "        # Calculate the loss.\n",
    "        loss = self.loss_fn(outputs, labels)\n",
    "\n",
    "        # Log the validation loss\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        # Update the validation accuracy metric with the current batch's results.\n",
    "        self.val_accuracy(outputs, labels)\n",
    "        # Log the validation accuracy\n",
    "        self.log(\"val_accuracy\", self.val_accuracy, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configures and returns the model's optimizer.\n",
    "\n",
    "        Returns:\n",
    "            An instance of the optimizer.\n",
    "        \"\"\"\n",
    "        # Create and return the AdamW optimizer.\n",
    "        return optim.AdamW(self.parameters(), lr=self.hparams.learning_rate, weight_decay=self.hparams.weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0b48cb",
   "metadata": {},
   "source": [
    "## Building the Benchmarking Framework\n",
    "\n",
    "Now that the model and data are defined, you need to build a system to automate the process of running and comparing the different optimization experiments. This will involve creating a set of reusable functions and components that work together to handle everything from performance measurement to executing the training loop.\n",
    "\n",
    "### Creating a Custom Callback for Measurement\n",
    "\n",
    "To measure the performance of each experiment reliably, you need a consistent way to collect data. Lightning provides an elegant solution for this called <code>[Callbacks](https://lightning.ai/docs/pytorch/stable/extensions/callbacks.html)</code>. A `Callback` is an object that can hook into the training process at various points, like the beginning of an epoch or the end of a training run, to execute your custom code. This is an essential tool for logging, monitoring, or in this case, performance measurement.\n",
    "\n",
    "You will create a `PerformanceCallback` class to gather all the metrics needed for the analysis:\n",
    "* **Peak memory usage**: The maximum GPU memory used at any single point during training.\n",
    "* **Validation accuracy**: The model's performance on the unseen validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce38eb5-7e4b-4f1a-b309-0afdc2fa82be",
   "metadata": {},
   "source": [
    "* Define `PerformanceCallback`:\n",
    "    * `__init__`: The constructor where you will initialize variables to store the data you collect.\n",
    "    * `on_train_start` & `on_train_end`: These hooks are used for memory monitoring. `on_train_start` resets the GPU's peak memory counter, and `on_train_end` records the final peak memory usage.\n",
    "    * `on_validation_epoch_end`: This hook runs after each validation epoch to save the latest validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ff7c8c-ed56-499b-978b-a268f601f1b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PerformanceCallback(Callback):\n",
    "    \"\"\"\n",
    "    A Lightning Callback to collect key performance metrics during the training\n",
    "    and validation lifecycle.\n",
    "\n",
    "    This callback measures:\n",
    "    - Validation accuracy\n",
    "    - Peak GPU memory usage during training\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes storage for the performance metrics to be collected.\"\"\"\n",
    "        # Initialize a list to store performance metrics\n",
    "        self.metrics = []\n",
    "        \n",
    "        # Initialize an attribute to store peak memory usage\n",
    "        self.peak_memory_mb = None\n",
    "\n",
    "    def on_train_start(self, trainer, pl_module=None):\n",
    "        \"\"\"\n",
    "        Resets peak GPU memory statistics at the start of training.\n",
    "    \n",
    "        Args:\n",
    "            trainer (pl.Trainer): The main Lightning Trainer instance.\n",
    "            pl_module (pl.LightningModule, optional): The LightningModule being trained.\n",
    "                                                     Required by the callback hook but not\n",
    "                                                     utilized here. Defaults to None.\n",
    "        \"\"\"\n",
    "        # Check if CUDA is available for GPU operations\n",
    "        if torch.cuda.is_available():\n",
    "            # Reset the peak memory statistics on the root device\n",
    "            torch.cuda.reset_peak_memory_stats(trainer.strategy.root_device)\n",
    "            # Clear the CUDA cache to free up memory\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module=None):\n",
    "        \"\"\"\n",
    "        Aggregates and logs metrics at the end of a validation epoch.\n",
    "    \n",
    "        Args:\n",
    "            trainer (pl.Trainer): The Lightning Trainer instance.\n",
    "            pl_module (pl.LightningModule, optional): The LightningModule being validated.\n",
    "                                                     Required by the callback hook but not\n",
    "                                                     utilized here. Defaults to None.\n",
    "        \"\"\"\n",
    "        # Check if the trainer is not in sanity checking mode\n",
    "        if not trainer.sanity_checking:\n",
    "            # Get the metrics collected by the trainer's callbacks\n",
    "            metrics = trainer.callback_metrics\n",
    "            # Append a dictionary with the validation accuracy to the metrics list\n",
    "            self.metrics.append({\n",
    "                \"val_accuracy\": metrics[\"val_accuracy\"].item() * 100\n",
    "            })\n",
    "\n",
    "    def on_train_end(self, trainer, pl_module=None):\n",
    "        \"\"\"\n",
    "        Records the peak GPU memory usage at the end of training.\n",
    "    \n",
    "        Args:\n",
    "            trainer (pl.Trainer): The PyTorch Lightning Trainer instance.\n",
    "            pl_module (pl.LightningModule, optional): The LightningModule that was trained.\n",
    "                                                     Required by the callback hook but not\n",
    "                                                     utilized here. Defaults to None.\n",
    "        \"\"\"\n",
    "        # Check if CUDA is available for GPU operations\n",
    "        if torch.cuda.is_available():\n",
    "            # Get the maximum memory allocated on the root device\n",
    "            peak_memory = torch.cuda.max_memory_allocated(trainer.strategy.root_device)\n",
    "            # Convert the peak memory from bytes to megabytes\n",
    "            self.peak_memory_mb = peak_memory / 1024**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3877961a-b21e-4c91-8fa2-e59158338060",
   "metadata": {},
   "source": [
    "### The Core Training Function\n",
    "\n",
    "To run your optimization experiments in a clean and repeatable way, you will encapsulate the main training logic into a dedicated function. This function's primary job is to configure the Lightning <code>[Trainer](https://lightning.ai/docs/pytorch/stable/common/trainer.html)</code>, the component responsible for handling the entire training process.\n",
    "\n",
    "To keep the code organized and to emphasize this core logic, you will define the `Trainer` setup in a separate `run_training` function. Each optimization experiment you run later will call this same function, just with different configuration settings.\n",
    "\n",
    "**The Lightning Advantage**\n",
    "\n",
    "The Lightning `Trainer` is the main event; it completely automates and handles the entire training process. This is your training loop. You can forget writing `for` loops, manually moving data to the GPU, and calling `.zero_grad()`, `.backward()`, and `.step()`. The Trainer orchestrates everything. You simply configure it and call `.fit()`.\n",
    "\n",
    "#### Encapsulating the Training Run\n",
    "\n",
    "* Define the `run_training` function, which contains the complete logic for a training run. It initializes a `Trainer`, runs `.fit()`, and then returns the `trainer` object, containing all the state and results from the training.\n",
    "    * `precision`:  Controls the numerical precision for training (e.g., `'16-mixed'` to enable mixed precision).\n",
    "    * `accumulate_grad_batches`: Specifies the number of batches to process before updating the model's weights, enabling gradient accumulation.\n",
    "    * `callbacks`: Takes a list of `Callback` objects. You will pass your `PerformanceCallback` here to collect metrics during the run.\n",
    "* Returning the `trainer` is important because it contains all the state and results from the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3ddb77-f6be-4bc0-89ec-0af437a813bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_training(model, data_module, num_epochs, precision, grad_accum, performance_callback):\n",
    "    \"\"\"\n",
    "    Configures and runs a Lightning training process.\n",
    "\n",
    "    Args:\n",
    "        model (pl.LightningModule): The model to be trained.\n",
    "        data_module (pl.LightningDataModule): The data module that provides the datasets.\n",
    "        num_epochs (int): The total number of epochs for training.\n",
    "        precision (str): The numerical precision for training ('32-true', '16-mixed').\n",
    "        grad_accum (int): The number of batches to accumulate gradients over.\n",
    "        performance_callback (pl.Callback): A callback to measure performance metrics.\n",
    "\n",
    "    Returns:\n",
    "        pl.Trainer: The trainer instance after fitting is complete.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize a Lightning Trainer with specific parameters\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=num_epochs,               # Set the maximum number of training epochs\n",
    "        accelerator=\"auto\",                  # Automatically select the best accelerator (e.g., CPU, GPU)\n",
    "        devices=1,                           # Use a single device for training\n",
    "        precision=precision,                 # Set the numerical precision for the training process\n",
    "        accumulate_grad_batches=grad_accum,  # Configure the number of batches to accumulate gradients before updating weights\n",
    "        callbacks=[performance_callback],    # Provide a list of callbacks to be used during training\n",
    "        logger=False,                        # Disable logging for this training run\n",
    "        enable_progress_bar=True,            # Enable the progress bar to show training progress\n",
    "        enable_model_summary=False,          # Disable the model summary\n",
    "        enable_checkpointing=False           # Disable model checkpointing\n",
    "    )\n",
    "    \n",
    "    # Start the training process using the provided model and data module\n",
    "    trainer.fit(model, data_module)\n",
    "    \n",
    "    # Return the configured and trained trainer instance\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bad7f91-eeb9-47f6-8969-7901ee322e85",
   "metadata": {},
   "source": [
    "### Orchestrating the Experiments\n",
    "\n",
    "Now you will create the main `run_optimization` function. This function will manage all the necessary parts for each experiment from start to finish. It will:\n",
    "\n",
    "* **Set up the experiment**: Create new instances of your `model` and the `PerformanceCallback`.\n",
    "* **Execute the training**: Call the `run_training` function to handle the actual training process.\n",
    "* **Process the results**: Extract metrics from your callback and format everything into a dictionary.\n",
    "* **Return the findings**: Return the formatted results for later comparison and plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b09f90-430e-41e1-b823-45eba6dfe4bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_optimization(name, data_module, num_epochs, precision, grad_accum):\n",
    "    \"\"\"\n",
    "    Orchestrates and runs a single, complete optimization experiment.\n",
    "\n",
    "    Args:\n",
    "        name (str): The display name for the experiment (e.g., \"Mixed Precision\").\n",
    "        data_module (pl.LightningDataModule): The data module for training and validation.\n",
    "        num_epochs (int): The number of epochs to train for.\n",
    "        precision (str): The training precision to use ('32-true', '16-mixed').\n",
    "        grad_accum (int): The number of batches for gradient accumulation.\n",
    "        \n",
    "    Returns:\n",
    "        tuple[dict, dict]: A tuple containing two dictionaries:\n",
    "        1. The summarized results for the comparison table.\n",
    "        2. The detailed data required for generating plots.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set up the experiment\n",
    "    print(f\"\\n--- Running Experiment: {name} (For {num_epochs} Epochs) ---\")\n",
    "    \n",
    "    model = CIFAR10LightningModule()\n",
    "    performance_callback = PerformanceCallback()\n",
    "\n",
    "    # Execute the training\n",
    "    trainer = run_training(\n",
    "        model=model,\n",
    "        data_module=data_module,\n",
    "        num_epochs=num_epochs,\n",
    "        precision=precision,\n",
    "        grad_accum=grad_accum,\n",
    "        performance_callback=performance_callback,\n",
    "    )\n",
    "\n",
    "    # Process the results\n",
    "    accuracies = [m[\"val_accuracy\"] for m in performance_callback.metrics]\n",
    "    \n",
    "    current_results = {\n",
    "        \"optimization\": name,\n",
    "        \"final_acc\": accuracies[-1],\n",
    "        \"peak_mem_mb\": performance_callback.peak_memory_mb,\n",
    "    }\n",
    "    \n",
    "    plot_info = {**current_results, \"accuracies\": accuracies}\n",
    "    \n",
    "    # Return the findings\n",
    "    return current_results, plot_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a764bbc9-f48a-42fe-98b8-94d762196a58",
   "metadata": {},
   "source": [
    "### Running the Optimization Experiments\n",
    "\n",
    "With all the runner functions in place, it's time to execute the experiments. First, you'll define the shared configurations for all the upcoming experimental runs:\n",
    "\n",
    "* `num_epochs`: Set the number of epochs for each optimization run.\n",
    "* `batch_size` and `num_workers`: You will use an effective `batch_size=256` and `num_workers=4` to remain within the memory limits of this environment.\n",
    "* `results[]`: An empty list to collect the final, summarized results from each experiment.\n",
    "* `plot_data[]`: A second empty list to store data needed for visualizations.\n",
    "* `data_module`: An instance of your `CIFAR10DataModule`, configured with the `batch_size` and `num_workers` you just defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c100bdd-1c5d-4527-b4cd-55748ca7aaec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the number of epochs for each run\n",
    "num_epochs = 5\n",
    "# Define the number of samples to be processed in each batch\n",
    "batch_size = 256\n",
    "# Set the number of parallel processes for data loading\n",
    "num_workers = 4\n",
    "\n",
    "# Initialize an empty list to store the final summary results for the comparison table\n",
    "results = []\n",
    "# Initialize an empty list to store detailed data for generating plots\n",
    "plot_data = []\n",
    "\n",
    "# Create an instance of the DataModule, configuring it with the specified batch size and number of workers\n",
    "data_module = CIFAR10DataModule(batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4aa8a4-f63f-45ec-887b-b9b9cc003ffb",
   "metadata": {},
   "source": [
    "Now you will see the different training optimizations in action. You are going to run several experiments to directly compare their effects. It is important to remember that no single technique is universally better than another. Each optimization presents a distinct balance of benefits and drawbacks, making it suitable for different scenarios. Your choice will depend on what you prioritize for your project, such as achieving maximum accuracy or the most efficient use of resources.\n",
    "\n",
    "The table below summarizes the different experiments.  Note that all experiment have the same `Effective Batch Size` of 256.\n",
    "\n",
    "|Name|Effective Batch Size|Batch Size|Gradient Accumulation|Precision|\n",
    "|:---|---|---|---|---|\n",
    "|Standard|256|256|1|32-bit|\n",
    "|Mixed Precision|256|256|1|16-bit|\n",
    "|Gradient Accumulation (Effective BS: 256-128)|256|128|2|32-bit|\n",
    "|Gradient Accumulation (Effective BS: 256-64)|256|64|4|32-bit|\n",
    "|Combined (Effective BS: 256-128)|256|128|2|16-bit|\n",
    "|Combined (Effective BS: 256-64)|256|64|4|16-bit|\n",
    "\n",
    "\n",
    "#### Standard Training\n",
    "\n",
    "This first run establishes your baseline. It is the standard, most direct approach to training a model. You will execute a training process without any advanced optimizations, which provides an essential reference point to fairly measure the effectiveness of the other techniques.\n",
    "\n",
    "You should consider this the default method for any new project. It is the best choice when your primary goal is to ensure maximum numerical stability and accuracy, and when you are not constrained by training time or available GPU memory.\n",
    "\n",
    "The primary benefit is its high precision, ensuring that all calculations are very accurate and your results are reliable. However, its main drawback is that it is resource intensive. This method uses the most memory and can be significantly slower than other techniques, especially on modern hardware designed to accelerate lower precision computations.\n",
    "\n",
    "**Standard Precision (`32-true`)**\n",
    "\n",
    "This setting tells the `Trainer` to perform all calculations using **full 32 bit floating point numbers**. This is the default data type for most deep learning operations and offers a high degree of numerical accuracy.\n",
    "\n",
    "* **precision**: Set to `\"32-true\"`.\n",
    "* **grad_accum**: Set to `1`, which means the model's weights will be updated after every single batch. This is normal, non-accumulated training behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71024d83-a446-4c77-8faa-3ca735d5beb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res, p_data = run_optimization(\n",
    "    name=\"Standard\",\n",
    "    precision=\"32-true\",\n",
    "    grad_accum=1,\n",
    "    data_module=data_module,\n",
    "    num_epochs=num_epochs,\n",
    ")\n",
    "\n",
    "results.append(res)\n",
    "plot_data.append(p_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7363bfbe-fa75-406e-8bfd-c7630450d707",
   "metadata": {},
   "source": [
    "#### Mixed Precision Training\n",
    "\n",
    "In this second run, you will explore **Mixed Precision Training**. This technique cleverly combines the use of two different numerical precisions. It aims to speed up training and reduce GPU memory usage by performing many computations using a fast **16 bit floating point** format. To maintain numerical stability, it strategically keeps critical operations, like weight updates, in the standard 32 bit format.\n",
    "\n",
    "You should use this technique when your goal is to **train faster** or **reduce the memory footprint** of your model. The speed benefits are especially noticeable on modern GPUs with hardware designed to accelerate lower precision math. It's an excellent option for training very large models that might not otherwise fit into your GPU's memory. The potential drawback of this technique is a slight risk of reduced final accuracy due to the lower precision used in some calculations. \n",
    "\n",
    "**Mixed Precision (`16-mixed`)**\n",
    "\n",
    "By setting the precision to `\"16-mixed\"`, you instruct Lightning to automatically handle the process. It will strategically use half precision (16 bit) for operations like matrix multiplications and full precision (32 bit) for operations where high accuracy is needed, like weight updates.\n",
    "\n",
    "* **precision**: Set to `\"16-mixed\"` to enable automatic mixed precision training.\n",
    "* **grad_accum**: This remains `1`, as you are only measuring the effect of mixed precision in this run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d3f81b-cfde-46eb-ba2d-11c30c67a5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "res, p_data = run_optimization(\n",
    "    name=\"Mixed Precision\",\n",
    "    precision=\"16-mixed\",\n",
    "    grad_accum=1,\n",
    "    data_module=data_module,\n",
    "    num_epochs=num_epochs,\n",
    ")\n",
    "\n",
    "results.append(res)\n",
    "plot_data.append(p_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70103ce-a793-4efe-b097-63bd873d8dfa",
   "metadata": {},
   "source": [
    "#### Gradient Accumulation\n",
    "\n",
    "Sometimes you want the benefits of reducing the batch size, to reduce GPU memory without compromising your accuracy. Gradient Accumulation is the solution for this exact problem. This technique works by processing several smaller batches one after another and adding up their gradients. Only after a set number of these small batches does your model perform a single weight update, which effectively simulates training with a much larger batch size without the high memory cost.\n",
    "\n",
    "You should use this technique when training is unstable and you believe a larger batch size would help, but you are limited by your hardware's memory. It is a very common and effective strategy for training large models, like Transformers, that often require large batches to converge well.\n",
    "\n",
    "The main benefit is achieving more stable training updates, which can lead to better model convergence, all while using less memory than a true large batch would require. The primary drawback, however, is that it can increase the total training time. Additionally, you should be cautious when using this technique with layers like Batch Normalization, as it can sometimes affect their performance.\n",
    "\n",
    "**Effective Batch Size (Effective BS)**\n",
    "\n",
    "This metric tells you the size of the batch you are simulating. The formula is:\n",
    "\n",
    "> Effective Batch Size = batch_size * grad_accum\n",
    "\n",
    "With a `batch_size` of 128 and `grad_accum` of 2, your effective batch size is 256.\n",
    "\n",
    "* **precision**: Set to `\"32-true\"` to measure only the effect of gradient accumulation.\n",
    "* **grad_accum**: Set to `2`. This tells the `Trainer` to process 2 batches before updating the model's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09a72ef-1294-4db3-bee7-28a0a717d279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data module setup\n",
    "batch_size = 128\n",
    "data_module = CIFAR10DataModule(batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "# Gradient accumulation setup\n",
    "effective_batch_size = 256\n",
    "grad_accum = effective_batch_size // batch_size\n",
    "\n",
    "res, p_data = run_optimization(\n",
    "    name=\"Gradient Accumulation (Effective BS: 256-128)\",\n",
    "    precision=\"32-true\",\n",
    "    grad_accum=grad_accum,\n",
    "    data_module=data_module,\n",
    "    num_epochs=num_epochs,\n",
    ")\n",
    "\n",
    "results.append(res)\n",
    "plot_data.append(p_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd05df2-a654-4143-9eac-7895207b87da",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Your Task**\n",
    "\n",
    "Now, perform another gradient accumulation run, with the same effective batch size of 256 but a different batch size.\n",
    "\n",
    "* **precision**: To ensure you are only measuring the impact of gradient accumulation, which precision setting should be used? (*Hint*: not mixed precision)\n",
    "\n",
    "* **grad_accum**: Given a `batch_size` of 64, what `grad_accum` value should you use to reach the target of 256?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7cb20f-ca19-474d-84ae-5cefaf96b8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Data module setup\n",
    "    batch_size = 64\n",
    "    data_module = CIFAR10DataModule(batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    res, p_data = run_optimization(\n",
    "        name=\"Gradient Accumulation (Effective BS: 256-64)\",\n",
    "\n",
    "        # Set precision for gradient accumulation\n",
    "        ### Add your code here\n",
    "        precision=\"32-true\",\n",
    "        # Set the accumulation steps to achieve an effective batch size of 256\n",
    "        ### Add your code here\n",
    "        grad_accum=4,\n",
    "\n",
    "        data_module=data_module,\n",
    "        num_epochs=num_epochs,\n",
    "    )\n",
    "\n",
    "    results.append(res)\n",
    "    plot_data.append(p_data)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\033[91mSomething went wrong, try again!\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe03015c-4660-4257-b436-49ea7c1ac0b6",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><span style=\"color:green;\"><strong>Solution (Click here to expand)</strong></span></summary>\n",
    "\n",
    "```python\n",
    "try:\n",
    "    # Data module setup\n",
    "    batch_size = 64\n",
    "    data_module = CIFAR10DataModule(batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    res, p_data = run_optimization(\n",
    "        name=\"Gradient Accumulation (Effective BS: 256-64)\",\n",
    "\n",
    "        # Set precision for gradient accumulation\n",
    "        precision=\"32-true\",\n",
    "        # Set the accumulation steps to achieve an effective batch size of 256\n",
    "        grad_accum=4,\n",
    "\n",
    "        data_module=data_module,\n",
    "        num_epochs=num_epochs,\n",
    "    )\n",
    "\n",
    "    results.append(res)\n",
    "    plot_data.append(p_data)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\033[91mSomething went wrong, try again!\")\n",
    "    raise e\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6786bcf-79b6-47ad-87fb-ae32a7de832e",
   "metadata": {},
   "source": [
    "#### Combining Optimizations\n",
    "\n",
    "For the next run, you will combine mixed precision and gradient accumulation. This is a powerful approach where you use both techniques at the same time to create a highly efficient training configuration. The goal is to see if you can get the synergistic benefits of both working together.\n",
    "\n",
    "You should consider this combined strategy when you are facing significant constraints in GPU memory. It is especially useful for training very large, state of the art models that require a large batch size for stable convergence but are too big to fit in memory using standard precision.\n",
    "\n",
    "The primary benefit is achieving maximum resource efficiency. Mixed precision reduces the memory footprint and speeds up computation, while gradient accumulation allows you to train with a stable, large effective batch size. The potential drawback is that the risks of each technique are combined. You must be mindful of a potential accuracy dip from mixed precision alongside the possibility of longer training times from gradient accumulation. As always, benchmarking is essential to confirm this combination yields the best overall result for your specific model and goal.\n",
    "\n",
    "* **precision**: Set to `\"16-mixed\"` to leverage the speed of half precision computation.\n",
    "* **grad_accum**: Set to `2` to simulate a batch size of 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0294b24f-7bb2-4655-bc94-a110ac565192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data module setup\n",
    "batch_size = 128\n",
    "data_module = CIFAR10DataModule(batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "# Gradient accumulation setup\n",
    "effective_batch_size = 256\n",
    "grad_accum = effective_batch_size // batch_size\n",
    "\n",
    "res, p_data = run_optimization(\n",
    "    name=\"Combined (Effective BS: 256-128)\",\n",
    "    precision=\"16-mixed\",\n",
    "    grad_accum=grad_accum,\n",
    "    data_module=data_module,\n",
    "    num_epochs=num_epochs,\n",
    ")\n",
    "\n",
    "results.append(res)\n",
    "plot_data.append(p_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd04ad3-52d1-44b6-9a6c-637aa7b7a10e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Your Task**\n",
    "\n",
    "For the final experiment, you'll combine both optimizations again, but this time with a batch size of 64 to obtain an effective batch size of 256.\n",
    "\n",
    "* **precision**: Since this is a combined optimization run, which precision setting should you use to get the memory benefits?\n",
    "\n",
    "* **grad_accum**: Given a `batch_size` of 64, what `grad_accum` value should you use to reach the target effective batch size of 256?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6d45da-61cb-4dc9-9aca-0d1a4b82cc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Data module setup\n",
    "    batch_size = 64\n",
    "    data_module = CIFAR10DataModule(batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    res, p_data = run_optimization(\n",
    "        name=\"Combined (Effective BS: 256-64)\",\n",
    "\n",
    "        # Use the setting for mixed precision\n",
    "        ### Add your code here\n",
    "        precision=\"16-mixed\", \n",
    "        # Set the accumulation steps to achieve an effective batch size of 256\n",
    "        ### Add your code here  \n",
    "        grad_accum= 4,   \n",
    "        \n",
    "        data_module=data_module,\n",
    "        num_epochs=num_epochs,\n",
    "    )\n",
    "\n",
    "    results.append(res)\n",
    "    plot_data.append(p_data)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\033[91mSomething went wrong, try again!\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06811c5-af04-44e5-8fc7-ee8fbc2da584",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><span style=\"color:green;\"><strong>Solution (Click here to expand)</strong></span></summary>\n",
    "\n",
    "```python\n",
    "try:\n",
    "    # Data module setup\n",
    "    batch_size = 64\n",
    "    data_module = CIFAR10DataModule(batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    res, p_data = run_optimization(\n",
    "        name=\"Combined (Effective BS: 256-64)\",\n",
    "\n",
    "        # Use the setting for mixed precision\n",
    "        precision=\"16-mixed\",\n",
    "        # Set the accumulation steps to achieve an effective batch size of 256\n",
    "        grad_accum=4,\n",
    "        \n",
    "        data_module=data_module,\n",
    "        num_epochs=num_epochs,\n",
    "    )\n",
    "\n",
    "    results.append(res)\n",
    "    plot_data.append(p_data)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\033[91mSomething went wrong, try again!\")\n",
    "    raise e\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8ef668-5638-41d3-8748-e6541933fcfe",
   "metadata": {},
   "source": [
    "### Analyzing the Results\n",
    "\n",
    "After running all the experiments, the final step is to display and visualize the collected data to understand the trade offs between the different strategies.\n",
    "\n",
    "* First, a summary table provides a detailed numerical look at the key metrics from every run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97990d2f-83d2-433b-b4ab-82c3f0d08183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the comparison table of all experiment results\n",
    "helper_utils.optimization_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be71bb33-8764-4538-b960-f60b8e244767",
   "metadata": {},
   "source": [
    "**Optimization Technique Abbreviations**\n",
    "\n",
    "To improve readability in the visualizations, the following abbreviations are used for each optimization technique:\n",
    "\n",
    "* **STD** - Standard\n",
    "* **MP** - Mixed Precision  \n",
    "* **GA256-128** - Gradient Accumulation (Effective BS: 256-128)\n",
    "* **GA256-64** - Gradient Accumulation (Effective BS: 256-64)\n",
    "* **Comb256-128** - Combined (Effective BS: 256-128)\n",
    "* **Comb256-64** - Combined (Effective BS: 256-64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e988ab59-6746-4d21-b7d3-07ba05d716e7",
   "metadata": {},
   "source": [
    "* Next, a bar chart compares the **final accuracy** (validation) of each technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fe558b-0848-4bb6-b4cc-746ad50310dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the final accuracy to compare each optimization technique\n",
    "helper_utils.plot_final_accuracy(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8418f6-8339-4b0b-ab51-cf8722aa407a",
   "metadata": {},
   "source": [
    "* The final trade off to consider is GPU memory usage. This chart visualizes the **peak memory** consumed during each run, which is critical for understanding which techniques are the most resource efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bf5022-1195-4160-83a9-e1eeb22a9611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the peak memory usage for each optimization\n",
    "helper_utils.plot_peak_memory(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6211f7-6ac8-4b81-a346-24953464b16a",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "The graph above shows that Mixed Precision reduces memory usage. Likewise, using smaller batch sizes (128 and 64) while maintaining an effective batch size of 256 also lowers memory consumption. Finally, combining Gradient Accumulation with Mixed Precision reduces memory usage even further, all without compromising validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac29005",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations on completing the lab! You have successfully built a complete benchmarking framework and used it to measure the impact of powerful memory optimization techniques. The results from your experiments demonstrate how to effectively reduce peak memory usage (infrastructure costs) while maintaining model performance. \n",
    "\n",
    "Also, you've deepened your skills with Lightning. You've seen how to use a custom **`Callback`** for detailed monitoring and how to leverage the **`Trainer`** to enable complex memory optimizations with simple, declarative parameters.\n",
    "\n",
    "You now have a practical, data-driven approach for making cost-effective decisions about model training. By systematically benchmarking different strategies, you can ensure you are making the most efficient use of your GPU memory budgetâ€”an essential skill for managing infrastructure costs while scaling your work to larger models and datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
