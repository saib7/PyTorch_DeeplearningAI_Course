{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8365a071-dd85-46e2-93ac-fd79c9eb3a83",
   "metadata": {},
   "source": [
    "# Introduction to Lightning and Performance Profiling\n",
    "\n",
    "When working with PyTorch, you often spend significant time writing boilerplate code for training loops, data handling, and device management. This repetitive work can distract you from the main goal: designing and training your model.\n",
    "\n",
    "This is where **[Lightning](https://lightning.ai/) (formerly PyTorch Lightning)** comes in. It is a high level framework that organizes your PyTorch code and automates the engineering, letting you focus on the research. This lab will introduce you to Lightning's structure and show you how it simplifies advanced tasks like performance tuning.\n",
    "\n",
    "You will also be introduced to **Profiling**, a technique used to analyze your code's performance and find \"bottlenecks\" that slow down your training. By the end of this lab, you will have used Lightning to diagnose and fix a real performance issue, giving you a complete workflow for building more efficient models.\n",
    "\n",
    "Specifically, you will:\n",
    "\n",
    "* Organize standard PyTorch code into Lightning's `LightningDataModule` and `LightningModule`.\n",
    "* Run a training loop to establish a baseline for speed and accuracy.\n",
    "* Use the integrated Profiler to diagnose a \"model complexity\" bottleneck.\n",
    "* Verify your fix by profiling a more efficient model and comparing the results.\n",
    "* Evaluate the final trade-off between training speed and model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412755e5-f714-4c75-97cd-f035bb1c8d85",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1b6edb-cd85-4f27-b925-cb9a243afb1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Redirect stderr to a black hole to catch other potential messages\n",
    "class BlackHole:\n",
    "    def write(self, message):\n",
    "        pass\n",
    "    def flush(self):\n",
    "        pass\n",
    "sys.stderr = BlackHole()\n",
    "\n",
    "# Ignore Python-level UserWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdd95af-313a-4ffa-a813-fcbd836bb726",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from lightning.pytorch.profilers import PyTorchProfiler\n",
    "from torch.profiler import schedule\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Accuracy\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import helper_utils\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65186287",
   "metadata": {},
   "source": [
    "## Defining the Data and Model with Lightning\n",
    "\n",
    "With your environment set up, it's time to structure the core components of your project. Lightning provides an organized approach by separating data-handling from model logic. You'll define these in the next two steps.\n",
    "\n",
    "### Step 1: Simplifying Data Loading with the `LightningDataModule`\n",
    "\n",
    "In your previous work with PyTorch, you have seen the standard data pipeline: you define a `Dataset` and then wrap it in a `DataLoader`. This often requires you to manage separate `DataLoader` instances for your training and validation sets, which can scatter your data-handling code.\n",
    "\n",
    "Lightning simplifies and organizes this entire process by encapsulating all data-related logic into a single, reusable class called a <code>[LightningDataModule](https://lightning.ai/docs/pytorch/stable/data/datamodule.html)</code>. This class becomes the central hub for sourcing, preparing, and delivering your data, which keeps your main training script clean and focused on the model.\n",
    "\n",
    "* Define the `CIFAR10DataModule` class by implementing these essential methods:\n",
    "    * **`__init__`**: The constructor where you define specifications for your data pipeline, such as `batch_size`, `num_workers`, and data `transforms`.\n",
    "    * **`prepare_data()`**: This method handles initial, one time setup like downloading the dataset. Lightning ensures this only happens on a single process to avoid conflicts.\n",
    "        * For this class, this method will check if the **CIFAR10** dataset is present and download it if not.\n",
    "    * **`setup()`**: Prepares the data for use by creating your training and validation `Dataset` splits. The `stage` argument *could* be used to set up different data for different stages (e.g., `'fit'`, `'validate'`, `'test'`). However, in this case, the setup is the same for all stages and, by default, prepares the data needed for the `'fit'` stage.\n",
    "        * Think of the **'fit'** stage as the combined training and validation loop. The **'test'** or **'validate'** stages are similar to a standalone evaluation phase in standard PyTorch.\n",
    "    * **`train_dataloader()` & `val_dataloader()`**: These methods return the familiar PyTorch `DataLoader` instances, configured with essential performance settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c315624-18ac-4093-a958-34f06fbd45b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CIFAR10DataModule(pl.LightningDataModule):\n",
    "    \"\"\"A LightningDataModule for the CIFAR10 dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir='./data', batch_size=128, num_workers=0):\n",
    "        \"\"\"\n",
    "        Initializes the DataModule.\n",
    "\n",
    "        Args:\n",
    "            data_dir (str): Directory to store the data.\n",
    "            batch_size (int): Number of samples per batch.\n",
    "            num_workers (int): Number of subprocesses for data loading.\n",
    "        \"\"\"\n",
    "        # Call the constructor of the parent class (LightningDataModule).\n",
    "        super().__init__()\n",
    "        # Store the data directory path.\n",
    "        self.data_dir = data_dir\n",
    "        # Store the batch size for the DataLoaders.\n",
    "        self.batch_size = batch_size\n",
    "        # Store the number of worker processes for data loading.\n",
    "        self.num_workers = num_workers\n",
    "        # Define a sequence of transformations to be applied to the images.\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"Downloads the CIFAR10 dataset if not already present.\"\"\"\n",
    "        \n",
    "        # Download the training split of CIFAR10.\n",
    "        datasets.CIFAR10(self.data_dir, train=True, download=True)\n",
    "        # Download the testing split of CIFAR10.\n",
    "        datasets.CIFAR10(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"\n",
    "        Assigns train/val datasets for use in dataloaders.\n",
    "\n",
    "        Args:\n",
    "            stage (str, optional): The stage of training (e.g., 'fit', 'test').\n",
    "                               The Lightning Trainer requires this argument, but it is not\n",
    "                               utilized in this implementation as the setup logic is the\n",
    "                               same for all stages. Defaults to None.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create the training dataset instance and apply the transformations.\n",
    "        self.cifar_train = datasets.CIFAR10(self.data_dir, train=True, transform=self.transform)\n",
    "        # Create the validation dataset instance (using the test set) and apply transformations.\n",
    "        self.cifar_val = datasets.CIFAR10(self.data_dir, train=False, transform=self.transform)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        \"\"\"Returns the DataLoader for the training set.\"\"\"\n",
    "        # The DataLoader handles batching, shuffling, and parallel data loading.\n",
    "        return DataLoader(self.cifar_train, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \"\"\"Returns the DataLoader for the validation set.\"\"\"\n",
    "        # Shuffling is not necessary for the validation set.\n",
    "        return DataLoader(self.cifar_val, batch_size=self.batch_size, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f2a427-6966-4ad7-931b-006693992a30",
   "metadata": {},
   "source": [
    "### Stage 2: Structuring Your Model with the `LightningModule`\n",
    "\n",
    "Now that you've organized your data handling, it's time to define the model itself. In standard PyTorch, you typically define your model's architecture in a class that inherits from `nn.Module`. The Lightning equivalent is the <code>[LightningModule](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html)</code>. This class is where you will organize all of your model-related code, from the layer definitions to the logic for a single training step.\n",
    "\n",
    "By separating the model's logic in the `LightningModule` from the training engine, Lightning lets you build powerful, reusable models without worrying about the complex engineering that makes them run.\n",
    "\n",
    "* Define `CIFAR10LightningModule` class with these key methods:\n",
    "    * **`__init__()`**: The constructor where you define your neural network architecture, loss function, and any metrics.\n",
    "    * **`forward()`**: This method defines the forward pass of your model, just like in a standard PyTorch module.\n",
    "    * **`training_step()`**: Here, you'll define the logic for a single training batch. You perform the forward pass, calculate the loss, and log metrics. You simply need to return the loss; Lightning's automation handles the backpropagation and weight updates for you.\n",
    "    * **`validation_step()`**: This contains the same logic, but for your validation data.\n",
    "    * **`configure_optimizers()`**: In this method, you select and return the optimizer and its hyperparameters. Lightning will use this to update your model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd0ee9f-499f-4ad1-9fe4-d3f21ac7b919",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CIFAR10LightningModule(pl.LightningModule):\n",
    "    \"\"\"A flexible LightningModule for CIFAR10 image classification.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 learning_rate=1e-3,\n",
    "                 weight_decay=0.01,\n",
    "                 conv_channels=(256, 512, 1024),\n",
    "                 linear_features=2048,\n",
    "                 num_classes=10):\n",
    "        \"\"\"\n",
    "        Initializes the LightningModule with configurable layer parameters.\n",
    "\n",
    "        Args:\n",
    "            learning_rate: The learning rate for the optimizer.\n",
    "            weight_decay: The weight decay (L2 penalty) for the optimizer.\n",
    "            conv_channels: A tuple specifying the output channels for each\n",
    "                           convolutional block.\n",
    "            linear_features: The number of features in the hidden fully\n",
    "                             connected layer.\n",
    "            num_classes: The number of output classes for the classification task.\n",
    "        \"\"\"\n",
    "        # Call the constructor of the parent class.\n",
    "        super().__init__()\n",
    "        # Save the hyperparameters passed to the constructor. This makes them\n",
    "        # accessible via `self.hparams` and logs them automatically.\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Calculate the flattened size of the feature maps after the final\n",
    "        # pooling layer. This is needed to define the input size of the\n",
    "        # first fully connected layer.\n",
    "        flattened_size = self.hparams.conv_channels[-1] * 4 * 4\n",
    "        \n",
    "        # Define the model's architecture using a sequential container.\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, self.hparams.conv_channels[0], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(self.hparams.conv_channels[0], self.hparams.conv_channels[1], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(self.hparams.conv_channels[1], self.hparams.conv_channels[2], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flattened_size, self.hparams.linear_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hparams.linear_features, self.hparams.num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize the loss function.\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Initialize metrics to track accuracy for training and validation.\n",
    "        self.train_accuracy = Accuracy(task=\"multiclass\", num_classes=self.hparams.num_classes)\n",
    "        self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=self.hparams.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x: The input tensor containing a batch of images.\n",
    "\n",
    "        Returns:\n",
    "            The output tensor (logits) from the model.\n",
    "        \"\"\"\n",
    "        # Pass the input through the sequential model.\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx=None):\n",
    "        \"\"\"\n",
    "        Performs a single training step.\n",
    "    \n",
    "        Args:\n",
    "            batch (Any): The data batch from the dataloader.\n",
    "            batch_idx (int, optional): The index of the current batch. The Lightning Trainer\n",
    "                                     requires this argument, but it is not utilized in this\n",
    "                                     implementation. Defaults to None.\n",
    "        \"\"\"\n",
    "        # Unpack the batch into inputs (images) and labels.\n",
    "        inputs, labels = batch\n",
    "        # Perform a forward pass to get the model's predictions (logits).\n",
    "        outputs = self(inputs)\n",
    "        # Calculate the loss.\n",
    "        loss = self.loss_fn(outputs, labels)\n",
    "\n",
    "        # Log the training loss at the end of each epoch.\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True)\n",
    "        # Update the training accuracy metric with the current batch's results.\n",
    "        self.train_accuracy(outputs, labels)\n",
    "        # Log the training accuracy at the end of each epoch.\n",
    "        self.log(\"train_accuracy\", self.train_accuracy, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        # Return the loss to Lightning for backpropagation.\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx=None):\n",
    "        \"\"\"\n",
    "        Performs a single validation step.\n",
    "    \n",
    "        Args:\n",
    "            batch (Any): The data batch from the dataloader.\n",
    "            batch_idx (int, optional): The index of the current batch. The Lightning Trainer\n",
    "                                     requires this argument, but it is not utilized in this\n",
    "                                     implementation. Defaults to None.\n",
    "        \"\"\"\n",
    "        # Unpack the batch into inputs (images) and labels.\n",
    "        inputs, labels = batch\n",
    "        # Perform a forward pass to get the model's predictions (logits).\n",
    "        outputs = self(inputs)\n",
    "        # Calculate the loss.\n",
    "        loss = self.loss_fn(outputs, labels)\n",
    "\n",
    "        # Log the validation loss at the end of each epoch.\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        # Update the validation accuracy metric with the current batch's results.\n",
    "        self.val_accuracy(outputs, labels)\n",
    "        # Log the validation accuracy at the end of each epoch.\n",
    "        self.log(\"val_accuracy\", self.val_accuracy, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configures and returns the model's optimizer.\n",
    "\n",
    "        Returns:\n",
    "            An instance of the optimizer.\n",
    "        \"\"\"\n",
    "        # Create and return the AdamW optimizer.\n",
    "        return optim.AdamW(self.parameters(), lr=self.hparams.learning_rate, weight_decay=self.hparams.weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e4d684-e1a2-43f8-9fb1-372428512913",
   "metadata": {},
   "source": [
    "Now you can instantiate the classes you've defined. This process is as straightforward as creating a standard PyTorch model or dataloader. The key difference is that you are creating Lightning objects that organize the familiar PyTorch logic.\n",
    "\n",
    "* `dm_loader`: This is the `CIFAR10DataModule` that will feed data to the model. You'll configure it to use `num_workers=2`.\n",
    "* `model_baseline`: This is the `CIFAR10LightningModule` that you will train and analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b99edf-44dc-4bba-92ce-1762701d08a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the DataModule (2 workers).\n",
    "dm_loader = CIFAR10DataModule(num_workers=2)\n",
    "\n",
    "# Create an instance of the LightningModule.\n",
    "model_baseline = CIFAR10LightningModule()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811d3f50-57bd-49cc-80ac-a904203de903",
   "metadata": {},
   "source": [
    "## A Quick Training Run\n",
    "\n",
    "Now that you have defined and instantiated your `LightningDataModule` and `LightningModule`, you can see them in action.\n",
    "\n",
    "The code below uses a helper function that leverages the Lightning `Trainer` to run a complete training loop for five epochs. Don't worry about the specifics of how the training function works; that will be covered in future material. For now, the goal is to see your Lightning components working together to train the model.\n",
    "\n",
    "As you run the cell, **pay attention to how long the training takes and the results**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7d8fc6-b7fd-4123-907a-42be0db113c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "baseline_results = helper_utils.run_full_training(model_baseline, dm_loader)\n",
    "\n",
    "print(\"\\nTraining Complete!\\n\")\n",
    "print(\"Final Training Metrics:\")\n",
    "\n",
    "print(f\"\\tTraining Accuracy:    {baseline_results['train_accuracy']}%\")\n",
    "print(f\"\\tValidation Accuracy:  {baseline_results['val_accuracy']}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181326f3",
   "metadata": {},
   "source": [
    "## Exploring Model Complexity\n",
    "\n",
    "Great work! You have successfully run a full training loop using the organized structure of Lightning. Now, reflect on that process. Did you notice the total training time? For a classic and relatively small dataset like **CIFAR-10**, it might have felt slower than you would hope. This experience leads you to an essential question every machine learning practitioner must ask: Why did it take that long, and can you achieve similar results more efficiently?\n",
    "\n",
    "To find the answer, you will need to investigate the potential causes. A common culprit for slow training is the model itself. This naturally leads you to question your specific architecture and ask:\n",
    "\n",
    "> Is This Model Too Complex for the Dataset?\n",
    "\n",
    "To investigate this, take a closer look at your baseline model's architecture. By default, it's configured with:\n",
    "\n",
    "* Convolutional channels: `(256, 512, 1024)`\n",
    "* Linear features: `2048`\n",
    "\n",
    "This is a deep and wide network. However, the **CIFAR-10** dataset consists of small 32x32 pixel images. An architecture this powerful might be overkill for this task.\n",
    "\n",
    "When a model is unnecessarily complex for a given dataset, it can create a **\"model complexity\" bottleneck**. This means the GPU spends a disproportionate amount of time on the model's own calculations, slowing down training without providing a significant accuracy benefit.\n",
    "\n",
    "So, **how can you verify this suspicion before running a full, time-consuming training session?** You will need a tool to look inside your code's execution and see where the time is being spent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f09f89-992d-482c-98dd-6b15fc6e2b73",
   "metadata": {},
   "source": [
    "## Profiling: Understanding Your Code's Performance\n",
    "\n",
    "The previous section concluded that you need a tool to look inside your code's execution. That tool is a **Profiler**.\n",
    "\n",
    "Profiling is the process of analyzing your code to get a detailed breakdown of where it spends the most time and resources, like CPU cycles, GPU time, and memory. This analysis is the key to confirming your hypothesis about the model's complexity and finding any performance **bottlenecks**, the specific parts of your code that are disproportionately slow.\n",
    "\n",
    "With profiling, you can answer important questions about your code's efficiency:\n",
    "\n",
    "* Is my model spending too much time on a specific operation?\n",
    "* Am I using my GPU effectively, or is it sitting idle waiting for data?\n",
    "* Are there any memory issues that could impact training?\n",
    "* Where should I focus my optimization efforts for the biggest impact?\n",
    "\n",
    "You typically profile your code *after* you have a working model but *before* you commit to long, expensive training runs. It helps you find and fix performance issues early, ensuring your training is as efficient as possible.\n",
    "\n",
    "**The Lightning Advantage**\n",
    "\n",
    "In standard PyTorch, you would need to manually import the profiler and manage its state within the training loop. Lightning simplifies this into a single, clean step. You just need to configure the profiler, and Lightning automatically integrates it into its execution routine to capture performance data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b005776-72bc-4c2e-af4b-269b1005b4ba",
   "metadata": {},
   "source": [
    "* Your first step is to specify a location where the generated profiling reports will be saved. These reports will contain all the performance data for your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0d1227-e05e-4ab0-997f-98e259ca6f66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_dir = \"./profiler_output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5369d1e0-8002-4214-a17a-66bb417e84bc",
   "metadata": {},
   "source": [
    "### Step 1: Configure the `PyTorchProfiler`\n",
    "\n",
    "Next, you will create an instance of the <code>[PyTorchProfiler](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.profilers.PyTorchProfiler.html)</code>. This object allows you to control exactly how the profiling is done.\n",
    "\n",
    "* `dirpath` & `filename`: These arguments tell the profiler where to save its output file.\n",
    "\n",
    "* `schedule`: This is a vital parameter that controls the profiler's activity to ensure you only measure stable training steps. It defines a sequence:\n",
    "    * `wait`: Ignores the first few batches.\n",
    "        * This step is needed to skip initial, one-time setup operations (like memory allocation) that aren't part of a normal training step. Unlike the other phases, the profiler is completely **idle** during this time.\n",
    "    * `warmup`: Runs a few more batches to allow hardware to stabilize.\n",
    "        * This step is needed because hardware like GPUs requires a few batches to reach a stable, peak-performance state. Unlike the `active` phase, the profiler runs but **discards** the performance data collected here.\n",
    "    * `active`: Begins recording performance data for the specified number of batches.\n",
    "        * This is the main measurement phase. It's different from the other two because this is the only phase where the profiler **records and saves** performance data for you to analyze.\n",
    "    * `repeat`: Specifies how many times this cycle should be executed.\n",
    ">    \n",
    "* `profile_memory`: Setting this to `True` tracks memory allocation, which is excellent for identifying operations with high memory consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f638bbbd-e2eb-40a9-b101-392ef76e90f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure the PyTorch Profiler\n",
    "profiler = PyTorchProfiler(\n",
    "    # Set the directory to save the profiler report\n",
    "    dirpath=log_dir,\n",
    "    # Specify the filename for the report\n",
    "    filename=\"profile_report\",\n",
    "    # Define the profiling schedule (wait -> warmup -> active)\n",
    "    # Total 14 steps\n",
    "    schedule=schedule(wait=2, warmup=2, active=10, repeat=1),\n",
    "    # Enable memory usage profiling\n",
    "    profile_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecfbc3c-817b-470f-a213-095e711f2c8e",
   "metadata": {},
   "source": [
    "### Step 2: Initialize the `Trainer`\n",
    "\n",
    "This is where all the pieces come together. You will now initialize the <code>[Trainer](https://lightning.ai/docs/pytorch/stable/common/trainer.html)</code>, the central engine in Lightning. It automates the entire training loop, which you will leverage here to perform a short diagnostic run. To switch from a normal training run to a profiling run, you simply pass your configured `profiler` object to the `Trainer`.\n",
    "\n",
    "* `profiler=profiler`: This is the key step where you attach the profiler to the training process.\n",
    "* `max_steps=14`: For this diagnostic run, you will limit training to just 14 steps, enough to cover the profiler's schedule (`wait=2` + `warmup=2` + `active=10`).\n",
    "* `accelerator=\"auto\"`: This parameter tells Lightning to automatically detect and use the available hardware.\n",
    "* `logger=False` & `enable_model_summary=False`: You will disable the default logger and model summary output to keep the console clean and focused on the profiler's results.\n",
    "* `enable_checkpointing=False`: This disables the automatic saving of model checkpoints, which is not needed for a short diagnostic run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626ded40-3e71-458b-acf5-7fd52fd1acdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = pl.Trainer(\n",
    "    # Attach the configured profiler\n",
    "    profiler=profiler,\n",
    "    # Limit training to 14 steps to match the profiler's schedule\n",
    "    max_steps=14,\n",
    "    # Automatically select the hardware accelerator (e.g., GPU, CPU)\n",
    "    accelerator=\"auto\",\n",
    "    # Use a single device for training\n",
    "    devices=1,\n",
    "    # Disable the default logger for a cleaner output\n",
    "    logger=False,\n",
    "    # Disable the model summary for the same reason\n",
    "    enable_model_summary=False,\n",
    "    # Disable automatic checkpointing\n",
    "    enable_checkpointing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa424797-02a8-4450-90b5-4e9f272fdf87",
   "metadata": {},
   "source": [
    "### Step 3: Run the Diagnostic and Profiling\n",
    "\n",
    "With the `profiler` and `Trainer` configured and your model and data objects ready, you can now start the diagnostic run.\n",
    "\n",
    "* You'll do this by calling `.fit()` on your `Trainer` instance. This single command handles the entire training loop. Because you attached the `profiler` to the `Trainer`, this command automatically executes the short, diagnostic profiling run instead of a full training session.\n",
    "    * `model_baseline` and `dm_loader`: These are the `LightningModule` and `LightningDataModule` objects you instantiated in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6f24bc-a2ba-49fb-9511-130277ae0db5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start the training and profiling run.\n",
    "trainer.fit(model_baseline, dm_loader)\n",
    "\n",
    "# Print a confirmation message when done.\n",
    "print(\"\\nProfiling Complete!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39ac812",
   "metadata": {},
   "source": [
    "### Step 4: Analyzing the Profiler Results: From Raw Data to Insights\n",
    "\n",
    "The `trainer.fit()` command you just ran generated a raw JSON trace file. This file contains a highly detailed, event-by-event log of every operation. \n",
    "The raw data is very fine-grained not necessarily focused on the type of operation that you want to put the emphasis on.\n",
    "\n",
    "Before you look at the summarized table, it's helpful to understand the **major categories of work** the profiler is tracking. \n",
    "During any training step, the time is generally spent on a few key things:\n",
    "\n",
    "* **Data Loading:** Moving a batch of data from your dataloader to the active device (e.g., the GPU).\n",
    "\n",
    "* **Model Computation:** The core mathematical work of your network. This includes the **forward pass** (running data through layers) and the **backward pass** (calculating gradients).\n",
    "\n",
    "* **Optimizer Step:** Applying the calculated gradients to update your model's weights.\n",
    "\n",
    "* **Framework Overhead:** The internal operations run by PyTorch and Lightning to coordinate the entire process.\n",
    "\n",
    "While a deep dive is beyond the scope of this lab, the `display_profiler_logs` function in the next cell parses all the granular events and helps you see which of these categories is taking the most time. It presents a simple, sorted summary of the most expensive operations.\n",
    "\n",
    "* Run the next cell to display the top 10 most time-consuming operations from your diagnostic run. The table is sorted by total time in descending order.\n",
    "    * Feel free to change the `head` value if you wish to see more or less rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5673bc-d8e4-4b61-b81c-f42743ab7fd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display the top 10 most time-consuming operations from the profiler's report\n",
    "helper_utils.display_profiler_logs(profiler, head=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f57089",
   "metadata": {},
   "source": [
    "#### Overview of the Profiler Table\n",
    "\n",
    "A high-level overview of the profiler table reveals the major categories of operations being tracked:\n",
    "\n",
    "- **ATen operations (`aten::...`)** : PyTorchâ€™s low-level tensor functions from the ATen C++ backend:\n",
    "  - `aten::copy_`\n",
    "  - `aten::to`\n",
    "  - `aten::_to_copy`\n",
    ">  \n",
    "- **Lightning utility** : Functions from Lightning that handle device placement:\n",
    "  - `transfer_batch_to_device`\n",
    ">\n",
    "- **CUDA runtime** : GPU driver/library calls for data transfer:\n",
    "  - `cudaMemcpyAsync`\n",
    ">\n",
    "- **Training logic** : High-level operations in the training loop:\n",
    "  - `ProfilerStep*` (the wrapper for the whole step)\n",
    "  - `Optimizer.step#AdamW.step` (the optimizer update)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ce999c",
   "metadata": {},
   "source": [
    "#### Focusing your Investigation on Model Computations\n",
    "\n",
    "As you can see, the report is very detailed, mixing high-level model computations with lower-level data transfer and framework overhead. To test the hypothesis about model complexity, you need to cut through this noise and narrow your focus on the model's computations.\n",
    "\n",
    "* Run the next cell to display a filtered table showing the total `ProfilerStep*` time (the full duration of one training iteration, including forward, backward, and optimization) alongside the four most time-consuming operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c453f4-5d50-4d0a-b2f6-20c17c0f4f8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display a focused summary of the profiler report for the baseline run.\n",
    "# This filters for the overall time and the top 4 computational operations.\n",
    "helper_utils.display_model_computation_logs(profiler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3e5c9e-536e-4a76-b730-bdcb1c9bfef0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Interpreting the Baseline Results: The Bottleneck**\n",
    "\n",
    "Looking at the filtered table you just generated, you can see exactly where the model spends most of its computational time. \n",
    "The `ProfilerStep*` entry represents the overall time for a single training step. \n",
    "Below it, you can see the most expensive mathematical operations: `aten::conv2d` (convolution) and the backward-pass operations for the model's layers.\n",
    "\n",
    "These operations are the fundamental building blocks of your network. Their high cost in this run leads to an important question: **what potential improvements could you make?**\n",
    "\n",
    "An architecture this large is likely more powerful than necessary for this dataset. The profiler shows that the heavy computational work of the model itself is the dominant factor slowing things down. The more complex these calculations are, the longer the GPU is occupied with each batch. \n",
    "\n",
    "Next, you will profile a second, more streamlined model to measure how simplifying the architecture impacts performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94bf572-7611-4587-91a0-4965dbd3e3b2",
   "metadata": {},
   "source": [
    "## Profiling a More Efficient Model\n",
    "\n",
    "The analysis in the last section suggested a **model complexity** bottleneck. Your baseline model's architecture is likely too powerful for the simple CIFAR-10 dataset, causing it to be unnecessarily slow.\n",
    "\n",
    "To test this hypothesis, you will now profile a second, more streamlined version of the model to measure how simplifying the architecture impacts performance.\n",
    "\n",
    "### Step 1: Configure a New Profiler\n",
    "\n",
    "**Your Task**\n",
    "\n",
    "* Your first step is to configure a new `PyTorchProfiler` for this second diagnostic run. The setup is identical to the baseline run, but you must provide a new `filename`. This is an important step to ensure you don't overwrite the results from your first analysis.\n",
    "    * `dirpath`: Set this to the `log_dir` variable.\n",
    "    * `filename`: Give it a new name, for example, `\"profile_report_efficient\"`.\n",
    "    * `schedule:`: Use the same schedule as the baseline profiler (`wait=2`, `warmup=2`, `active=10`, `repeat=1`).\n",
    "    * `profile_memory`: Enable this by setting it to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24d8493-efd2-4f61-847d-e2ff4b539a96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Configure the PyTorch Profiler for the efficient model run\n",
    "    profiler_efficient = PyTorchProfiler(\n",
    "        \n",
    "        # Set the directory to save the profiler report\n",
    "        dirpath= log_dir,\n",
    "        \n",
    "        # Specify a new filename for the report\n",
    "        filename=\"profile_report_efficient\",\n",
    "        \n",
    "        # Define the profiling schedule (wait -> warmup -> active)\n",
    "        schedule= schedule(wait=2, warmup=2, active=10, repeat=1),\n",
    "        \n",
    "        # Enable memory usage profiling\n",
    "        profile_memory=True\n",
    "    )\n",
    "\n",
    "    print(\"\\033[92mPyTorchProfiler configured successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\033[91mSomething went wrong, try again!\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f864586-d838-44e3-9f87-3ca0bf4d7a4b",
   "metadata": {},
   "source": [
    "<br>\n",
    "<details>\n",
    "<summary><span style=\"color:green;\"><strong>Solution (Click here to expand)</strong></span></summary>\n",
    "\n",
    "```python\n",
    "# Configure the PyTorch Profiler for the efficient model run\n",
    "profiler_efficient = PyTorchProfiler( ### Add your code here\n",
    "    \n",
    "    # Set the directory to save the profiler report\n",
    "    dirpath=log_dir, ### Add your code here\n",
    "    \n",
    "    # Specify a new filename for the report\n",
    "    filename=\"profile_report_efficient\", ### Add your code here\n",
    "    \n",
    "    # Define the profiling schedule (wait -> warmup -> active)\n",
    "    schedule=schedule(wait=2, warmup=2, active=10, repeat=1), ### Add your code here\n",
    "    \n",
    "    # Enable memory usage profiling\n",
    "    profile_memory=True ### Add your code here\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1a9e59-3aca-41f3-94c2-2970de03aeb7",
   "metadata": {},
   "source": [
    "### Step 2: Configure a New Trainer\n",
    "\n",
    "**Your Task**\n",
    "\n",
    "* Next, you'll create a new `Trainer` instance for this run. To ensure a fair comparison with your baseline, you will use the exact same configuration as before. The only change is passing in your new `profiler_efficient` object.\n",
    "    * `profiler`: Attach the `profiler_efficient` object you just created.\n",
    "    * `max_steps`: Limit training to `14` steps to match the profiler's schedule.\n",
    "    * `accelerator`: Set to `\"auto\"` to automatically select the hardware.\n",
    "    * `devices`: Use a single device (1).\n",
    "    * `logger`: Disable the logger by setting it to `False`.\n",
    "    * `enable_model_summary`: Disable the model summary (`False`).\n",
    "    * `enable_checkpointing`: Disable automatic checkpointing (`False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b405c9ff-5c7f-43dd-8c00-361d712f06ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Initialize the Trainer\n",
    "    trainer_efficient = pl.Trainer(\n",
    "        \n",
    "        # Attach the configured profiler\n",
    "        profiler=profiler_efficient,\n",
    "        \n",
    "        # Limit training to 14 steps to match the profiler's schedule\n",
    "        max_steps=14,\n",
    "        \n",
    "        # Automatically select the hardware accelerator (e.g., GPU, CPU)\n",
    "        accelerator=\"auto\",\n",
    "        \n",
    "        # Use a single device for training\n",
    "        devices=1,\n",
    "        \n",
    "        # Disable the default logger for a cleaner output\n",
    "        logger= False,\n",
    "        \n",
    "        # Disable the model summary for the same reason\n",
    "        enable_model_summary= False,\n",
    "\n",
    "        # Disable automatic checkpointing\n",
    "        enable_checkpointing= False\n",
    "    )\n",
    "\n",
    "    print(\"\\033[92mTrainer configured successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\033[91mSomething went wrong, try again!\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30274555-2335-43a1-9bf7-5a2e029efcb3",
   "metadata": {},
   "source": [
    "<br>\n",
    "<details>\n",
    "<summary><span style=\"color:green;\"><strong>Solution (Click here to expand)</strong></span></summary>\n",
    "\n",
    "```python\n",
    "# Initialize the Trainer\n",
    "trainer_efficient = pl.Trainer( ### Add your code here\n",
    "    \n",
    "    # Attach the configured profiler\n",
    "    profiler=profiler_efficient, ### Add your code here\n",
    "    \n",
    "    # Limit training to 14 steps to match the profiler's schedule\n",
    "    max_steps=14, ### Add your code here\n",
    "    \n",
    "    # Automatically select the hardware accelerator (e.g., GPU, CPU)\n",
    "    accelerator=\"auto\", ### Add your code here\n",
    "    \n",
    "    # Use a single device for training\n",
    "    devices=1, ### Add your code here\n",
    "    \n",
    "    # Disable the default logger for a cleaner output\n",
    "    logger=False, ### Add your code here\n",
    "    \n",
    "    # Disable the model summary for the same reason\n",
    "    enable_model_summary=False, ### Add your code here\n",
    "\n",
    "    # Disable automatic checkpointing\n",
    "    enable_checkpointing=False ### Add your code here\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5b4ccc-7929-449a-8a85-52b822b5d949",
   "metadata": {},
   "source": [
    "### Step 3: Profile the Efficient Model\n",
    "\n",
    "Now, you'll instantiate your `LightningModule` again, but this time with a much simpler architecture (`conv_channels=(32, 64, 128`) and `linear_features=512`), to create the \"efficient\" version of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44ae823-079d-41b3-9c65-ae88db89a04a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new instance of the model with a much simpler architecture.\n",
    "model_efficient = CIFAR10LightningModule(\n",
    "    conv_channels=(32, 64, 128),\n",
    "    linear_features=512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721a0446-b43c-45a2-83d1-91376d2f16c2",
   "metadata": {},
   "source": [
    "**Your Task**\n",
    "\n",
    "Now it's time to start the second diagnostic run. You'll do this by calling the `.fit()` method on your new `trainer_efficient` instance.\n",
    "\n",
    "You have to use the same data module (`dm_loader`) as before. This is an important part of the experiment, as it ensures the only variable you've changed is the model's architecture.\n",
    "\n",
    "* Call `.fit()` on the `trainer_efficient` object.\n",
    "* Pass the `model_efficient` and `dm_loader` as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caba291b-3f47-4f94-ae62-523b5151ea6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Start the second diagnostic run with the new, streamlined model.\n",
    "    ### Add your code here\n",
    "    trainer_efficient.fit(model_efficient, dm_loader)\n",
    "    \n",
    "    print(\"\\nProfiling Complete!\\n\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"\\033[91mSomething went wrong, try again!\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8917f386-fbfd-41ec-b904-0e20272dbb9c",
   "metadata": {},
   "source": [
    "<br>\n",
    "<details>\n",
    "<summary><span style=\"color:green;\"><strong>Solution (Click here to expand)</strong></span></summary>\n",
    "\n",
    "```python\n",
    "# Start the second diagnostic run with the new, streamlined model.\n",
    "trainer_efficient.fit(model_efficient, dm_loader) ### Add your code here\n",
    "\n",
    "print(\"\\nProfiling Complete!\\n\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f93dfe0-a5ed-4d2e-be9e-9eeea0316547",
   "metadata": {},
   "source": [
    "### Step 4: Analyzing the Efficient Profiler Results\n",
    "\n",
    "* Run the following cell to display the new profiler report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ec7b40-4552-4be3-b8bf-875da8deaf1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display the top 10 most time-consuming operations from the profiler_efficient's report\n",
    "helper_utils.display_profiler_logs(profiler_efficient, head=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb431c6f-2854-46b3-b22f-9c99ad6295c7",
   "metadata": {},
   "source": [
    "#### Comparing the Results\n",
    "\n",
    "Now for the direct comparison. The cell below will generate a summary table showing the performance of the key computational operations before and after you simplified the model's architecture.\n",
    "\n",
    "* Run the next cell to display the comparison. Notice the difference in the total time for the `ProfilerStep*` and how the execution time for each computational kernel has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6321e47a-5731-418e-a946-cc6c4ab2f388",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate the comparison report.\n",
    "helper_utils.display_comparison_report(profiler, profiler_efficient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7b612b-829f-4dbd-a2d5-e80333f27778",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Analyzing the Improvement: The Impact of Simplicity\n",
    "\n",
    "The comparison table clearly shows a significant performance gain from simplifying the model's architecture. The most important change is the dramatic reduction in the overall `ProfilerStep*` time, meaning the simpler model completes a training step much faster. This speed-up comes directly from reducing the time spent on core computational kernels like `aten::conv2d` and their backward passes.\n",
    "\n",
    "This demonstrates a key lesson in model optimization: **your architecture should match your dataset's complexity**. For a simple dataset like CIFAR-10, a smaller model is more efficient, allowing you to train faster.\n",
    "\n",
    "#### A Note on the Results\n",
    "\n",
    "You might find it interesting that while the overall `ProfilerStep*` time dropped significantly, the time for individual kernels like `aten::conv2d` or `aten::convolution_backward` changed only slightly, and may in some cases have even increased. This is not an error; it highlights how performance bottlenecks work.\n",
    "\n",
    "* **Before (Complex Model)**: The model had very large tensors (e.g., `1024` channels). Handling these large shapes adds a lot of overhead in memory management and framework logic *around* the core calculations. The GPU might even be underutilized if the overhead of preparing these large tensors causes small delays between operations.\n",
    "\n",
    "* **After (Simple Model)**: The core math for a single convolution doesn't fundamentally change, but the tensors it operates on are much smaller (e.g., `128` channels). The massive speed-up comes from the reduced overhead. With simpler data shapes to manage, the framework can feed operations to the GPU more efficiently, leading to a much faster total step time even if the time for one specific kernel doesn't change dramatically.\n",
    "\n",
    "Essentially, you have removed the **model complexity bottleneck**, allowing the entire pipeline to run much more smoothly.\n",
    "\n",
    "#### Beyond Model Complexity\n",
    "\n",
    "It's important to remember that this is just one type of performance issue the profiler can help diagnose. In other scenarios, you might use it to uncover **data loading bottlenecks** (where the GPU is idle waiting for data from the CPU), inefficient memory usage, or other parts of the pipeline that slow down your training. The key is to use the profiler as a versatile tool to form and test hypotheses about any aspect of your code's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256c685b-bfb5-4890-9e86-d18beaf25bf1",
   "metadata": {},
   "source": [
    "## Training the Efficient Model\n",
    "\n",
    "Great work! Your analysis and changes have paid off. The profiler has confirmed that the simplified model is significantly faster per step. However, speed is only half of the story.\n",
    "\n",
    "The final, essential question is: **does this efficiency come at the cost of performance?** A faster model is only useful if it can still achieve good accuracy. To answer this, you will now run a full training loop on `model_efficient` and inspect its final metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da616cdd-3351-4604-9e0f-9fbf7a57b79d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "efficient_results = helper_utils.run_full_training(model_efficient, dm_loader)\n",
    "\n",
    "print(\"\\nTraining Complete!\\n\")\n",
    "print(\"Final Training Metrics:\")\n",
    "\n",
    "print(f\"\\tTraining Accuracy:    {efficient_results['train_accuracy']}%\")\n",
    "print(f\"\\tValidation Accuracy:  {efficient_results['val_accuracy']}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517a0c59-ece2-448b-b60a-644c4198b0b3",
   "metadata": {},
   "source": [
    "* Finally, display a summary table to compare the final metrics of both models side-by-side, making it easy to see the trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44586fad-4593-4afc-a433-fefd73962bcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "helper_utils.display_metrics_comparison(baseline_results, efficient_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018c5f3c-be25-4f32-a9b1-4705a68e9d75",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now, analyze your results in the table above.\n",
    "\n",
    "Compare the validation metrics for your efficient model to the baseline. You'll likely find that the performance is very close. It's common for a well-sized, efficient model to perform comparably and sometimes even slightly better than an overly complex one on the same dataset.\n",
    "\n",
    "The key takeaway is to evaluate the trade-off. In this case, you have likely achieved a significant improvement in training speed without a major compromise in your model's ability to generalize. With more hyperparameter tuning or by training for more epochs, you could likely improve these results even further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85b73a2-d52e-47ab-a822-c5c7d540f49c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations on completing the lab! You've successfully navigated a complete workflow for diagnosing and improving a model's performance using **Lightning**.\n",
    "\n",
    "You've seen firsthand how Lightning's structure, through the `LightningDataModule` and `LightningModule`, removes boilerplate and organizes your project. This clean structure makes it much simpler to integrate powerful tools like the profiler.\n",
    "\n",
    "More importantly, you've learned a practical, data-driven approach to optimization. You started by establishing a baseline, used the **profiler** to form a hypothesis about a performance bottleneck, tested that hypothesis with a second profiling run, and finally, verified that your more efficient model performed equally well.\n",
    "\n",
    "For those interested in exploring the full, granular details, you can download the `.JSON` trace files from the `./profiler_output/` directory and upload them to a trace viewer like the [Perfetto UI](https://ui.perfetto.dev). This provides far more information than the summary table.\n",
    "\n",
    "Mastering these profiling techniques is an essential skill for building efficient models and scaling up to larger models and datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
