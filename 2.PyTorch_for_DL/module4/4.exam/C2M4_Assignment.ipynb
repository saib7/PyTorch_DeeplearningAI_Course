{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "570f9fbe-d8da-4b53-8edf-f7bf3b830695",
   "metadata": {},
   "source": [
    "# Programming Assignment: A Pneumonia Diagnostic Assistant\n",
    "\n",
    "Congratulations on developing a strong skill set in PyTorch and Lightning! You have learned how to construct neural networks and optimize training pipelines. This assignment is your opportunity to channel that expertise into a project with real world significance.\n",
    "\n",
    "The true power of artificial intelligence is demonstrated when it is applied to solve complex, meaningful problems. In this assignment, you will begin your journey into the impactful field of medical AI by building a tool to assist clinicians with pneumonia diagnosis. Your mission is to engineer the core deep learning engine for an AI powered diagnostic assistant. This is a direct application of the professional grade skills you have been developing, challenging you to build a solution that is not only accurate but also robust and well structured.\n",
    "\n",
    "Throughout this programming assignment, you will:\n",
    "\n",
    "* Assemble a `LightningDataModule` to create a clean and reusable data pipeline.\n",
    "* Build a `LightningModule` to encapsulate your model, loss function, and training logic.\n",
    "* Configure a Lightning `Trainer` with callbacks to run an efficient training loop.\n",
    "* Evaluate your model's performance to understand its predictive capabilities.\n",
    "\n",
    "You will develop a model that can analyze chest X-ray images and classify them as **Normal**, **Bacterial Pneumonia**, or **Viral Pneumonia**. This classification is an essential task, as a quick and accurate diagnosis can lead to better patient outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0c8ae6",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='submission'></a>\n",
    "\n",
    "<h4 style=\"color:green; font-weight:bold;\">TIPS FOR SUCCESSFUL GRADING OF YOUR ASSIGNMENT:</h4>\n",
    "\n",
    "* All cells are frozen except for the ones where you need to submit your solutions or when explicitly mentioned you can interact with it.\n",
    "\n",
    "* In each exercise cell, look for comments `### START CODE HERE ###` and `### END CODE HERE ###`. These show you where to write the solution code. **Do not add or change any code that is outside these comments**.\n",
    "\n",
    "* You can add new cells to experiment but these will be omitted by the grader, so don't rely on newly created cells to host your solution code, use the provided places for this.\n",
    "\n",
    "* Avoid using global variables unless you absolutely have to. The grader tests your code in an isolated environment without running all cells from the top. As a result, global variables may be unavailable when scoring your submission. Global variables that are meant to be used will be defined in UPPERCASE.\n",
    "\n",
    "* To submit your notebook for grading, first save it by clicking the ðŸ’¾ icon on the top left of the page and then click on the `Submit assignment` button on the top right of the page.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81718fc-facd-4002-aa3b-15c6af5f041c",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Imports](#0)\n",
    "- [1 - Chest X-Ray Images](#1)\n",
    "    - [1.1 - Dataset Preparation](#1-1)\n",
    "    - [1.2 - Data Structure](#1-2)\n",
    "    - [1.3 - The Visual Challenge: Differentiating Pneumonia Types](#1-3)\n",
    "- [2 - Building the DataModule](#2)\n",
    "    - [2.1 - Utilities for the DataModule](#2-1)\n",
    "    - [2.2 - Assembling the ChestXRayDataModule](#2-2)\n",
    "        - **[Exercise 1 - ChestXRayDataModule](#ex-1)**\n",
    "- [3 - Building the LightningModule](#3)\n",
    "    - [3.1 - Utilities for the LightningModule](#3-1)\n",
    "    - [3.2 - Assembling the ChestXRayClassifier](#3-2)\n",
    "        - **[Exercise 2 - ChestXRayClassifier](#ex-2)**\n",
    "- [4 - Training the Model](#4)\n",
    "    - [4.1 - Configuring Early Stopping](#4-1)\n",
    "        - **[Exercise 3 - early_stopping](#ex-3)**\n",
    "    - [4.2 - Configuring the Trainer](#4-2)\n",
    "        - **[Exercise 4 - run_training](#ex-4)**\n",
    "    - [4.3 - Training Your Diagnostic Assistant](#4-3)\n",
    "- [5 - Evaluating Model Performance](#5)\n",
    "    - [5.1 - Confusion Matrix and Per-Class Accuracy](#5-1)\n",
    "    - [5.2 - Visualizing Predictions](#5-2)\n",
    "    - [5.3 - A True Test](#5-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4afa72-c7ba-4cb0-8f0b-483c347e3bf9",
   "metadata": {},
   "source": [
    "<a name='0'></a>\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc27d5e7-7c12-4362-80ae-f0e48687e208",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Redirect stderr to a black hole to catch other potential messages\n",
    "class BlackHole:\n",
    "    def write(self, message):\n",
    "        pass\n",
    "    def flush(self):\n",
    "        pass\n",
    "sys.stderr = BlackHole()\n",
    "\n",
    "# Ignore Python-level UserWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4d9291-634e-4027-b199-d96a657fead5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.classification import Accuracy\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision import models as tv_models\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c99e14e-eb5e-48bb-9fde-69e79892d703",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import helper_utils\n",
    "import unittests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251dbaf2-37c8-4e99-9251-c71200853f5b",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Chest X-Ray Images\n",
    "\n",
    "An effective medical AI model is built upon a foundation of high-quality, relevant data. For your assignment, you will work with the [Chest X-Ray Images (Pneumonia) dataset](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia), a valuable collection of medical imagery. This dataset consists of chest X-ray images sourced from retrospective studies of pediatric patients aged one to five years at the Guangzhou Women and Childrenâ€™s Medical Center. To ensure diagnostic quality, every image was evaluated and graded by two medical experts before its inclusion.\n",
    "\n",
    "<a name='1-1'></a>\n",
    "### 1.1 - Dataset Preparation\n",
    "\n",
    "To tailor the data for your diagnostic tool, a few important modifications were made. The original dataset was organized into `train`, `test`, and `val` folders. To streamline the evaluation process, the `test` and `val` sets were combined into a single validation set for you to use.\n",
    "\n",
    "More importantly, this assignment presents you with a nuanced, multiclass problem. The original dataset only identified images as `NORMAL` or `PNEUMONIA`. In the version you will use, the `PNEUMONIA` class has been split into two distinct categories: `BACTERIAL_PNEUMONIA` and `VIRAL_PNEUMONIA`. This change provides a more detailed diagnostic task, allowing your model to learn the subtle differences between the two types of infection.\n",
    "\n",
    "Finally, preventing model bias is an essential part of building a fair and reliable medical tool. For that reason, the dataset was **balanced**. Images from the more frequent classes were randomly removed until every class in the training and validation sets contained the exact same number of images.\n",
    "\n",
    "* Begin with defining the path to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fa8dcd-95ae-4d63-befc-4eb6453e34cc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "data_dir = \"./chest_xray/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daabb66c-9e37-4c43-9e04-8fc64d3e2978",
   "metadata": {},
   "source": [
    "<a name='1-2'></a>\n",
    "### 1.2 - Data Structure\n",
    "\n",
    "The data is organized into `train` and `val` directories, with subdirectories for each of the three classes. You will see the following structure:\n",
    "\n",
    "```\n",
    "./chest_xray/\n",
    "â”‚\n",
    "â”œâ”€â”€ train/\n",
    "â”‚   â”œâ”€â”€ NORMAL/\n",
    "â”‚   â”‚   â”œâ”€â”€ IM-0001-0001.jpeg\n",
    "â”‚   â”‚   â””â”€â”€ ...\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€â”€ BACTERIAL_PNEUMONIA/\n",
    "â”‚   â”‚   â”œâ”€â”€ person1_bacteria_1.jpeg\n",
    "â”‚   â”‚   â””â”€â”€ ...\n",
    "â”‚   â”‚\n",
    "â”‚   â””â”€â”€ VIRAL_PNEUMONIA/\n",
    "â”‚       â”œâ”€â”€ person140_virus_284.jpeg\n",
    "â”‚       â””â”€â”€ ...\n",
    "â”‚\n",
    "â””â”€â”€ val/\n",
    "    â”œâ”€â”€ NORMAL/\n",
    "    â”‚   â”œâ”€â”€ IM-0001-0001.jpeg\n",
    "    â”‚   â””â”€â”€ ...\n",
    "    â”‚\n",
    "    â”œâ”€â”€ BACTERIAL_PNEUMONIA/\n",
    "    â”‚   â”œâ”€â”€ person1946_bacteria_4874.jpeg\n",
    "    â”‚   â””â”€â”€ ...\n",
    "    â”‚\n",
    "    â””â”€â”€ VIRAL_PNEUMONIA/\n",
    "        â”œâ”€â”€ person16_virus_44.jpeg\n",
    "        â””â”€â”€ ...\n",
    "```\n",
    "\n",
    "* Run the cell below to confirm the distribution of images across all classes in both the training and validation sets is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c432fcd1-eb17-4c14-b128-54b14dd1cdd6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "helper_utils.display_dataset_count(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6a07ff-03fc-42f4-b670-493a842e2ccd",
   "metadata": {},
   "source": [
    "<a name='1-3'></a>\n",
    "### 1.3 - The Visual Challenge: Differentiating Pneumonia Types\n",
    "\n",
    "To appreciate the challenge your model is undertaking, it's helpful to recognize the general visual patterns associated with each class. Think of a chest X-ray as a picture where dense materials like bone appear white, and air-filled spaces like healthy lungs look dark. An infection can cause parts of the lungs to fill with fluid, making them look hazy or white.\n",
    "\n",
    "Keep in mind that these are simplified observations for this assignment; a definitive diagnosis requires the skill of a trained medical expert.\n",
    "\n",
    "* A **Normal** chest X-ray generally shows lungs that are clear and dark, indicating they are properly filled with air. You can typically see the sharp outlines of the heart, diaphragm, and rib cage.\n",
    "\n",
    "* **Bacterial Pneumonia** often presents as a dense, white, or cloudy patch that is more localized to a specific area of the lung, sometimes called a \"consolidation\".\n",
    "\n",
    "* In comparison, **Viral Pneumonia** tends to look more widespread and scattered. The pattern is often a more diffuse, hazy, or streaky whiteness that can be seen throughout both lungs.\n",
    "\n",
    "Run the cell below to visualize some random examples from the training dataset and see these patterns for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe277ea5-46c7-45c9-8ab6-75d59b02e7e0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Display a grid of random images from the training set to visualize the data.\n",
    "helper_utils.display_random_images(data_dir + \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186f793e-4b88-4c20-b311-e3d60a57d421",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Building the `DataModule`\n",
    "\n",
    "You have already seen how Lightning simplifies the data handling process. You will now use its `DataModule` to manage everything related to the dataset for this assignment. Your first exercise ahead is to piece together the `ChestXRayDataModule`.\n",
    "\n",
    "Before you build the module, you will first define the essential components, or \"utilities,\" that your `DataModule` will use.\n",
    "\n",
    "<a name='2-1'></a>\n",
    "### 2.1 - Utilities for the `DataModule`\n",
    "\n",
    "The code cells below contain the pre-written transformation pipelines and helper functions needed for your `DataModule`. Review each one carefully to understand its role.\n",
    "\n",
    "* `TRAIN_TRANSFORM` and `VAL_TRANSFORM`: These are the `torchvision` transform pipelines. The training pipeline includes data augmentation to help your model generalize, while the validation pipeline performs only the necessary preprocessing for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90c36c0-e8a0-41a2-a7c6-68cc193f936e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# Define the series of transformations to be applied to the training images.\n",
    "TRAIN_TRANSFORM = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    # Perform a random affine transformation: shifting and scaling the image.\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize the tensor image with a precalculated mean and standard deviation\n",
    "    # of this dataset.\n",
    "    transforms.Normalize([0.482, 0.482, 0.482], [0.222, 0.222, 0.222])\n",
    "])\n",
    "\n",
    "# Define the transformations for the validation images.\n",
    "VAL_TRANSFORM = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize the tensor image with a precalculated mean and standard deviation\n",
    "    # of this dataset.\n",
    "    transforms.Normalize([0.482, 0.482, 0.482], [0.222, 0.222, 0.222])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f26b533-7bc6-4d52-a8c5-c618d9a28964",
   "metadata": {},
   "source": [
    "* `create_datasets(â€¦)`: This function takes file paths and transforms as input and returns the initialized training and validation `ImageFolder` datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096b8482-75a6-43e9-b5bd-364412270883",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def create_datasets(train_path, val_path, train_transform, val_transform):\n",
    "    \"\"\"\n",
    "    Creates and returns the necessary datasets for training and validation.\n",
    "\n",
    "    Args:\n",
    "        train_path (str): The file path to the training images.\n",
    "        val_path (str): The file path to the validation images.\n",
    "        train_transform (callable): Transformations to apply to the training data.\n",
    "        val_transform (callable): Transformations to apply to the validation data.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the train_dataset and val_dataset.\n",
    "    \"\"\"\n",
    "    # Create the training and validation datasets from the image folders.\n",
    "    train_dataset = datasets.ImageFolder(train_path, train_transform)\n",
    "    val_dataset = datasets.ImageFolder(val_path, val_transform)\n",
    "\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd43330-91f9-4f28-a077-6e9f2e4f4aa1",
   "metadata": {},
   "source": [
    "* `load_dataloader(â€¦)`: This function takes the datasets and other parameters to create and return *either* a training *or* validation `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c78a18-2989-4a03-9392-faa2e43ce0a7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def load_dataloader(train_dataset, val_dataset, batch_size, is_train_loader):\n",
    "    \"\"\"\n",
    "    Creates and returns either a training or validation DataLoader.\n",
    "\n",
    "   Args:\n",
    "        train_dataset (Dataset): The dataset for the training loader.\n",
    "        val_dataset (Dataset): The dataset for the validation loader.\n",
    "        batch_size (int): The number of samples per batch to load.\n",
    "        is_train_loader (bool): A flag to determine which loader to create.\n",
    "                                If True, creates the training loader.\n",
    "                                If False, creates the validation loader.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: The configured PyTorch DataLoader.\n",
    "    \"\"\"\n",
    "    # Check the flag to determine whether to create a training or validation loader.\n",
    "    if is_train_loader==True:\n",
    "        # Create the training DataLoader.\n",
    "        loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        # Create the validation DataLoader.\n",
    "        loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f5609a-3bee-4c96-bb6a-278346a57908",
   "metadata": {},
   "source": [
    "<a name='2-2'></a>\n",
    "### 2.2 - Assembling the `ChestXRayDataModule`\n",
    "\n",
    "You have now defined all the necessary utilities for your data pipeline. You are at the final assembly stage, where you will bring these components together inside a `LightningDataModule`.\n",
    "\n",
    "The `DataModule` acts as the organizational backbone for your entire data pipeline. It encapsulates all data related logic, from loading and splitting to transforming and batching, into a single, reusable class. This practice keeps your main model code clean and ensures your data handling is reproducible, which is essential for any medical AI application.\n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### Exercise 1 - ChestXRayDataModule\n",
    "\n",
    "Your exercise is to complete the `ChestXRayDataModule` using the utilities you have just reviewed.\n",
    "\n",
    "**Your Task**:\n",
    "\n",
    "* **Inside the `__init__` method**: \n",
    ">    * This is the setup method for your `DataModule`. You need to store the `data_dir` and `batch_size` that are passed in.\n",
    ">    * You also need to assign the pre-defined transform pipelines, `TRAIN_TRANSFORM` and `VAL_TRANSFORM`, to instance attributes so they can be accessed by other methods in the class.\n",
    "\n",
    "* **Inside the `setup` method**: \n",
    ">    * This method is responsible for creating the datasets. You'll use the `create_datasets` helper function to initialize `self.train_dataset` and `self.val_dataset`.\n",
    ">    * Make sure you pass the correct arguments to the function, including the paths and the transforms you stored earlier.\n",
    "\n",
    "* **Inside the `train_dataloader` and `val_dataloader` methods**: \n",
    ">    * These methods create the data loaders that will feed batches of data to your model.\n",
    ">    * You'll need to call the `load_dataloader` helper function in each method. Pay close attention to the `is_train_loader` flag to ensure you create the correct type of loader (one for training, one for validation).\n",
    "\n",
    "<details>\n",
    "<summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
    "\n",
    "If you're stuck, here is a more detailed breakdown.\n",
    "\n",
    "**For the `__init__` method**:\n",
    "\n",
    "* You're simply saving the arguments passed to the constructor.\n",
    "* For example, the first line will be `self.data_dir = data_dir`. Follow this exact pattern for `batch_size`, `train_transform`, and `val_transform`.\n",
    "\n",
    "**For the `setup` method**:\n",
    "\n",
    "* You need to call the `create_datasets` function and assign its two outputs to `self.train_dataset` and `self.val_dataset`.\n",
    "* The function call should look like this:\n",
    "    > `self.train_dataset, self.val_dataset = call create_datasets(using the train_path, val_path, self.train_transform, and self.val_transform as arguments)`\n",
    "\n",
    "**For the `train_dataloader` method**:\n",
    "\n",
    "* Call the `load_dataloader` function. The key is to tell it you want a *training* loader.\n",
    "* The call will look like this:\n",
    "    > `train_loader = load_dataloader(with self.train_dataset, self.val_dataset, self.batch_size, and the is_train_loader flag set to True)`\n",
    "\n",
    "**For the `val_dataloader` method**:\n",
    "\n",
    "* This is very similar to the training loader, but you must indicate that you want a *validation* loader.\n",
    "* The call will look like this:\n",
    "    > `val_loader = load_dataloader(with self.train_dataset, self.val_dataset, self.batch_size, and the is_train_loader flag set to False)`\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7a5a7f-0481-4ed5-95af-477a8926d292",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CLASS: ChestXRayDataModule\n",
    "\n",
    "class ChestXRayDataModule(pl.LightningDataModule):\n",
    "    \"\"\"\n",
    "    A LightningDataModule encapsulates all the steps involved in preparing data\n",
    "    for a PyTorch model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, batch_size=64):\n",
    "        \"\"\"\n",
    "        Initializes the DataModule and stores key parameters.\n",
    "\n",
    "        Args:\n",
    "            data_dir (str): Directory where the data is stored.\n",
    "            batch_size (int): Number of samples per batch in the DataLoader.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        # Store the constructor arguments as instance attributes.\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Assign the globally defined transformations to this instance.\n",
    "        self.train_transform = TRAIN_TRANSFORM\n",
    "        self.val_transform = VAL_TRANSFORM\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # DO NOT REMOVE `NONE` OF THESE PLACEHOLDERS\n",
    "        # Placeholders for datasets, to be assigned in setup().\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"\n",
    "        Assigns the train and validation datasets.\n",
    "\n",
    "        Args:\n",
    "        stage (str, optional): The stage of training (e.g., 'fit', 'test').\n",
    "                               The Lightning Trainer requires this argument, but it is not\n",
    "                               utilized in this implementation as the setup logic is the\n",
    "                               same for all stages. Defaults to None.\n",
    "        \"\"\"\n",
    "        # Construct the full paths to the train and validation image folders.\n",
    "        train_path = os.path.join(self.data_dir, \"train\")\n",
    "        val_path = os.path.join(self.data_dir, \"val\")\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        # Call the `create_datasets` helper function with the correct of the\n",
    "        # arguments to initialize self.train_dataset and self.val_dataset.\n",
    "        self.train_dataset, self.val_dataset = create_datasets(\n",
    "            train_path,\n",
    "            val_path,\n",
    "            self.train_transform,\n",
    "            self.val_transform\n",
    "        )\n",
    "            \n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"Returns the DataLoader for the training set.\"\"\"\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        # Call the `load_dataloader` helper function to create the\n",
    "        # training loader.\n",
    "        train_loader = load_dataloader(\n",
    "            self.train_dataset,\n",
    "            self.val_dataset,\n",
    "            self.batch_size,\n",
    "            True\n",
    "        )\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \"\"\"Returns the DataLoader for the validation set.\"\"\"\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        # Call the `load_dataloader` helper function to create the\n",
    "        # validation loader.\n",
    "        val_loader = load_dataloader(\n",
    "            self.train_dataset,\n",
    "            self.val_dataset,\n",
    "            self.batch_size,\n",
    "            False\n",
    "        ) \n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        return val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc24418-5551-46c7-939c-8ca483fe98fe",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Instantiate the DataModule for verification.\n",
    "dm_verify = ChestXRayDataModule(data_dir=data_dir, batch_size=8)\n",
    "# Set up the datasets.\n",
    "dm_verify.setup()\n",
    "train_loader_verify = dm_verify.train_dataloader()\n",
    "val_loader_verify = dm_verify.val_dataloader()\n",
    "\n",
    "# --- Verify the Training Set ---\n",
    "print(\"--- Training Set ---\")\n",
    "print(f\"Total samples in the training dataset:    {len(dm_verify.train_dataset)}\")\n",
    "print(f\"DataLoader batch size:                    {train_loader_verify.batch_size}\")\n",
    "print(f\"DataLoader length (number of batches):    {len(train_loader_verify)}\")\n",
    "\n",
    "# --- Verify the Validation Set ---\n",
    "print(\"\\n--- Validation Set ---\")\n",
    "print(f\"Total samples in the validation dataset:  {len(dm_verify.val_dataset)}\")\n",
    "print(f\"DataLoader batch size:                    {val_loader_verify.batch_size}\")\n",
    "print(f\"DataLoader length (number of batches):    {len(val_loader_verify)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38d58fc-7d23-4891-9dbd-a44339ec010c",
   "metadata": {},
   "source": [
    "#### Expected Output:\n",
    "\n",
    "```\n",
    "--- Training Set ---\n",
    "Total samples in the training dataset:    4014\n",
    "DataLoader batch size:                    8\n",
    "DataLoader length (number of batches):    502\n",
    "\n",
    "--- Validation Set ---\n",
    "Total samples in the validation dataset:  444\n",
    "DataLoader batch size:                    8\n",
    "DataLoader length (number of batches):    56\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab9a5ab-c76c-453c-8967-4c7c018e9b7e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "unittests.exercise_1(ChestXRayDataModule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf07e8a-7877-4cb6-9033-42c1de7e464b",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Building the `LightningModule`\n",
    "\n",
    "With the data pipeline complete, your next task is to construct the `ChestXRayClassifier`, which is a `LightningModule`. This class is the core of your AI model; it organizes all the essential components for training, including the model architecture, the loss function, and the optimizers, into a single, self contained system.\n",
    "\n",
    "Just like before, you will first review the pre-written utilities that your `LightningModule` will use to function.\n",
    "\n",
    "<a name='3-1'></a>\n",
    "### 3.1 - Utilities for the `LightningModule`\n",
    "\n",
    "The helper functions below are designed to handle model initialization and optimizer configuration. Review each one to understand its specific role before using them in the main exercise.\n",
    "\n",
    "* `load_resnet18(â€¦)`: This function initializes a ResNet-18 model and prepares it for transfer learning. It replaces the final classification layer with a new one suited for your three-class problem and freezes all other layers. This ensures you are only training the new head on top of the powerful, pre-trained features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafe6627-4253-498a-9393-f227c5bf6adb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def load_resnet18(num_classes, weights_path):\n",
    "    \"\"\"\n",
    "    Initializes a ResNet-18 model, loads weights, and sets it up\n",
    "    for transfer learning (feature extraction).\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): The number of output classes for the new classifier head.\n",
    "        weights_path (str): The file path to the saved .pth model weights.\n",
    "\n",
    "    Returns:\n",
    "        A PyTorch model (ResNet-18) where all layers are frozen except for\n",
    "        the final classifier head.\n",
    "    \"\"\"\n",
    "    # Initialize a ResNet-18 model without pre-trained weights.\n",
    "    model = tv_models.resnet18(weights=None)\n",
    "\n",
    "    # Replace the classifier head to match the number of classes for the new task.\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    # Load the state dictionary (weights) from the local file.\n",
    "    state_dict = torch.load(weights_path, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    # Freeze all the parameters in the model.\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Unfreeze ONLY the parameters of the new classifier head.\n",
    "    for param in model.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d169ac5-3d71-4fa7-84fe-9b7146eec9eb",
   "metadata": {},
   "source": [
    "* `define_optimizer_and_scheduler(â€¦)`: This function returns the `AdamW` optimizer and a `ReduceLROnPlateau` learning rate scheduler. The scheduler will automatically reduce the learning rate if the validation loss stops improving, which is a common technique for effective training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc762c94-a196-4ce2-9470-3aab93ae906b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def define_optimizer_and_scheduler(model, learning_rate, weight_decay):\n",
    "    \"\"\"\n",
    "    Defines the optimizer and learning rate scheduler for the model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model for which to configure the optimizer.\n",
    "                           Its parameters will be passed to the optimizer.\n",
    "        learning_rate (float): The learning rate for the optimizer.\n",
    "        weight_decay (float): The weight decay (L2 penalty) for the optimizer.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the configured optimizer and lr_scheduler.\n",
    "    \"\"\"\n",
    "    # Create the optimizer.\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    # Create the learning rate scheduler.\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=0.1,\n",
    "        patience=2\n",
    "    )\n",
    "    \n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7477eb-e382-4d34-a268-26bd98ed6d6a",
   "metadata": {},
   "source": [
    "<a name='3-2'></a>\n",
    "### 3.2 - Assembling the `ChestXRayClassifier`\n",
    "\n",
    "You've now reached the heart of the assignment: assembling the `ChestXRayClassifier`. This is where you will define the complete system for your diagnostic model.\n",
    "\n",
    "The `LightningModule` is the central command center for your model. It brings together the neural network architecture (your pre-trained ResNet-18), the loss function for measuring error, the metrics for tracking performance, and the optimizers that drive the learning process. By organizing these components into a single class, you create a clean, portable, and highly structured system, a hallmark of professional deep learning engineering.\n",
    "\n",
    "<a name='ex-2'></a>\n",
    "### Exercise 2 - ChestXRayClassifier\n",
    "\n",
    "Your exercise is to complete the `ChestXRayClassifier` using the utilities you have just reviewed.\n",
    "\n",
    "**Your Task**:\n",
    "\n",
    "* **Inside the `__init__` method**: \n",
    ">    * This method sets up the core components of your classifier.\n",
    ">    * You need to initialize the pre-trained model by calling the `load_resnet18` helper function.\n",
    ">    * Define the loss function. For this multi-class classification problem, `nn.CrossEntropyLoss` is the right choice.\n",
    ">    * Define the accuracy metric using `Accuracy`. Remember to configure it for a \"multiclass\" task.\n",
    "\n",
    "* **Inside the `training_step` method**: \n",
    ">    * This method defines the logic for a single forward pass and loss calculation during training.\n",
    ">    * First, unpack the input `batch` into images and their corresponding labels.\n",
    ">    * Next, get the model's raw predictions (`logits`) by performing a forward pass on the images.\n",
    ">    * Finally, compute the training loss by comparing the model's logits with the true labels.\n",
    "\n",
    "* **Inside the `validation_step` method**: \n",
    ">    * This method is similar to `training_step` but is used for the validation loop.\n",
    ">    * You'll unpack the `batch`, get the `logits`, and calculate the `loss` in the same way.\n",
    ">    * You also need to calculate the `accuracy` for the validation batch using the metric you defined in `__init__`.\n",
    "\n",
    "* **Inside the `configure_optimizers` method**: \n",
    ">    * This method sets up the optimizer and learning rate scheduler for the training process.\n",
    ">    * You will call the `define_optimizer_and_scheduler` helper function to get the configured optimizer and scheduler.\n",
    "\n",
    "<details>\n",
    "<summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
    "\n",
    "If you're stuck, here is a more detailed breakdown.\n",
    "\n",
    "**For the `__init__` method**:\n",
    "\n",
    "* To load the model, you'll call the provided helper function:\n",
    "    > `self.model = load_resnet18(with num_classes from self.hparams, and weights_path from self.hparams)`\n",
    "* Instantiating the loss function is a direct call with no arguments:\n",
    "    > `self.loss_fn = instantiate nn.CrossEntropyLoss()`\n",
    "* For the accuracy metric, you need to provide the task type and number of classes:\n",
    "    > `self.accuracy = instantiate Accuracy(with the task set to \"multiclass\", and num_classes from self.hparams)`\n",
    "\n",
    "**For the `training_step` method**:\n",
    "\n",
    "* Unpacking the batch is straightforward: `x, y = batch`.\n",
    "* A forward pass is done by calling the instance itself on the input tensor: `logits = self(x)`\n",
    "* Calculate the loss by passing the predictions and true labels to your loss function:\n",
    "    > `loss = self.loss_fn(logits, y)`\n",
    "\n",
    "**For the `validation_step` method**:\n",
    "\n",
    "* The first three steps are identical to the `training_step`.\n",
    "* To calculate accuracy, you'll use the accuracy object you created, passing it the same logits and labels:\n",
    "    > `acc = self.accuracy(logits, y)`\n",
    "\n",
    "**For the `configure_optimizers` method**:\n",
    "\n",
    "* You need to call the helper function and capture its two outputs.\n",
    "* The function call should look like this:\n",
    "    > `optimizer, scheduler = call define_optimizer_and_scheduler(passing self.model, and the learning_rate and weight_decay from self.hparams)`\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ced0eb6-3ad9-403d-962a-c292164a94e5",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CLASS: ChestXRayClassifier\n",
    "\n",
    "class ChestXRayClassifier(pl.LightningModule):\n",
    "    \"\"\"A LightningModule that is focused on tracking validation loss and accuracy.\"\"\"\n",
    "\n",
    "    def __init__(self, model_weights_path, num_classes=3, learning_rate=1e-3, weight_decay=1e-2):\n",
    "        \"\"\"\n",
    "        Initializes the ChestXRayClassifier module.\n",
    "\n",
    "        Args:\n",
    "            model_weights_path (str): The file path to the pre-trained ResNet-18 model weights.\n",
    "            num_classes (int): The number of classes for classification. Defaults to 3.\n",
    "            learning_rate (float): The learning rate for the optimizer. Defaults to 1e-3.\n",
    "            weight_decay (float): The weight decay (L2 penalty) for the optimizer. Defaults to 1e-2.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Save all __init__ arguments (model_weights_path, num_classes, etc.) to self.hparams\n",
    "        # For example, youâ€™ll access `num_classes` as `self.hparams.num_classes`\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        # Call the `load_resnet18` helper function to get the pre-trained model.\n",
    "        self.model = load_resnet18(\n",
    "            self.hparams.num_classes,\n",
    "            model_weights_path\n",
    "        )\n",
    "        \n",
    "        # Define the Cross Entropy loss function.\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Define the accuracy metric using `Accuracy`.\n",
    "        # Remember to specify the task and the number of classes.\n",
    "        self.accuracy = Accuracy(\n",
    "            task=\"multiclass\",\n",
    "            num_classes=self.hparams.num_classes\n",
    "        )\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of images.\n",
    "        \"\"\"\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx=None):\n",
    "        \"\"\"\n",
    "        Performs a single training step. Loss calculation is required for backpropagation.\n",
    "\n",
    "        Args:\n",
    "        batch (tuple): A tuple containing the input images and their labels.\n",
    "        batch_idx (int): The index of the current batch. The Lightning Trainer\n",
    "                         requires this argument, but it's not utilized in this\n",
    "                         implementation as the logic is the same for all batches.\n",
    "        \"\"\"\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        # Unpack the batch into images and labels.\n",
    "        x, y = batch\n",
    "        # Perform a forward pass to get the model's logits.\n",
    "        logits = self.forward(x)\n",
    "        # Calculate the loss by comparing the logits to the true labels.\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx=None):\n",
    "        \"\"\"\n",
    "        Performs a single validation step and logs only the loss and accuracy.\n",
    "\n",
    "        Args:\n",
    "        batch (tuple): A tuple containing the input images and their labels.\n",
    "        batch_idx (int): The index of the current batch. The Lightning Trainer\n",
    "                         requires this argument, but it's not utilized in this\n",
    "                         implementation as the logic is the same for all batches.\n",
    "        \"\"\"\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        # Unpack the batch into images and labels.\n",
    "        x, y = batch\n",
    "        # Perform a forward pass to get the model's logits.\n",
    "        logits = self.forward(x)\n",
    "         # Calculate the loss.\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        # Calculate the accuracy.\n",
    "        acc = self.accuracy(logits, y)\n",
    "        \n",
    "         ### END CODE HERE ###\n",
    "        \n",
    "        # Log metrics for this validation epoch and show them in the progress bar.\n",
    "        self.log_dict({'val_loss': loss, 'val_acc': acc}, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Configures the optimizers and learning rate scheduler.\"\"\"\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        # Call the `define_optimizer_and_scheduler` helper function.\n",
    "        optimizer, scheduler = define_optimizer_and_scheduler(\n",
    "            self.model,\n",
    "            self.hparams.learning_rate,\n",
    "            self.hparams.weight_decay\n",
    "        )\n",
    "         ### END CODE HERE ###\n",
    "        \n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": \"val_loss\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d20449-187c-4bbf-9fcd-b06d53e3821c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Instantiate the classifier for verification.\n",
    "weights_path = helper_utils.setup_dummy_weights()\n",
    "verify_model = ChestXRayClassifier(model_weights_path=weights_path)\n",
    "\n",
    "# Get the optimizer and scheduler.\n",
    "optimizer_config = verify_model.configure_optimizers()\n",
    "optimizer = optimizer_config[\"optimizer\"]\n",
    "scheduler = optimizer_config[\"lr_scheduler\"][\"scheduler\"]\n",
    "\n",
    "# --- Print the results to verify ---\n",
    "print(\"--- LightningModule Components ---\")\n",
    "print(f\"Model Architecture: {verify_model.model.__class__.__name__}\")\n",
    "print(f\"Classifier Head:    {verify_model.model.fc}\")\n",
    "print(f\"Loss Function:      {verify_model.loss_fn.__class__.__name__}\")\n",
    "print(f\"Accuracy Metric:    {verify_model.accuracy.__class__.__name__}(num_classes={verify_model.accuracy.num_classes})\")\n",
    "print(f\"Optimizer:          {optimizer.__class__.__name__}\")\n",
    "print(f\"LR Scheduler:       {scheduler.__class__.__name__}\")\n",
    "# Clean dummy weights\n",
    "helper_utils.cleanup_dummy_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64121cd-4926-4d85-b6cd-5ad7faa32fd1",
   "metadata": {},
   "source": [
    "#### Expected Output\n",
    "\n",
    "```\n",
    "--- LightningModule Components ---\n",
    "Model Architecture: ResNet\n",
    "Classifier Head:    Linear(in_features=512, out_features=3, bias=True)\n",
    "Loss Function:      CrossEntropyLoss\n",
    "Accuracy Metric:    MulticlassAccuracy(num_classes=3)\n",
    "Optimizer:          AdamW\n",
    "LR Scheduler:       ReduceLROnPlateau\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6001d22b-ea03-429b-8c2e-c7da3c959b56",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "unittests.exercise_2(ChestXRayClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fa3d25-b63e-47fd-a149-415ed368de7d",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Training the Model\n",
    "\n",
    "You have successfully prepared the data and assembled the model. The final step is to configure and run the training process. For this purpose, you will set up a callback for efficient training and the trainer.\n",
    "\n",
    "<a name='4-1'></a>\n",
    "### 4.1 - Configuring Early Stopping\n",
    "\n",
    "Training a model for too long can lead to overfitting, while stopping too soon can result in an undertrained model. A common technique to find the right balance is **early stopping**.\n",
    "\n",
    "Lightning's [EarlyStopping](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.EarlyStopping.html) callback monitors a specific metric and automatically stops the training process once a desired performance threshold is met. This ensures you train just long enough to achieve your goal without wasting computational resources.\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### Exercise 3 - early_stopping\n",
    "\n",
    "Your exercise is to configure the `EarlyStopping` callback to intelligently manage your training duration.\n",
    "\n",
    "**Your Task**:\n",
    "\n",
    "* **Instantiate the `EarlyStopping` callback**:\n",
    ">\n",
    "    * You need to create an instance of the `EarlyStopping` class with the correct parameters.\n",
    ">    \n",
    "    * `monitor`: Specify the metric to track. For this assignment, you'll monitor the validation accuracy, which is logged as `\"val_acc\"`.\n",
    ">    \n",
    "    * `stopping_threshold`: Set the `stop_threshold` value for the monitored metric. The training will stop once this value is reached.\n",
    ">    \n",
    "    * `patience`: Define the number of epochs to wait for an improvement in `val_acc`. If the accuracy doesn't improve for this many consecutive epochs, training will stop. You should set this to **half** of the total `num_epochs`. Remember, patience must be an **integer**.\n",
    ">    \n",
    "    * `mode`: Tell the callback whether it should look for an increase or decrease in the monitored metric. Since a higher validation accuracy is better, you should set this to `\"max\"`.\n",
    "\n",
    "<details>\n",
    "<summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
    "\n",
    "If you're stuck, here is a more detailed breakdown.\n",
    "\n",
    "**For instantiating the `EarlyStopping` callback**:\n",
    "\n",
    "You are filling in the arguments for a single function call. The pseudo-code for the complete instantiation looks like this:\n",
    "```\n",
    "    stop = EarlyStopping(\n",
    "        monitor is set to the string 'val_acc',\n",
    "        stopping_threshold is set to the stop_threshold function argument,\n",
    "        patience is set to half of the num_epochs converted to an integer,\n",
    "        mode is set to the string 'max'\n",
    "        )\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afed6a0-9c09-47d0-a77b-f0c527e2045a",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: early_stopping\n",
    "\n",
    "def early_stopping(num_epochs, stop_threshold):\n",
    "    \"\"\"\n",
    "    Configures and returns a Lightning EarlyStopping callback.\n",
    "\n",
    "    Args:\n",
    "        num_epochs (int): The maximum number of epochs, used to set patience.\n",
    "        stop_threshold (float): The validation accuracy threshold to stop training.\n",
    "\n",
    "    Returns:\n",
    "        EarlyStopping: The configured Lightning callback.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Instantiate the EarlyStopping callback.\n",
    "    stop = EarlyStopping(\n",
    "        # Specify the metric to monitor as validation accuracy.\n",
    "        monitor=\"val_acc\",\n",
    "        \n",
    "        # Set the value that the monitored metric must reach to stop training.\n",
    "        stopping_threshold=stop_threshold,\n",
    "        \n",
    "        # Set patience to half of the total epochs. \n",
    "        # HINT: Use type casting to ensure the value is an integer.\n",
    "        patience= int(num_epochs / 2),\n",
    "        \n",
    "        # Set the mode to \"max\" because a higher accuracy is better.\n",
    "        mode=\"max\"\n",
    "    ) \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870c1ad1-36f2-4007-86c1-da1beb0c3389",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Define sample parameters for verification.\n",
    "num_epochs_verify = 15\n",
    "stop_threshold_verify = 0.90\n",
    "\n",
    "# Call the function to create the callback.\n",
    "verify_callback = early_stopping(num_epochs_verify, stop_threshold_verify)\n",
    "\n",
    "# --- Print the results to verify ---\n",
    "print(\"--- EarlyStopping Configuration ---\")\n",
    "print(f\"Metric to Monitor:     {verify_callback.monitor}\")\n",
    "print(f\"Stopping Threshold:    {verify_callback.stopping_threshold}\")\n",
    "print(f\"Patience:              {verify_callback.patience}\")\n",
    "print(f\"Mode:                  {verify_callback.mode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f70155e-e78f-43ed-9d7d-13aa18299922",
   "metadata": {},
   "source": [
    "#### Expected Output:\n",
    "\n",
    "```\n",
    "--- EarlyStopping Configuration ---\n",
    "Metric to Monitor:     val_acc\n",
    "Stopping Threshold:    0.9\n",
    "Patience:              7\n",
    "Mode:                  max\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3b6dc7-6401-4a8c-b401-4d53285d15f3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "unittests.exercise_3(early_stopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f60e832-65bd-4731-bf3d-41dd5921a4ac",
   "metadata": {},
   "source": [
    "<a name='4-2'></a>\n",
    "### 4.2 - Configuring the Trainer\n",
    "\n",
    "With your `DataModule`, `LightningModule`, and `EarlyStopping` callback ready, you can now configure the training process. The `pl.Trainer` is the engine that orchestrates the entire training loop, handling everything from hardware acceleration to callbacks automatically.\n",
    "\n",
    "<a name='ex-4'></a>\n",
    "### Exercise 4 - run_training\n",
    "\n",
    "Your exercise is to configure the Lightning `Trainer` and use it to run the training process.\n",
    "\n",
    "**Your Task**:\n",
    "\n",
    "* **Instantiate the `pl.Trainer`**:\n",
    "    * You need to complete the instantiation of the `Trainer` object.\n",
    "    * Set the maximum number of training epochs to the `num_epochs` provided.\n",
    "    * Enable **16-bit mixed-precision training** for faster computation.\n",
    "    * Pass the `callback` to the `Trainer`. Note that the callbacks argument expects a list of callbacks.\n",
    "* **Run the training**:\n",
    "    * After configuring the `Trainer`, you must start the training loop.\n",
    "    * Call the `.fit()` method on your trainer instance, passing it the `model` and `data_module`.\n",
    "\n",
    "<details>\n",
    "<summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
    "\n",
    "If you're stuck, here is a more detailed breakdown.\n",
    "\n",
    "**For instantiating the `pl.Trainer`**:\n",
    "\n",
    "* You are filling in the arguments for the `pl.Trainer` constructor.\n",
    "* The `max_epochs` argument should be set to the `num_epochs` variable.\n",
    "* The precision argument should be the string `'16-mixed'`.\n",
    "* The `callbacks` argument must be a list. The call will look like this:\n",
    "    > `callbacks=[callback]`\n",
    "\n",
    "**For running the training**:\n",
    "\n",
    "* This is a method call on the `trainer` object you just created.\n",
    "* The pseudo-code for this call is:\n",
    "    > `call the fit method on the trainer, passing in the model and the data_module`\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e05e395-7bcc-47cf-99b7-9c32ce20105b",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: run_training\n",
    "\n",
    "def run_training(model, data_module, num_epochs, callback, progress_bar=True, dry_run=False):\n",
    "    \"\"\"\n",
    "    Configures and runs a Lightning mixed-precision training process.\n",
    "\n",
    "    Args:\n",
    "        model (pl.LightningModule): The model to be trained.\n",
    "        data_module (pl.LightningDataModule): The data module that provides the datasets.\n",
    "        num_epochs (int): The maximum number of epochs for training.\n",
    "        callback (pl.Callback): A callback, such as for early stopping.\n",
    "        progress_bar (bool): If True, shows the training progress bar. Defaults to True.\n",
    "        dry_run (bool): If True, runs a quick single batch \"dry run\" to test the code.\n",
    "                        Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            - pl.Trainer: The trainer instance after fitting is complete.\n",
    "            - pl.LightningModule: The trained model with updated weights.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Instantiate the PyTorch Lightning Trainer with specific configurations.\n",
    "    trainer = pl.Trainer(\n",
    "        # Set the maximum number of training epochs.\n",
    "        max_epochs=num_epochs,\n",
    "        \n",
    "        # Automatically select the best hardware accelerator (GPU, CPU).\n",
    "        accelerator=\"auto\", \n",
    "        # Use a single device.\n",
    "        devices=1, \n",
    "        \n",
    "        # Enable 16-bit mixed-precision training to speed up computation.\n",
    "        precision=\"16-mixed\",\n",
    "        # Add callback\n",
    "        callbacks=[callback],\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "        # Disable the default logger.\n",
    "        logger=False,\n",
    "        # Show the training progress bar.\n",
    "        enable_progress_bar=progress_bar,\n",
    "        # Disable the model summary printout\n",
    "        enable_model_summary=False,\n",
    "        # Disable automatic model checkpointing.\n",
    "        enable_checkpointing=False,\n",
    "        # fast_dev_run flag for test runs.\n",
    "        fast_dev_run=dry_run\n",
    "    )\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Run the training process.\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return trainer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70ca2e5-0aa0-472a-8efc-8b8b6c660f60",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Set up the components for verification run.\n",
    "verify_dm = ChestXRayDataModule(data_dir=data_dir, batch_size=2)\n",
    "weights_path = helper_utils.setup_dummy_weights()\n",
    "verify_model = ChestXRayClassifier(model_weights_path=weights_path)\n",
    "num_epochs=1\n",
    "verify_callback = early_stopping(num_epochs=num_epochs, stop_threshold=0.99)\n",
    "\n",
    "# Call the run_training function with dry_run=True.\n",
    "print(\"--- Verifying Training Run (Dry Run) ---\")\n",
    "run_training(\n",
    "    model=verify_model, \n",
    "    data_module=verify_dm, \n",
    "    num_epochs=num_epochs, \n",
    "    callback=verify_callback,\n",
    "    dry_run=True\n",
    ")\n",
    "\n",
    "print(\"The Trainer configured and ran a single batch without errors.\")\n",
    "# Clean dummy weights\n",
    "helper_utils.cleanup_dummy_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0bab68-77c8-423f-9d82-5f327bf47051",
   "metadata": {},
   "source": [
    "#### Expected Output:\n",
    "\n",
    "```\n",
    "--- Verifying Training Run (Dry Run) ---\n",
    "Epochâ€‡0:â€‡100% 1/1\n",
    "The Trainer configured and ran a single batch without errors.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f37d89-3c80-4f8a-9db8-d0fff6e34520",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "unittests.exercise_4(run_training, ChestXRayDataModule, ChestXRayClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35adda95-b670-487d-b80d-3cf6bd44f007",
   "metadata": {},
   "source": [
    "---\n",
    "# Submission Note\n",
    "\n",
    "Congratulations! You've completed the final graded exercise of this assignment.\n",
    "\n",
    "If you've successfully passed all the unit tests above, you've completed the core requirements of this assignment. Feel free to [submit](#submission) your work now. The grading process runs in the background, so it will not disrupt your progress and you can continue on with the rest of the material.\n",
    "\n",
    "**ðŸš¨ IMPORTANT NOTE** If you have passed all tests within the notebook, but the autograder shows a system error after you submit your work:\n",
    "\n",
    "<div style=\"background-color: #1C1C1E; border: 1px solid #444444; color: #FFFFFF; padding: 15px; border-radius: 5px;\">\n",
    "    <p><strong>Grader Error: Grader feedback not found</strong></p>\n",
    "    <p>Autograder failed to produce the feedback...</p>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "This is typically a temporary system glitch. The most common solution is to resubmit your assignment, as this often resolves the problem. Occasionally, it may be necessary to resubmit more than once. \n",
    ">\n",
    "If the error persists, please reach out for support in the [DeepLearning.AI Community Forum](https://community.deeplearning.ai/c/course-q-a/pytorch-for-developers/pytorch-techniques-and-ecosystem-tools/561).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fe4305-50d1-4ad6-a4a5-5d0d5bb5d667",
   "metadata": {},
   "source": [
    "<a name='4-3'></a>\n",
    "### 4.3 - Training Your Diagnostic Assistant\n",
    "\n",
    "You have now assembled and verified all the necessary components. This final code cell brings everything together to run the full training process.\n",
    "\n",
    "Training a model for a task like this isn't a quick process; it requires a significant amount of time and computation. To save you from a lengthy training session, you are being provided with pre-trained weights for the ResNet-18 model backbone.\n",
    "\n",
    "**How the Model Was Trained**\n",
    "\n",
    "The setup used to train this model was nearly identical to the one in this assignment. The key difference was a two-stage training strategy, which you were introduced to in Module 2 of this course.\n",
    "\n",
    "**Why a Two-Stage Strategy?**\n",
    "\n",
    "The original ResNet-18 model was trained on general images (like photos of cats, dogs, and cars), not medical scans. While its core layers are excellent at detecting basic shapes and textures, they need to be adapted for the specific patterns found in X-rays. This two-stage approach provides a much faster turnaround with limited resources compared to training the entire model from scratch.\n",
    "\n",
    "* **Feature Extraction**: First, only the new classifier head was trained for **10 epochs** with a learning rate of **1e-3**. This quickly teaches the final layer to make predictions based on the general features from the frozen ResNet backbone.\n",
    "\n",
    "* **Fine-Tuning**: Simply training the classifier head isn't always enough. In this stage, the **last two convolutional blocks** (`layer3` and `layer4`) of the ResNet model were \"unfrozen\" and the entire model was trained for another **15 epochs** with a much smaller learning rate of **1e-5**. This carefully adjusts the pre-trained layers to become better at identifying pneumonia-specific patterns.\n",
    "\n",
    "After this two-stage process, the model achieved a final validation accuracy of approximately **80%**.\n",
    "\n",
    "* Run the cell below to define the path to these pre-trained model weights, which you will use to train the model's classifier head further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d990a278-05ae-4813-9ce2-41d76056666a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Define the file path for the pre-trained model weights.\n",
    "pretrained_weights = \"./resnet18_chest_xray_classifier_weights.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e632102-d38e-4be7-bc55-a7a66e7b9366",
   "metadata": {},
   "source": [
    "These are the key hyperparameters for your training run. Feel free to experiment with different values, but start with these recommended numbers:\n",
    "\n",
    "* Set the maximum number of training epochs to **10**.\n",
    "* Define the target validation accuracy for early stopping as **85%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708aa2c2-ec89-4db9-bf4c-beb2ce690bf7",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# EDITABLE CELL\n",
    "\n",
    "# Set the maximum number of training cycles (epochs).\n",
    "training_epochs = 10\n",
    "\n",
    "# Define the validation accuracy threshold to stop training once reached.\n",
    "target_accuracy = 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98433018-c713-4835-ac6c-a4dab120814b",
   "metadata": {},
   "source": [
    "* Create the `EarlyStopping` callback by calling your `early_stopping` function with the hyperparameters you just defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5601b4-ba0c-451c-adbf-94e527aadec8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Create the EarlyStopping callback by calling your function with the defined hyperparameters.\n",
    "early_stopping_callback = early_stopping(training_epochs, target_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e4f699-d3b4-4517-ab2c-9185a520c0d7",
   "metadata": {},
   "source": [
    "* Instantiate your `ChestXRayDataModule` and `ChestXRayClassifier`.\n",
    "* Call your `run_training` function with all the components to start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97a4f30-bd8c-446b-a217-e3798559e90c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Set the random seed to ensure the experiment is reproducible.\n",
    "pl.seed_everything(15)\n",
    "\n",
    "# Instantiate the DataModule to handle data loading and transformations.\n",
    "dm = ChestXRayDataModule(data_dir)\n",
    "dm.setup()\n",
    "\n",
    "# Instantiate the LightningModule, passing in the path to the pre-trained weights.\n",
    "model = ChestXRayClassifier(model_weights_path=pretrained_weights)\n",
    "\n",
    "# Call the training function with all the components to start the training process.\n",
    "trained_trainer, trained_model = run_training(\n",
    "    model, dm, training_epochs, early_stopping_callback\n",
    ")\n",
    "\n",
    "# Print a message to confirm that the training has finished.\n",
    "print(\"\\n--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aea6ab-bbd7-4eb4-8334-5a55f08bc2e0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Congratulations on training your model! The `Trainer` object keeps a record of the metrics from the training run.\n",
    "\n",
    "* Run the cell below to access the final validation accuracy from your `trained_trainer` object and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a18a12-a739-4d07-b399-2310a94fcc20",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Get the final metrics from the trainer object\n",
    "final_metrics = trained_trainer.callback_metrics\n",
    "\n",
    "# Extract the validation accuracy and convert it to a number\n",
    "final_val_acc = final_metrics[\"val_acc\"].item()\n",
    "\n",
    "# Print the final validation accuracy\n",
    "print(f\"Final Validation Accuracy: {final_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75755973-ff73-4900-862d-99dea50c74a9",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Evaluating Model Performance\n",
    "\n",
    "Your model has been trained, and you have its final validation accuracy. Now, you will dive deeper to understand its performance on a more granular level. An overall accuracy score is useful, but it doesn't tell the whole story, especially in a medical context where the cost of misclassification can be high.\n",
    "\n",
    "<a name='5-1'></a>\n",
    "### 5.1 - Confusion Matrix and Per-Class Accuracy\n",
    "\n",
    "To begin your analysis, you will generate a **confusion matrix**. This powerful tool will show not just how many predictions were correct, but also what kind of mistakes the model made (e.g., confusing Bacterial Pneumonia with Viral Pneumonia).\n",
    "\n",
    "From this matrix, you will also calculate the **per-class accuracy**, which tells you how well the model performed on each individual class (`NORMAL`, `BACTERIAL_PNEUMONIA`, and `VIRAL_PNEUMONIA`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c802fd-e1b4-4f2f-b84e-2fcdba889f99",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Evaluate the trained model to display a per-class accuracy report and a confusion matrix.\n",
    "helper_utils.per_class_acc_and_conf_matrix(trained_model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1d98c7-bdd8-40c5-aaba-ae14287b1c96",
   "metadata": {},
   "source": [
    "<a name='5-2'></a>\n",
    "### 5.2 - Visualizing Predictions\n",
    "\n",
    "Your analysis is more complete when you look at how your model performs on individual images. Take a look at some of the predictions your model makes on the validation set.\n",
    "\n",
    "* Run the `display_random_predictions` function, which makes use of the validation dataset from the `dm` `DataModule`. Each execution will choose a random batch of images to display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3d546a-f10c-4e1d-a414-d74c2368d930",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "helper_utils.display_random_predictions(trained_model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fea6a1-f263-4820-bb79-558c902395ef",
   "metadata": {},
   "source": [
    "<a name='5-3'></a>\n",
    "### 5.3 - A True Test\n",
    "\n",
    "Now, for a true test of your modelâ€™s predictions. Some images were removed from the original dataset and saved as a truly unseen group. This means your model has never had any exposure to these images during training or validation.\n",
    "\n",
    "These images are available at the following paths:\n",
    "\n",
    "* **BACTERIAL_PNEUMONIA**\n",
    "    * \"./unseen/BACTERIAL_PNEUMONIA/person296_bacteria_1394.jpeg\" \n",
    "    * \"./unseen/BACTERIAL_PNEUMONIA/person441_bacteria_1916.jpeg\" \n",
    "    * \"./unseen/BACTERIAL_PNEUMONIA/person564_bacteria_2342.jpeg\" \n",
    "* **NORMAL**\n",
    "    * \"./unseen/NORMAL/IM-0353-0001.jpeg\" \n",
    "    * \"./unseen/NORMAL/IM-0633-0001.jpeg\" \n",
    "    * \"./unseen/NORMAL/NORMAL2-IM-0866-0001.jpeg\" \n",
    "* **VIRAL_PNEUMONIA**\n",
    "    * \"./unseen/VIRAL_PNEUMONIA/person1369_virus_2356.jpeg\" \n",
    "    * \"./unseen/VIRAL_PNEUMONIA/person1465_virus_2537.jpeg\" \n",
    "    * \"./unseen/VIRAL_PNEUMONIA/person1537_virus_2674.jpeg\" \n",
    "\n",
    "Use the `display_prediction` function to see how well your trained model performs on these images.\n",
    "\n",
    "* Feel free to set the image_path to any one of these images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c75b4e-1044-406d-a726-51096bdff920",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# EDITABLE CELL\n",
    "\n",
    "# Define the file path for the unseen image to be predicted.\n",
    "image_path = \"./unseen/BACTERIAL_PNEUMONIA/person296_bacteria_1394.jpeg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f6312f-c0cd-4292-ac3c-7cafb86d4e5c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Use the display_prediction function to show the image and its predicted class.\n",
    "helper_utils.display_prediction(trained_model, dm, image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dd4f72-3e0b-4329-84ce-e9a3eed00898",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Excellent work on completing this assignment. You have successfully navigated the process of building a complete deep learning system for medical image analysis. By applying your knowledge of PyTorch and Lightning, you have engineered the core engine for a pneumonia diagnostic assistant, turning theoretical concepts into a practical and meaningful application.\n",
    "\n",
    "You began this assignment with a mission to solve a real world problem, and you have accomplished that by creating a well organized and effective classifier. The skills you practiced here, especially structuring your project with the `DataModule` and `LightningModule`, are vital for building scalable and maintainable AI systems in any professional environment. You should be proud of the sophisticated tool you have built. Keep leveraging these skills to tackle new and exciting challenges."
   ]
  }
 ],
 "metadata": {
  "grader_version": "1",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
