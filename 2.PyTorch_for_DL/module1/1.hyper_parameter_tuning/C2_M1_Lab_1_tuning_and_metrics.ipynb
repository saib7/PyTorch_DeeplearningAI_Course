{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "677a2e55",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning: Learning Rate and Metrics\n",
    "\n",
    "Welcome to this first exploration of hyperparameter tuning using the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset!  This dataset consists on 60,000 32x32 color images in 10 classes (6,000 images per class). Here are the classes in the dataset, as well as 10 random images from each: \n",
    "\n",
    "![](./nb_image/cifar10.png)\n",
    "\n",
    "In this notebook, you'll focus specifically on the learning rate, an essential hyperparameter that dictates the pace at which a model learns during training. You'll work with a simple convolutional neural network (CNN) and observe how changes in hyperparameters affect the model's outcomes.\n",
    "\n",
    "This lab will cover the following:\n",
    "\n",
    "* Examining the effects of different **learning rates** on model performance.\n",
    "\n",
    "* Introducing and using additional metrics like **precision, recall, and F1 score** for a more complete evaluation.\n",
    "\n",
    "* Exploring the effect of **batch size** on different metrics using an imbalanced dataset in an optional section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e28835",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebe8e4f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "\n",
    "import helper_utils\n",
    "\n",
    "helper_utils.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44dd2a81-5c2c-426b-be1a-98e63791174f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# # Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248ce8e8",
   "metadata": {},
   "source": [
    "## Learning Rate Optimization on CIFAR-10\n",
    "\n",
    "You will now explore how the learning rate affects the performance of a simple convolutional neural network (CNN) on the CIFAR-10 dataset.\n",
    "For that you will train a simple CNN model with different learning rates and observe the validation accuracy to understand the sensitivity of the model to this hyperparameter.\n",
    "\n",
    "The code below sets up the necessary functions for training the model and evaluating its performance:\n",
    "\n",
    "- `SimpleCNN`: defines a small convolutional neural network architecture.\n",
    "\n",
    "- `evaluate_accuracy`: computes the accuracy on a validation dataset.\n",
    "\n",
    "These functions encapsulate the key elements required for running the hyperparameter optimization experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89711681",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"A simple Convolutional Neural Network (CNN) architecture.\n",
    "\n",
    "    This class defines a two-layer CNN with max pooling, dropout, and\n",
    "    fully connected layers, suitable for basic image classification tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the layers of the neural network.\"\"\"\n",
    "        # Initialize the parent nn.Module class\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # First convolutional layer (3 input channels, 16 output channels, 3x3 kernel)\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        # Second convolutional layer (16 input channels, 32 output channels, 3x3 kernel)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        # Max pooling layer with a 2x2 window and stride of 2\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # First fully connected (linear) layer\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 64)\n",
    "        # Second fully connected (linear) layer, serving as the output layer\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "         \n",
    "    def forward(self, x):\n",
    "        \"\"\"Defines the forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of shape (batch_size, 3, height, width).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output logits from the network.\n",
    "        \"\"\"\n",
    "        # Apply first convolution, ReLU activation, and max pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # Apply second convolution, ReLU activation, and max pooling\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Flatten the feature maps for the fully connected layers\n",
    "        x = x.view(-1, 32 * 8 * 8)\n",
    "        # Apply the first fully connected layer with ReLU activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Apply dropout for regularization\n",
    "        x = self.dropout(x)\n",
    "        # Apply the final output layer\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def evaluate_accuracy(model, val_loader, device):\n",
    "    \"\"\"Calculates the accuracy of a model on a given dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to be evaluated.\n",
    "        val_loader (DataLoader): The DataLoader containing the validation or test data.\n",
    "        device: The device (e.g., 'cuda' or 'cpu') to perform the evaluation on.\n",
    "\n",
    "    Returns:\n",
    "        float: The computed accuracy of the model on the dataset.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    # Initialize counters for accuracy calculation\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Disable gradient computation for evaluation\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the data in the provided loader\n",
    "        for inputs, labels in val_loader:\n",
    "            # Move input and label tensors to the specified device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # Perform a forward pass to get model outputs\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Get the predicted class by finding the index of the maximum logit\n",
    "            _, predicted = outputs.max(1)\n",
    "            # Update the count of correctly classified samples\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            # Update the total number of samples processed\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    # Calculate the final accuracy\n",
    "    accuracy = total_correct / total_samples\n",
    "    # Return the computed accuracy\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427f921c",
   "metadata": {},
   "source": [
    "In the code below, the `train_and_evaluate` function:\n",
    "\n",
    "- defines the model  \n",
    "- sets up the optimizer with a specified learning rate and defines the loss function  \n",
    "- prepares the train and validation dataloaders  \n",
    "- trains the model for a specified number of epochs  \n",
    "- evaluates the model's performance on the validation set\n",
    "\n",
    "The functions `get_dataset_dataloaders` and `train_model` are imported from `helper_utils` and handle dataset preparation and model training, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cef3e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(learning_rate, device, n_epochs=25, batch_size=128):\n",
    "    \"\"\"Trains and evaluates a model for a specific learning rate and configuration.\n",
    "\n",
    "    This function orchestrates the entire workflow: it sets a random seed,\n",
    "    initializes the model, optimizer, and dataloaders, trains the model,\n",
    "    and finally evaluates its accuracy on a validation set.\n",
    "\n",
    "    Args:\n",
    "        learning_rate (float): The learning rate to use for the optimizer.\n",
    "        device: The device (e.g., 'cuda' or 'cpu') for training and evaluation.\n",
    "        n_epochs (int, optional): The number of epochs for training. Defaults to 25.\n",
    "        batch_size (int, optional): The batch size for the dataloaders. Defaults to 128.\n",
    "\n",
    "    Returns:\n",
    "        float: The final validation accuracy of the trained model.\n",
    "    \"\"\"\n",
    "    # Set the random seed for reproducibility\n",
    "    helper_utils.set_seed(42)\n",
    "\n",
    "    # Initialize the CNN model and move it to the specified device\n",
    "    model = SimpleCNN().to(device)\n",
    "\n",
    "    # Define the loss function\n",
    "    loss_fcn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Define the optimizer with the given learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Prepare the training and validation dataloaders\n",
    "    train_dataloader, val_dataloader = helper_utils.get_dataset_dataloaders(batch_size=batch_size)\n",
    "\n",
    "    # Call the main training loop to train the model\n",
    "    helper_utils.train_model(model=model, optimizer=optimizer, loss_fcn=loss_fcn, train_dataloader=train_dataloader, device=device, n_epochs=n_epochs) \n",
    "\n",
    "    # Evaluate the trained model's accuracy on the validation set\n",
    "    accuracy = evaluate_accuracy(model=model, val_loader=val_dataloader, device=device)\n",
    "\n",
    "    # Print the final results for this configuration\n",
    "    print(\n",
    "        f\"Learning Rate: {learning_rate}, Accuracy: {accuracy:.4f}\"\n",
    "    )\n",
    "    # Return the computed accuracy\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e669122",
   "metadata": {},
   "source": [
    "A few different learning rates are used from a low ($10^{-5}$) to a high value ($10^{-1}$).\n",
    "\n",
    "For each learning rate, the model is trained for **25 epochs** with a **batch size of 128**.\n",
    "After training, the model is evaluated on the validation set to compute the validation accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52f77ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [31:02<00:00, 91.5kB/s]   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687d4eb13f4140bd9aaf84abb3ebb9f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current Epoch:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52463bedda5d40de86fabeebadf73cb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current Batch:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train Loss: 2.2604\n",
      "Epoch 10 - Train Loss: 2.1798\n",
      "Epoch 15 - Train Loss: 2.1048\n",
      "Epoch 20 - Train Loss: 2.0460\n",
      "Epoch 25 - Train Loss: 1.9959\n",
      "Training complete!\n",
      "\n",
      "Learning Rate: 1e-05, Accuracy: 0.3240\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d47d7af49c41b7b54458c869597962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current Epoch:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "627b69ceeeda4e59998146606c8a6532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current Batch:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train Loss: 1.9068\n",
      "Epoch 10 - Train Loss: 1.6964\n",
      "Epoch 15 - Train Loss: 1.5813\n",
      "Epoch 20 - Train Loss: 1.5115\n",
      "Epoch 25 - Train Loss: 1.4459\n",
      "Training complete!\n",
      "\n",
      "Learning Rate: 0.0001, Accuracy: 0.4885\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dad4caae9034233b9fa9b1fdf592228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current Epoch:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d69a16675284669a82d91ae88475161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current Batch:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train Loss: 1.5232\n",
      "Epoch 10 - Train Loss: 1.3653\n",
      "Epoch 15 - Train Loss: 1.2141\n",
      "Epoch 20 - Train Loss: 1.1190\n",
      "Epoch 25 - Train Loss: 1.0013\n",
      "Training complete!\n",
      "\n",
      "Learning Rate: 0.001, Accuracy: 0.5855\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbdb8833383141299c71b9cc91f432d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current Epoch:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39dc8e60fd904e2db6a35109556ea597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current Batch:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train Loss: 1.6899\n",
      "Epoch 10 - Train Loss: 1.5898\n",
      "Epoch 15 - Train Loss: 1.5328\n",
      "Epoch 20 - Train Loss: 1.4965\n",
      "Epoch 25 - Train Loss: 1.4509\n",
      "Training complete!\n",
      "\n",
      "Learning Rate: 0.01, Accuracy: 0.4045\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "113a929c96134cc29bf6dcc3eafa4170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current Epoch:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdaf2889181949c69f499783258cccc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current Batch:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train Loss: 2.3090\n",
      "Epoch 10 - Train Loss: 2.3106\n",
      "Epoch 15 - Train Loss: 2.3088\n",
      "Epoch 20 - Train Loss: 2.3106\n",
      "Epoch 25 - Train Loss: 2.3086\n",
      "Training complete!\n",
      "\n",
      "Learning Rate: 0.1, Accuracy: 0.1050\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.00001, 0.0001, 0.001, 0.01, 0.1] # low to high\n",
    "accuracies = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    acc = train_and_evaluate(learning_rate=lr, device=device)\n",
    "    accuracies.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f2b1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.plot_results(learning_rates, accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4249bf",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The plot shows that the learning rate of 0.001 provides the highest validation accuracy, indicating it is the **optimal choice** between the tested values.\n",
    "\n",
    "Smaller learning rates leads to slower learning and lower accuracy, suggesting that the optimizer will take a long time to converge. \n",
    "Larger learning rates results in unstable training and lower accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fd3d1e",
   "metadata": {},
   "source": [
    "## Other relevant metrics: Recall, Precision, and F1 Score\n",
    "\n",
    "Deep learning models are typically trained by optimizing a *loss function*, such as **cross-entropy loss**. \n",
    "This loss quantifies the difference between the predicted probabilities and the true labels, guiding the model to adjust its parameters.\n",
    "It provides a scalar loss that can be minimized using gradient-based optimization algorithms.\n",
    "\n",
    "However, the loss function optimized during training does not always align with the **evaluation metrics** you care about in practice. Metrics like **accuracy**, **precision**, **recall**, and **F1 score** offer a more nuanced understanding of model performance.\n",
    "\n",
    "-  **Accuracy**: The ratio of correct predictions to the total number of predictions. \n",
    "But it can be misleading in **imbalanced datasets**, where the model may achieve high accuracy simply by predicting the majority class.\n",
    "\n",
    "- **Precision and Recall**:\n",
    "These metrics evaluate performance on a **per-class** basis, offering better insight into class-specific behaviors.\n",
    "For each class $k$:\n",
    "  $$\n",
    "  \\text{Precision}_k = \\frac{\\text{TP}_k}{\\text{TP}_k + \\text{FP}_k}\n",
    "  $$\n",
    "  $$\n",
    "  \\text{Recall}_k = \\frac{\\text{TP}_k}{\\text{TP}_k + \\text{FN}_k}\n",
    "  $$\n",
    "\n",
    "  Where:\n",
    "  - **TPₖ** (True Positives): Correct predictions for class $k$.\n",
    "  - **FPₖ** (False Positives): Instances incorrectly predicted as class $k$.\n",
    "  - **FNₖ** (False Negatives): Instances truly belonging to class $k$ but predicted as another class.\n",
    "  \n",
    "  To evaluate overall performance in a multiclass setting, these metrics can be aggregated:\n",
    "\n",
    "  - **Macro-average**:\n",
    "    Averages metrics across classes, treating all classes equally.\n",
    "    *Useful when class balance is important.*\n",
    "  - **Micro-average**:\n",
    "    Computes metrics globally by aggregating all TP, FP, and FN.\n",
    "    *Useful when class sizes vary significantly.*\n",
    "  - **Weighted-average**:\n",
    "    Averages metrics across classes, weighted by the number of true instances per class.\n",
    "    *Balances influence of both major and minor classes.*\n",
    "\n",
    "- **F1 Score**:\n",
    "  It combines precision and recall into a single number, useful when both false positives and false negatives are important:\n",
    "  $$\n",
    "  \\text{F1}_k = 2 \\cdot \\frac{\\text{Precision}_k \\cdot \\text{Recall}_k}{\\text{Precision}_k + \\text{Recall}_k}\n",
    "  $$\n",
    "  Like precision and recall, it can be aggregated (macro, micro, weighted) depending on the evaluation goal.\n",
    "\n",
    "All these metrics range from 0 to 1, where higher values indicate better performance. The goal is to maximize each metric.\n",
    "\n",
    "Some use cases for these metrics include:\n",
    "\n",
    "- **Precision** is essential when **false positives** are costly (e.g., spam detection, medical diagnosis).\n",
    "- **Recall** is vital when **false negatives** are costly (e.g., disease screening, fraud detection).\n",
    "- **F1 score** is ideal when both false positives and false negatives matter, as it provides a trade-off between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3dd955",
   "metadata": {},
   "source": [
    "## Exercise: Implementing Metrics in PyTorch\n",
    "\n",
    "The following function `evaluate_metrics` is designed to evaluate a model's performance on a validation dataset using various metrics such as accuracy, precision, recall, and F1 score.\n",
    "\n",
    "Your exercise is to complete the `evaluate_metrics` function, which calculates key performance metrics for a model using the `torchmetrics` library.\n",
    "\n",
    "**Your Task**:\n",
    "\n",
    "* **Initialize the metrics**:\n",
    "    * In the first code block, create instances for **precision, recall, and F1 score** using the corresponding classes from `torchmetrics`.\n",
    "    * For each metric, ensure you set the parameters correctly: `task=\"multiclass\"`, `num_classes=num_classes`, and `average=\"macro\"`.\n",
    "* **Update the metrics in the loop**:\n",
    "    * Within the second code block, inside the `for` loop, you must update each metric with the results from the current batch.\n",
    "    * Call the `.update()` method on your `precision_metric`, `recall_metric`, and `f1_metric` objects, passing in the predicted values and the true labels.\n",
    "* **Compute the final scores**:\n",
    "    * In the final code block, after the evaluation loop is complete, compute the final value for each metric.\n",
    "    * Call the `.compute()` method for each metric and then use `.item()` to get the final scalar result to assign to the `precision`, `recall`, and `f1` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fcfa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(model, val_dataloader, device, num_classes=10):\n",
    "    \"\"\"Evaluates the model on a given dataset using multiple metrics.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The PyTorch model to be evaluated.\n",
    "        val_dataloader (DataLoader): The DataLoader containing the validation data.\n",
    "        device (torch.device): The device (e.g., 'cuda' or 'cpu') to run the evaluation on.\n",
    "        num_classes (int, optional): The number of classes in the dataset. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the computed accuracy, precision, recall, and F1 score.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize accuracy metric\n",
    "    accuracy_metric = torchmetrics.Accuracy(\n",
    "        task=\"multiclass\", num_classes=num_classes, average=\"macro\"\n",
    "    ).to(device)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Initialize precision metric  using `torchmetrics.Precision`\n",
    "    precision_metric = torchmetrics.Precision(\n",
    "        task=\"multiclass\", num_classes=num_classes, average=\"macro\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize recall metric using `torchmetrics.Recall`\n",
    "    recall_metric = torchmetrics.Recall(\n",
    "        task=\"multiclass\", num_classes=num_classes, average=\"macro\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize F1 score metric using `torchmetrics.F1Score`\n",
    "    f1_metric = torchmetrics.F1Score(\n",
    "        task=\"multiclass\", num_classes=num_classes, average=\"macro\"\n",
    "    ).to(device)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Disable gradient computation during evaluation\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the validation dataloader\n",
    "        for inputs, labels in val_dataloader:\n",
    "            # Move inputs and labels to the specified device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # Get model predictions\n",
    "            outputs = model(inputs)\n",
    "            # Get the predicted class by finding the index of the maximum logit\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            # Update metrics with the predictions and true labels for the current batch\n",
    "            accuracy_metric.update(predicted, labels)\n",
    "            \n",
    "    ### START CODE HERE ###\n",
    "            \n",
    "            # Update `precision_metric` using `.update` method \n",
    "            precision_metric.update(predicted, labels)\n",
    "            \n",
    "            # Update `recall_metric` using `.update` method\n",
    "            recall_metric.update(predicted, labels)\n",
    "            \n",
    "            # Update `f1_metric` using `.update` method\n",
    "            f1_metric.update(predicted, labels)\n",
    "             \n",
    "    # Compute the final metrics over the entire dataset\n",
    "    accuracy = accuracy_metric.compute().item()\n",
    "    \n",
    "    # Compute precision using `.compute` method and get the value with `.item()`\n",
    "    precision = precision_metric.compute().item()\n",
    "    \n",
    "    # Compute recall using `.compute` method and get the value with `.item()`\n",
    "    recall = recall_metric.compute().item()\n",
    "    \n",
    "    # Compute F1 score using `.compute` method and get the value with `.item()`\n",
    "    f1 = f1_metric.compute().item()\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee43ed2c-ec0c-4d42-8419-b4cf349726db",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<details>\n",
    "<summary><span style=\"color:green;\"><strong>Solution (Click here to expand)</strong></span></summary>\n",
    "\n",
    "```python\n",
    "def evaluate_metrics(model, val_dataloader, device, num_classes=10):\n",
    "    \"\"\"Evaluates the model on a given dataset using multiple metrics.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The PyTorch model to be evaluated.\n",
    "        val_dataloader (DataLoader): The DataLoader containing the validation data.\n",
    "        device (torch.device): The device (e.g., 'cuda' or 'cpu') to run the evaluation on.\n",
    "        num_classes (int, optional): The number of classes in the dataset. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the computed accuracy, precision, recall, and F1 score.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize accuracy metric\n",
    "    accuracy_metric = torchmetrics.Accuracy(\n",
    "        task=\"multiclass\", num_classes=num_classes, average=\"macro\"\n",
    "    ).to(device)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Initialize precision metric  using `torchmetrics.Precision`\n",
    "    precision_metric = torchmetrics.Precision(\n",
    "        task=\"multiclass\", num_classes=num_classes, average=\"macro\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize recall metric using `torchmetrics.Recall`\n",
    "    recall_metric = torchmetrics.Recall(\n",
    "        task=\"multiclass\", num_classes=num_classes, average=\"macro\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize F1 score metric using `torchmetrics.F1Score`\n",
    "    f1_metric = torchmetrics.F1Score(\n",
    "        task=\"multiclass\", num_classes=num_classes, average=\"macro\"\n",
    "    ).to(device)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Disable gradient computation during evaluation\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the validation dataloader\n",
    "        for inputs, labels in val_dataloader:\n",
    "            # Move inputs and labels to the specified device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # Get model predictions\n",
    "            outputs = model(inputs)\n",
    "            # Get the predicted class by finding the index of the maximum logit\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            # Update metrics with the predictions and true labels for the current batch\n",
    "            accuracy_metric.update(predicted, labels)\n",
    "            \n",
    "    ### START CODE HERE ###\n",
    "            \n",
    "            # Update `precision_metric` using `.update` method \n",
    "            precision_metric.update(predicted, labels)\n",
    "            \n",
    "            # Update `recall_metric` using `.update` method\n",
    "            recall_metric.update(predicted, labels)\n",
    "            \n",
    "            # Update `f1_metric` using `.update` method\n",
    "            f1_metric.update(predicted, labels)\n",
    "             \n",
    "    # Compute the final metrics over the entire dataset\n",
    "    accuracy = accuracy_metric.compute().item()\n",
    "    \n",
    "    # Compute precision using `.compute` method and get the value with `.item()`\n",
    "    precision = precision_metric.compute().item()\n",
    "    \n",
    "    # Compute recall using `.compute` method and get the value with `.item()`\n",
    "    recall = recall_metric.compute().item()\n",
    "    \n",
    "    # Compute F1 score using `.compute` method and get the value with `.item()`\n",
    "    f1 = f1_metric.compute().item()\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "```\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4d7cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK YOUR IMPLEMENTATION\n",
    "\n",
    "# model\n",
    "model = SimpleCNN().to(device)\n",
    "\n",
    "# dataloaders\n",
    "train_dataloader, val_dataloader = helper_utils.get_dataset_dataloaders(batch_size=128)\n",
    "\n",
    "accuracy, precision, recall, f1 = evaluate_metrics(model=model, val_dataloader=val_dataloader, device=device)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8d6794",
   "metadata": {},
   "source": [
    "#### Expected Output (approximate values):\n",
    "\n",
    "```\n",
    "Accuracy: 0.1030, Precision: 0.0103, Recall: 0.1000, F1 Score: 0.0187\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c31df7d",
   "metadata": {},
   "source": [
    "### How other metrics change with learning rate\n",
    "\n",
    "*How do these metrics change with different learning rates?*  \n",
    "To explore this, you will train the model with the same five learning rates and evaluate it using accuracy, precision, recall, and F1 score.\n",
    "\n",
    "A new `train_and_evaluate_metrics` function is defined to handle the training and evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a957e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_metrics(learning_rate, device, n_epochs=25, batch_size=128, imbalanced=False):\n",
    "    \"\"\"Trains and evaluates a model, returning a comprehensive set of metrics.\n",
    "\n",
    "    This function orchestrates the end-to-end machine learning pipeline for a\n",
    "    given configuration. It sets a random seed, initializes the model and\n",
    "    related components, loads data, runs the training loop, and evaluates\n",
    "    the model's performance using accuracy, precision, recall, and F1-score.\n",
    "\n",
    "    Args:\n",
    "        learning_rate (float): The learning rate for the optimizer.\n",
    "        device: The device (e.g., 'cuda' or 'cpu') for training and evaluation.\n",
    "        n_epochs (int, optional): The number of training epochs. Defaults to 25.\n",
    "        batch_size (int, optional): The batch size for dataloaders. Defaults to 128.\n",
    "        imbalanced (bool, optional): A flag to use an imbalanced dataset.\n",
    "                                     Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the final validation accuracy, precision,\n",
    "               recall, and F1-score.\n",
    "    \"\"\"\n",
    "    # Set the random seed for reproducibility\n",
    "    helper_utils.set_seed(42)\n",
    "\n",
    "    # Initialize the CNN model and move it to the specified device\n",
    "    model = SimpleCNN().to(device)\n",
    "\n",
    "    # Define the loss function for training\n",
    "    loss_fcn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Define the optimizer with the specified learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Prepare the training and validation dataloaders\n",
    "    train_dataloader, val_dataloader = helper_utils.get_dataset_dataloaders(batch_size=batch_size, imbalanced=imbalanced)\n",
    "\n",
    "    # Call the main training loop to train the model\n",
    "    helper_utils.train_model(model=model, optimizer=optimizer, loss_fcn=loss_fcn, train_dataloader=train_dataloader, device=device, n_epochs=n_epochs) \n",
    "\n",
    "    # Evaluate the trained model to get a full set of performance metrics\n",
    "    accuracy, precision, recall, f1 = evaluate_metrics(model, val_dataloader, device)\n",
    "\n",
    "    # Print the final results for this configuration\n",
    "    print(\n",
    "        f\"Learning Rate: {learning_rate}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\"\n",
    "    )\n",
    "    # Return the computed performance metrics\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4c4cdd",
   "metadata": {},
   "source": [
    "For each learning rate, the model is trained for **25 epochs** and batch size equals **128**.\n",
    "After training, the model is evaluated on the validation set to compute accuracy, precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810be437",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_metrics = []\n",
    "\n",
    "# Loop through different learning rates and collect metrics\n",
    "for lr in learning_rates:\n",
    "\n",
    "    # Train and evaluate the model, collecting metrics, for a given learning rate\n",
    "    n_epochs = 25\n",
    "    batch_size = 128\n",
    "    acc, prec, rec, f1 = train_and_evaluate_metrics(learning_rate=lr, device=device, n_epochs=n_epochs, batch_size=batch_size)\n",
    "\n",
    "    metrics_lr = {\n",
    "        \"learning_rate\": lr,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1_score\": f1,\n",
    "    }\n",
    "\n",
    "    dict_metrics.append(metrics_lr)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame for easier plotting\n",
    "df_metrics = pd.DataFrame(dict_metrics)\n",
    "\n",
    "\n",
    "helper_utils.plot_metrics_vs_learning_rate(df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e285bbe",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "In this case, since the dataset is balanced, you can observe that all metrics (accuracy, precision, recall, F1-score) behave similarly for most learning rates, and the value of $10^{-3}$ achieves the best performance across the board."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f0948b",
   "metadata": {},
   "source": [
    "## (Optional) Further Exploration: Batch Size Optimization on an Imbalanced Dataset\n",
    "\n",
    "In this section, you'll revisit the previous experiments with the following setup:\n",
    "\n",
    "- **Learning Rate:** will be set to $10^{−3}$.\n",
    "- **Imbalanced Dataset:** You'll work with a customized subset of the CIFAR-10 dataset to better observe how metrics are affected by class imbalance.\n",
    "\n",
    "The new variable you'll explore is **Batch Size**. You will vary its values (32, 64, and 128) to see how these influence different performance metrics.\n",
    "\n",
    "**Class Imbalanced CIFAR-10 Dataset**\n",
    "For this experiment, a subset of the CIFAR-10 dataset has been crafted to simulate class imbalance, focusing on three classes: `Cat`, `Dog`, and `Frog`. To mimic a scenario where some classes are more prevalent, the dataset includes all available `Cat` instances, while reducing the number of samples for the other two classes. Specifically, 50% of the original `Dog` images and only 20% of the `Frog` images are retained. As a result, the dataset comprises:\n",
    "\n",
    "- 6,000 `Cat` images\n",
    "- 3,000 `Dog` images\n",
    "- 1,200 `Frog` images\n",
    "This setup creates a pronounced imbalance, spotlighting the `Cat` class and reflecting real-world situations where certain classes are underrepresented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7cd04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will take about 15 minutes.\n",
    "\n",
    "dict_metrics = []\n",
    "\n",
    "# Loop through different batch sizes and collect metrics\n",
    "batch_sizes = [32, 64, 128]\n",
    "\n",
    "# Set a fixed learning rate\n",
    "lr = 0.001  # Medium learning rate\n",
    "\n",
    "# Set imbalance to True to use imbalanced dataset\n",
    "imbalanced = True\n",
    "\n",
    "n_epochs = 25\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    acc, prec, rec, f1 = train_and_evaluate_metrics(batch_size=bs, n_epochs=n_epochs, learning_rate=lr, device=device, imbalanced=imbalanced)\n",
    "\n",
    "    metrics_bs = {\n",
    "        \"batch_size\": bs,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1_score\": f1,\n",
    "    }\n",
    "    dict_metrics.append(metrics_bs)\n",
    "\n",
    "df_metrics = pd.DataFrame(dict_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fc6628",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.plot_metrics_vs_batch_size(df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8602fd5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Observe that the optimal batch size depends significantly on the metric being prioritized. **It's common to find that one batch size may optimize a particular metric, while a different batch size yields better results for another**. In conclusion, *a parameter that is optimal for one metric might not be optimal for another*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffa1fc0",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations on completing this hyperparameter tuning exercise! In this notebook, you explored the impact of varying learning rates on the performance of a CNN trained on the CIFAR-10 dataset. \n",
    "By adjusting the learning rate, you observed changes in validation accuracy, and you gained exposure to alternative evaluation metrics like precision, recall, and F1 score. \n",
    "\n",
    "Furthermore, you explored the effects of batch size on model performance using an imbalanced subset of the CIFAR-10 dataset.\n",
    "\n",
    "This experience has deepened your understanding of the significance of learning rate selection in model training and highlighted the utility of comprehensive metrics for evaluating model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
