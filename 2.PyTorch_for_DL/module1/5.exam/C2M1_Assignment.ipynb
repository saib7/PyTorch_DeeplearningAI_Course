{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a957e5bd",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "Title: FakeFinder: Building an AI to Detect AI-Generated Images"
   },
   "source": [
    " # Programming Assignment - FakeFinder : Building an AI to Detect AI-Generated Images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9620aecd",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "SMALL GENERAL DESCRIPTION"
   },
   "source": [
    "\n",
    "Can Machines Detect Their Own Creations?\n",
    "\n",
    "Welcome, **AI Explorer**! In this assignment, you'll embark on a journey into the dynamic landscape of AutoMLâ€”where you'll discover how artificial intelligence can learn to optimize itself.\n",
    "Your expeditionâ€™s goal? Equip your AI Explorer with the skills to distinguish genuine images from creations generated by state-of-the-art AI models like Stable Diffusion, MidJourney, and DALLÂ·E.\n",
    "\n",
    "\n",
    "In this comprehensive exploration of image authenticity detection, you'll master the following techniques:\n",
    "\n",
    "* Construct **Flexible Convolutional Neural Networks (CNNs)** by using `nn.Sequential` to dynamically create adaptable architectures, based on varying hyperparameters such as the number of layers and filter sizes.\n",
    "* Design a robust **hyperparameter search space** for Optuna to optimize models.\n",
    "* Define and implement an **objective function** that guides the automated hyperparameter tuning process.\n",
    "* Run an Optuna study to discover effective CNN configurations.\n",
    "* Analyze **efficiency metrics** to refine your selection among the top-performing CNN models within the study.\n",
    "\n",
    "\n",
    "You will now chart a course to build a robust and swift FakeFinder!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edeeb16",
   "metadata": {
    "title": "Tips for successful grading"
   },
   "source": [
    "---\n",
    "<a name='submission'></a>\n",
    "\n",
    "<h4 style=\"color:green; font-weight:bold;\">TIPS FOR SUCCESSFUL GRADING OF YOUR ASSIGNMENT:</h4>\n",
    "\n",
    "* All cells are frozen except for the ones where you need to submit your solutions or when explicitly mentioned you can interact with it.\n",
    "\n",
    "* In each exercise cell, look for comments `### START CODE HERE ###` and `### END CODE HERE ###`. These show you where to write the solution code. **Do not add or change any code that is outside these comments**.\n",
    "\n",
    "* You can add new cells to experiment but these will be omitted by the grader, so don't rely on newly created cells to host your solution code, use the provided places for this.\n",
    "\n",
    "* Avoid using global variables unless you absolutely have to. The grader tests your code in an isolated environment without running all cells from the top. As a result, global variables may be unavailable when scoring your submission. Global variables that are meant to be used will be defined in UPPERCASE.\n",
    "\n",
    "* To submit your notebook for grading, first save it by clicking the ðŸ’¾ icon on the top left of the page and then click on the `Submit assignment` button on the top right of the page.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a029f32b-20b3-49aa-98f5-df655759a953",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Imports](#0)\n",
    "- [1 - Preparing the Expedition: Data Inspection](#1)\n",
    "    - [1.1 - Exploring real vs. fake images](#1-1)\n",
    "- [2 - Crafting Your AI Explorer: Building a Flexible CNN](#2)\n",
    "    - **[Exercise 1 - FlexibleCNN](#ex-1)**\n",
    "- [3 - Charting New Territories: Designing the Hyperparameter Search Space](#3)\n",
    "    - **[Exercise 2 - design_search_space](#ex-2)**\n",
    "- [4 - Letting AI Take the Lead: Implementing the Optuna Objective](#4)\n",
    "    - **[Exercise 3 - objective_function](#ex-3)**\n",
    "- [5 - Navigating the Hyperparameter Landscape: Running the Optuna Study](#5)\n",
    "    - [5.1 - Analyzing the Results: Evaluating the Best Models](#5-1)\n",
    "        - **[Exercise 4 - get_trainable_params](#ex-4)**\n",
    "- [6 - (Optional) Evaluating the model with alternative metrics](#6)\n",
    "    - [6.1 - Precision and Recall](#6-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe4f096",
   "metadata": {
    "title": "Imports"
   },
   "source": [
    "<a name='0'></a>\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ed869a6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "graded"
    ],
    "title": "Code cell for imports"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchmetrics.classification import Accuracy, Precision, Recall\n",
    "\n",
    "from helper_utils import evaluate_model, extract_attr, get_data_loaders, training_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb99e4ad",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 0,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import helper_utils\n",
    "import unittests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b89e3810",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4624dff9",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "Section 1: Preparing the Expedition: Data Inspection"
   },
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Preparing the Expedition: Data Inspection\n",
    "\n",
    " You'll be working with a curated collection of images showcasing both real images and synthetic creations from state-of-the-art generative models.\n",
    " The dataset you'll explore is a subset of the [AI-Generated Images vs Real Images](https://www.kaggle.com/datasets/tristanzhang32/ai-generated-images-vs-real-images) dataset.\n",
    "\n",
    " The original dataset contains 60,000 imagesâ€”half generated by advanced AI models (Stable Diffusion, MidJourney, and DALLÂ·E), and half genuine images from sources like Pexels, Unsplash, and WikiArt. \n",
    " To keep your exploration efficient and manageable, you'll work with a carefully selected subset:\n",
    " - **5,000 images** for training.\n",
    " - **1,000 images** for testing.\n",
    "\n",
    "\n",
    " The images are organized into two main folders (`train` and `test`), each containing two subdirectories:\n",
    " - `real`: genuine images.\n",
    " - `fake`: AI-generated synthetic images.\n",
    "\n",
    "\n",
    "Note: Some of the images labeled as `real` are actually drawings rather than photographs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bdc3b6",
   "metadata": {
    "title": "Subsection 1.1: Exploring genuine vs. synthetic images"
   },
   "source": [
    "<a name='1-1'></a>\n",
    "### 1.1 - Exploring real vs. fake images\n",
    "\n",
    "It's time to visually explore the terrain you'll be traversing. \n",
    "Youâ€™ll visualize some random images -from both the training and testing sets- to get familiar with both categories.\n",
    "\n",
    " Here you will iterate through the dataset, displaying a handful of representative images for an initial exploration.\n",
    " The `helper_utils` module provides a function to display random images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3da0a9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 0,
    "tags": [],
    "title": "# Display random images from the dataset"
   },
   "outputs": [],
   "source": [
    "AIvsReal_path = \"./AIvsReal_sampled\"  \n",
    "\n",
    "# Display example images\n",
    "for split in ['train', 'test']:\n",
    "    for category in ['real', 'fake']:\n",
    "        helper_utils.show_random_images(split, category, AIvsReal_path, num_images=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60e374e",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "SECTION 2: Flexible CNN Architecture"
   },
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Crafting Your AI Explorer: Building a Flexible CNN\n",
    "In this section, you'll construct the core of your AI Explorer: a **flexible convolutional neural network (CNN)**. \n",
    "Instead of building a fixed CNN architecture, you'll define a model that is dynamically constructed based on provided parameters, such as the number of layers, filter sizes, and activation functions. \n",
    "\n",
    "This parameterization allows for tailoring the network to specific tasks and datasets, enhancing its effectiveness without the need to redesign it from scratch.\n",
    "Additionally, the scalability of this approach ensures the network can efficiently adapt to varying complexities,\n",
    "from simple to advanced tasks, making it versatile in its applications.\n",
    "\n",
    "This flexibility will be crucial later when you automate architecture exploration using Optuna."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8dcd4f",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "EXERCISE 1 - FlexibleCNN"
   },
   "source": [
    "<a name='ex-1'></a>\n",
    "### Exercise 1 - FlexibleCNN\n",
    "\n",
    "**Your Task:**\n",
    "\n",
    "For the `features` neural network setup (achieved with [`ModuleList`](https://docs.pytorch.org/docs/stable/generated/torch.nn.ModuleList.html)):\n",
    "\n",
    "* **`__init__`**: \n",
    "    * Construct each `convolutional_block` using a `Sequential` container composed of:\n",
    "        * A 2d convolutional layer\n",
    "        * A 2d [batch normalization layer](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html) to normalize the output of the convolutional layer.\n",
    "        * A ReLU activation function\n",
    "        * A 2d max pooling layer\n",
    "    * Ensure each convolutional block connects correctly by properly setting `in_channels` for subsequent layers.\n",
    ">\n",
    " * **`_create_classifier`**: \n",
    "    * Use `nn.Sequential` to build the classifier structure with the following components:\n",
    "        - dropout layer (provided), \n",
    "        - fully connected layer, \n",
    "        - ReLU activation function (provided),\n",
    "        - dropout layer (provided),\n",
    "        - linear output layer.\n",
    ">\n",
    " * **`forward`**: \n",
    "    * Flatten the output after the last layer in the `.features` module to prepare it for the fully connected layer of the classifier, while keeping the batch dimension intact.\n",
    "    * Extract the size of the flattened tensor `x` to determine the input size for the fully connected layer.\n",
    "    * Create the classifier, based on the `._flattened_size` attribute.\n",
    "\n",
    "<details>\n",
    "<summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
    "\n",
    "If you find yourself stuck, here is a more detailed breakdown.\n",
    "\n",
    "**For the `__init__` method:**\n",
    "* The `convolutional_block` is an `nn.Sequential` container that you will define inside the for loop.\n",
    "* The layers inside it should be initialized as follows:\n",
    "    * `nn.Conv2d`: Use the current `in_channels`, the `out_channels` for this layer, the specified `kernel_size`, and the calculated `padding`.\n",
    "    * `nn.BatchNorm2d`: The `num_features` argument for this layer must match the `out_channels` of the preceding convolutional layer.\n",
    "    * `nn.MaxPool2d`: Set the `kernel_size` to `2` and the `stride` to `2`.\n",
    "* To update the channels for the next iteration, use this logic:\n",
    "    * > `in_channels = the out_channels from the current layer`\n",
    "\n",
    "**For the `_create_classifier` method:**\n",
    "* This is another `nn.Sequential` container.\n",
    "* The first `nn.Linear` layer should map from `flattened_size` to `self.fc_size`.\n",
    "* The final `nn.Linear` layer should map from `self.fc_size` to `self.num_classes`.\n",
    "\n",
    "**For the `forward` method:**\n",
    "* To flatten the tensor `x`, you can use pseudocode like this:\n",
    "    * > `x = flatten the tensor x, starting from dimension 1`\n",
    "* To get the size for the classifier, the pseudocode is:\n",
    "    * > `self._flattened_size = get the size of dimension 1 from the tensor x`\n",
    "* To create the classifier, simply call the helper method you already implemented:\n",
    "    * > `call the _create_classifier method, passing it self._flattened_size`\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4723f6f2",
   "metadata": {
    "deletable": false,
    "lines_to_next_cell": 0,
    "tags": [
     "graded"
    ],
    "title": "SOLUTION, EXERCISE 1"
   },
   "outputs": [],
   "source": [
    "# GRADED CLASS: FlexibleCNN\n",
    "\n",
    "class FlexibleCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A customizable convolutional neural network (CNN) for image classification.\n",
    "    It dynamically constructs convolutional blocks based on provided hyperparameters\n",
    "    such as the number of layers, filter sizes, kernel sizes, and dropout rates.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, n_layers, n_filters, kernel_sizes, dropout_rate, fc_size, num_classes=2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the FlexibleCNN.\n",
    "\n",
    "        Args:\n",
    "            n_layers (int): Number of convolutional layers.\n",
    "            n_filters (list): Number of filters for each convolutional layer.\n",
    "            kernel_sizes (list): Kernel sizes for each convolutional layer.\n",
    "            dropout_rate (float): Dropout rate for regularization.\n",
    "            fc_size (int): Number of units in the fully connected layer.\n",
    "            num_classes (int): Number of output classes.\n",
    "        \"\"\"\n",
    "        super(FlexibleCNN, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.features = nn.ModuleList()\n",
    "        in_channels = 3  # RGB input images\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        for i in range(n_layers): \n",
    "            # Create convolutional layer with dynamic parameters\n",
    "            \n",
    "            # Extract the number of filters and kernel size for the current layer, from n_filters and kernel_sizes\n",
    "            out_channels = n_filters[i] \n",
    "            kernel_size = kernel_sizes[i] \n",
    "            \n",
    "            \n",
    "            padding = (kernel_size - 1) // 2 \n",
    "\n",
    "            # Create a convolutional block, by using a `nn.Sequential` container to group layers together\n",
    "            block = nn.Sequential(\n",
    "                # Add a Convolutional layer `Conv2d` with parameters: `in_channels`, `out_channels`, `kernel_size`, and `padding`\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding),\n",
    "                # Add a Batch normalization layer `BatchNorm2d` with `num_features` as `out_channels` \n",
    "                nn.BatchNorm2d(num_features=out_channels),\n",
    "                # Add a ReLU activation\n",
    "                nn.ReLU(), \n",
    "                # Add a MaxPool2d layer `MaxPool2d`, with `kernel_size=2` and `stride=2`\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "                \n",
    "            # Append the convolutional block to the features ModuleList\n",
    "            self.features.append(block)\n",
    "\n",
    "            # Update in_channels for the next layer (the input channels for the next layer is the output channels of the current layer)\n",
    "            in_channels = out_channels\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.fc_size = fc_size        \n",
    "                \n",
    "        # Classifier will be initialized after calculating flattened size\n",
    "        self.classifier = None  \n",
    "        self._flattened_size = None \n",
    "        \n",
    "\n",
    "    def _create_classifier(self, flattened_size):\n",
    "        \"\"\"\n",
    "        Creates the fully connected classifier part of the model based on the flattened feature size.\n",
    "\n",
    "        Args:\n",
    "            flattened_size (int): Size of the flattened feature maps.\n",
    "        \"\"\"\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        # Create the classifier using a Sequential container\n",
    "        self.classifier = nn.Sequential(\n",
    "            # Add a dropout layer with the dropout rate defined at initialization\n",
    "            nn.Dropout(p=self.dropout_rate),\n",
    "            # Add a fully connected layer `Linear` with `in_features=flattened_size` and `out_features` as `fc_size`\n",
    "            nn.Linear(in_features=flattened_size, out_features=self.fc_size),\n",
    "            # Activation function\n",
    "            nn.ReLU(),\n",
    "            # Another dropout layer\n",
    "            nn.Dropout(p=self.dropout_rate),\n",
    "            # Add the final fully connected layer with `in_features` as `fc_size` and `out_features` as `num_classes`\n",
    "            nn.Linear(in_features=self.fc_size, out_features=self.num_classes),\n",
    "        )\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the FlexibleCNN.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor (batch of images).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor (classification scores).\n",
    "        \"\"\"\n",
    "        # Apply convolutional feature extraction layers\n",
    "        for layer in self.features:\n",
    "            x = layer(x)\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        # Flatten the output x for the classifier (start_dim=1 to keep the batch dimension)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "        # Dynamically create classifier if it doesn't exist\n",
    "        if self.classifier is None:\n",
    "            # Get the size of the flattened feature maps from the x tensor\n",
    "            self._flattened_size = x.shape[1]\n",
    "\n",
    "            # Create the classifier with the `_flattened_size` \n",
    "            self._create_classifier(self._flattened_size)\n",
    "\n",
    "            # Extract the device from the input tensor\n",
    "            device = x.device\n",
    "\n",
    "            # Move the classifier to the same device as the input tensor, to ensure compatibility with GPU/CPU\n",
    "            if self.classifier is not None:\n",
    "                self.classifier.to(device)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Classification\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d325ad55",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 0,
    "title": "PRE-CHECK EXERCISE 1"
   },
   "outputs": [],
   "source": [
    "# Create the model with specific parameters\n",
    "n_layers = 3\n",
    "n_filters = [16, 32, 64]\n",
    "kernel_sizes = [3, 3, 3]\n",
    "dropout_rate = 0.5\n",
    "fc_size = 128\n",
    "\n",
    "model = FlexibleCNN(\n",
    "    n_layers=n_layers,\n",
    "    n_filters=n_filters,\n",
    "    kernel_sizes=kernel_sizes,\n",
    "    dropout_rate=dropout_rate,\n",
    "    fc_size=fc_size,\n",
    ").to(DEVICE)\n",
    "\n",
    "resolution = 32\n",
    "x_sample = torch.randn(1, 3, resolution, resolution).to(DEVICE)  # Example input tensor\n",
    "\n",
    "\n",
    "# Forward pass through the model\n",
    "output = model(x_sample)\n",
    "\n",
    "# print the model features architecture\n",
    "print(f\"FlexibleCNN features architecture:\\n{model.features}\")\n",
    "\n",
    "print(f\"FlexibleCNN classifier architecture:\\n{model.classifier}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ad40e6",
   "metadata": {
    "title": "EXPECTED OUTPUT, EXERCISE 1"
   },
   "source": [
    " #### Expected Output:\n",
    "\n",
    " ```\n",
    "FlexibleCNN features architecture:\n",
    "ModuleList(\n",
    "  (0): Sequential(\n",
    "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (2): ReLU()\n",
    "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  )\n",
    "  (1): Sequential(\n",
    "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (2): ReLU()\n",
    "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  )\n",
    "  (2): Sequential(\n",
    "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (2): ReLU()\n",
    "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  )\n",
    ")\n",
    "FlexibleCNN classifier architecture:\n",
    "Sequential(\n",
    "  (0): Dropout(p=0.5, inplace=False)\n",
    "  (1): Linear(in_features=1024, out_features=128, bias=True)\n",
    "  (2): ReLU()\n",
    "  (3): Dropout(p=0.5, inplace=False)\n",
    "  (4): Linear(in_features=128, out_features=2, bias=True)\n",
    ")\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec962b3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 0,
    "title": "TEST EXERCISE 1"
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "unittests.exercise_1(FlexibleCNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77749d2",
   "metadata": {
    "title": "GENERAL DESCRIPTION AND EXERCISE 2, HYPERPARAMETER SEARCH"
   },
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Charting New Territories: Designing the Hyperparameter Search Space\n",
    "\n",
    "With your flexible CNN architecture ready, the next step of your expedition involves charting the hyperparameter landscape.\n",
    "You'll create a detailed search space using **Optuna**, enabling automatic exploration of architectural and training hyperparameters.  \n",
    "Carefully defining this search space allows your AI Explorer to efficiently navigate different configurations for identifying optimal models.\n",
    "\n",
    "<a name='ex-2'></a>\n",
    "### Exercise 2 - design_search_space\n",
    "\n",
    "**Your Task:**\n",
    "\n",
    "Implement a structured search space for Optuna that selects values for both architectural and training hyperparameters.\n",
    "\n",
    "* Use the `trial.suggest_int`, `trial.suggest_float`, and `trial.suggest_categorical` methods to define these parameters (for a summary of the available distributions see [Optuna Trial](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html)).\n",
    "* Make sure that the right types (`int`, `float`, `categorical`) are used for each hyperparameter, and ensure that the limits are the proposed ones within the code. \n",
    "* **When using `trial.suggest_` methods, always provide a name for the hyperparameter. The names used match exactly those defined in the code to maintain consistency.**\n",
    "\n",
    "Your search space should clearly define ranges or options for:\n",
    "\n",
    "* **`CNN Architecture`**: \n",
    ">    * Number of convolutional layers. (`n_layers`)\n",
    ">    * Number of filters for each convolutional layer. (`n_filters_layer{i}`)\n",
    ">    * Kernel sizes. (`kernel_size_layer{i}`)\n",
    ">    * Dropout rates. (`dropout_rate`)\n",
    ">    * Number of units in the fully connected (dense) layer. (`fc_size`)\n",
    "\n",
    "* **`Training Parameters`**: \n",
    ">    * Learning rate. (`learning_rate`)\n",
    ">    * Image resolution. (`resolution`)\n",
    ">    * Batch size. (`batch_size`)\n",
    "\n",
    "<details>\n",
    "<summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
    "\n",
    "If you need some help, here is a more detailed guide for each hyperparameter.\n",
    "\n",
    "**CNN Architecture Hyperparameters:**\n",
    "\n",
    "* **n_layers**:\n",
    "    * > `n_layers = use the trial object to suggest an integer named \"n_layers\" between 1 and 3`\n",
    "* **n_filters**:\n",
    "    * This is a list comprehension. For each layer `i`, you need to:\n",
    "    * > `suggest an integer named f\"n_filters_layer{i}\" between 8 and 64, with a step of 8`\n",
    "* **kernel_sizes**:\n",
    "    * Similar to filters, this is a list comprehension. For each layer `i`:\n",
    "    * > `suggest an integer named f\"kernel_size_layer{i}\" between 3 and 5, with a step of 2`\n",
    "* **dropout_rate**:\n",
    "    * > `dropout_rate = use the trial object to suggest a float named \"dropout_rate\" between 0.1 and 0.5`\n",
    "* **fc_size**:\n",
    "    * > `fc_size = use the trial object to suggest an integer named \"fc_size\" between 64 and 512, with a step of 64`\n",
    "\n",
    "**Training Hyperparameters:**\n",
    "\n",
    "* **learning_rate**:\n",
    "    * This requires a logarithmic scale.\n",
    "    * > `learning_rate = use the trial object to suggest a float named \"learning_rate\" between 1e-4 and 1e-2, and set the log parameter to True`\n",
    "* **resolution**:\n",
    "    * This is a choice from a predefined list.\n",
    "    * > `resolution = use the trial object to suggest a categorical value named \"resolution\" from the list [16, 32, 64]`\n",
    "* **batch_size**:\n",
    "    * This is also a choice from a list.\n",
    "    * > `batch_size = use the trial object to suggest a categorical value named \"batch_size\" from the list [8, 16]`\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74fed70",
   "metadata": {
    "deletable": false,
    "lines_to_next_cell": 0,
    "tags": [
     "graded"
    ],
    "title": "SOLUTION, EXERCISE 2: design_search_space"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: design_search_space\n",
    "\n",
    "def design_search_space(trial):\n",
    "    \"\"\"\n",
    "    Design the search space for hyperparameter optimization of the FlexibleCNN model.\n",
    "    This function uses Optuna to suggest hyperparameters for the CNN architecture and training process.\n",
    "    Args:\n",
    "        trial (optuna.Trial): An Optuna trial object used to suggest hyperparameters.\n",
    "    Returns:\n",
    "        dict: A dictionary containing the suggested hyperparameters.    \n",
    "    \"\"\"\n",
    "    # CNN Architecture Hyperparameters\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Use trial.suggest_* to set n_layers. Name it \"n_layers\", and set it to be an integer between 1 and 3.\n",
    "    n_layers =  trial.suggest_int(\"n_layers\", 1, 3)\n",
    "       \n",
    "    # Use trial.suggest_* to set each filter size in n_filters.\n",
    "    # Name each filter size as \"n_filters_layer{i}\" where i is the layer index and set it to be an integer between 8 and 64 with step 8.\n",
    "    n_filters = [ \n",
    "        trial.suggest_int(f\"n_filters_layer{i}\", 8, 64, step=8) for i in range(n_layers)\n",
    "    ]\n",
    "    \n",
    "    # Use trial.suggest_* to set each kernel size in kernel_sizes.\n",
    "    # Name each kernel size as \"kernel_size_layer{i}\" where i is the layer index and set it to be an integer between 3 and 5 with step 2.\n",
    "    kernel_sizes = [ \n",
    "        trial.suggest_int(f\"kernel_size_layer{i}\", 3, 5, step=2) for i in range(n_layers)\n",
    "    ] \n",
    "\n",
    "    # Use trial.suggest_* to set dropout_rate, name it \"dropout_rate\", and set it to be a float between 0.1 and 0.5.\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "    \n",
    "    # Use trial.suggest_* to set fc_size, name it \"fc_size\", and set it to be an integer between 64 and 512 with step 64.\n",
    "    fc_size = trial.suggest_int(\"fc_size\", 64, 512, step=64)\n",
    "\n",
    "    \n",
    "    # Training Hyperparameters\n",
    "\n",
    "    # Use trial.suggest_* to set learning_rate, name it \"learning_rate\", and set it to be a float between 1e-4 and 1e-2 with logarithmic scale (log=True).\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "\n",
    "    # Use trial.suggest_* to set resolution, name it \"resolution\", and set it to be one of [16, 32, 64].\n",
    "    resolution = trial.suggest_categorical(\"resolution\", [16, 32, 64])\n",
    "    \n",
    "    # Use trial.suggest_* to set batch_size, name it \"batch_size\", and set it to be one of [8, 16].\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16])\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return {\n",
    "        \"n_layers\": n_layers,\n",
    "        \"n_filters\": n_filters,\n",
    "        \"kernel_sizes\": kernel_sizes,\n",
    "        \"dropout_rate\": dropout_rate,\n",
    "        \"fc_size\": fc_size,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"resolution\": resolution,\n",
    "        \"batch_size\": batch_size,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2d27b6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 0,
    "title": "PRE-CHECK EXERCISE 2"
   },
   "outputs": [],
   "source": [
    "# To verify that the design_search_space function works correctly, you can run it with a fixed trial.\n",
    "\n",
    "# Create a fixed trial with specific hyperparameters\n",
    "fixed_params = {\n",
    "    \"n_layers\": 2,\n",
    "    \"n_filters_layer0\": 16,\n",
    "    \"n_filters_layer1\": 32,\n",
    "    \"kernel_size_layer0\": 3,\n",
    "    \"kernel_size_layer1\": 5,\n",
    "    \"dropout_rate\": 0.001,\n",
    "    \"fc_size\": 128,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"resolution\": 32,\n",
    "    \"batch_size\": 16,\n",
    "}\n",
    "toy_trial = optuna.trial.FixedTrial(fixed_params)\n",
    "\n",
    "# Display the design search space for the fixed trial\n",
    "pprint(design_search_space(toy_trial))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc28b37b",
   "metadata": {
    "title": "EXPECTED OUTPUT, EXERCISE 2"
   },
   "source": [
    "#### Expected Output:\n",
    "\n",
    "```\n",
    "{'batch_size': 16,\n",
    " 'dropout_rate': 0.001,\n",
    " 'fc_size': 128,\n",
    " 'kernel_sizes': [3, 5],\n",
    " 'learning_rate': 0.001,\n",
    " 'n_filters': [16, 32],\n",
    " 'n_layers': 2,\n",
    " 'resolution': 32}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6bcf64",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [],
    "title": "TEST EXERCISE 2"
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "unittests.exercise_2(design_search_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fd7dde",
   "metadata": {
    "title": "SECTION 4: Implementing the Optuna Objective Function"
   },
   "source": [
    "<a name='4'></a> \n",
    "## 4 - Letting AI Take the Lead: Implementing the Optuna Objective\n",
    "\n",
    "You've charted a flexible model architecture and outlined a search spaceâ€”now it's time to put it all together. \n",
    "In this section, you'll implement the **objective function** that powers the AutoML process.\n",
    "The objective function in Optuna encapsulates the necessary steps to evaluate the model's performance, such as accuracy, for each sampled set of hyperparameters, thus guiding the optimization process.\n",
    "\n",
    "In the current context, each trial consists of training the Flexible CNN model with a specific set of hyperparameters and evaluating its performance on a validation dataset.\n",
    "`get_data_loaders`, `train_model`, and `evaluate_model` are helper functions to facilitate the training and evaluation process, including data loading, model training, and validation accuracy calculation.\n",
    "Those are wrappers around concepts you have already used in the previous assignments, so you can focus on the objective function itself.\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### Exercise 3 - objective_function\n",
    "\n",
    "As mentioned, our objective function should execute a complete training loop, including validation. \n",
    "Between other steps, it should: define parameters to load the data, instantiate the model, and subsequently train and evaluate the model.\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "Implement the `objective_function` that Optuna will use to assess the performance of various hyperparameter configurations.\n",
    "\n",
    "* Define `params` using the `design_search_space` function you implemented earlier.\n",
    "* Complete the `transform` to resize the images to the chosen resolution.\n",
    "* Instantiate the `FlexibleCNN` model with the sampled hyperparameters.\n",
    "\n",
    "<details>\n",
    "<summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
    "\n",
    "If you are stuck, this detailed guide should help you.\n",
    "\n",
    "* **Getting the parameters**: This is a single function call.\n",
    "    * > `params = call the design_search_space function, passing it the trial object`\n",
    "\n",
    "* **Defining the transform**: You only need to complete the `transforms.Resize` part. The resolution value is stored in your `params` dictionary.\n",
    "    * > `transforms.Resize((get the \"resolution\" from params, get the \"resolution\" from params))`\n",
    "\n",
    "* **Instantiating the model**: You must pass the values from your `params` dictionary as arguments to the `FlexibleCNN` constructor. Follow the pattern shown below for all the required parameters.\n",
    "    * The first argument is `n_layers=params[\"n_layers\"]`.\n",
    "    * Do the same for `n_filters`, `kernel_sizes`, `dropout_rate`, and `fc_size`.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ade66ab",
   "metadata": {
    "deletable": false,
    "lines_to_next_cell": 0,
    "tags": [
     "graded"
    ],
    "title": "SOLUTION, EXERCISE 3"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: objective_function\n",
    "\n",
    "def objective_function(trial, device, dataset_path, n_epochs=4, silent=False, test=False):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna to optimize the hyperparameters of the FlexibleCNN model.\n",
    "    Args:\n",
    "        trial (optuna.Trial): An Optuna trial object used to suggest hyperparameters.\n",
    "        n_epochs (int): Number of epochs for training the model.\n",
    "        silent (bool): If True, suppresses output during training and evaluation.\n",
    "        test (bool): If True, extracts attributes from the trial for evaluation purposes.\n",
    "    Returns:\n",
    "        float: The accuracy of the model on the validation set.\n",
    "    \"\"\"\n",
    "\n",
    "    # === construction of model, dataloaders ===\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # use design_search_space to get the parameters for the trial\n",
    "    params = design_search_space(trial)\n",
    "\n",
    "    # add the transform to resize the images to the specified resolution in params\n",
    "    transform = transforms.Compose([\n",
    "            transforms.Resize((params[\"resolution\"], params[\"resolution\"])),\n",
    "            transforms.ToTensor(), \n",
    "        ])\n",
    "\n",
    "    # define the model using the FlexibleCNN class with the parameters from the trial\n",
    "    model = FlexibleCNN(\n",
    "        n_layers = params[\"n_layers\"],\n",
    "        n_filters = params[\"n_filters\"],\n",
    "        kernel_sizes = params[\"kernel_sizes\"],\n",
    "        dropout_rate = params[\"dropout_rate\"],\n",
    "        fc_size = params[\"fc_size\"],\n",
    "    ) \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Get data loaders\n",
    "    train_loader, val_loader = get_data_loaders(transform, params[\"batch_size\"], dataset_path)\n",
    "    \n",
    "    # === Optimizer and Loss Function ===\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "    loss_fcn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # === Training the model ===\n",
    "\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # = Training =\n",
    "    for epoch in range(n_epochs):\n",
    "        _ = training_epoch(\n",
    "            model,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            loss_fcn,\n",
    "            device,\n",
    "            epoch,\n",
    "            n_epochs,\n",
    "            silent=silent,\n",
    "        )\n",
    "\n",
    "    # === Evaluation ===\n",
    "\n",
    "    accuracy = evaluate_model(model, val_loader, device, silent=silent)\n",
    "\n",
    "    # NOTE: the following line is only for evaluation purposes\n",
    "    if test:\n",
    "        extract_attr(trial, transform, model, params) \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70dbdf8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "title": "# PRE-CHECK EXERCISE 3"
   },
   "outputs": [],
   "source": [
    "# It takes about 1 minute to train for 1 epoch\n",
    "# Run the objective function with a fixed trial \n",
    "\n",
    "fixed_trial = optuna.trial.FixedTrial({\n",
    "    \"n_layers\": 2,\n",
    "    \"n_filters_layer0\": 16,\n",
    "    \"n_filters_layer1\": 32,\n",
    "    \"kernel_size_layer0\": 3,\n",
    "    \"kernel_size_layer1\": 3,\n",
    "    \"dropout_rate\": 0.3,\n",
    "    \"fc_size\": 128,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"resolution\": 32,\n",
    "    \"batch_size\": 16,\n",
    "})\n",
    "\n",
    "#_ = helper_utils.run_silent_function(fixed_trial, objective_function)\n",
    "objective_function(trial=fixed_trial, device=DEVICE, n_epochs=1, dataset_path=AIvsReal_path, silent=False, test=True) \n",
    "\n",
    "print('\\n Some objects from the trial: \\n')\n",
    "\n",
    "print('transform:', fixed_trial.user_attrs['transform'])\n",
    "print('\\n model:', fixed_trial.user_attrs['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63184e7",
   "metadata": {
    "title": "EXPECTED OUTPUT, EXERCISE 3"
   },
   "source": [
    "#### Expected Output (Approximately):\n",
    "\n",
    "```\n",
    "Epoch [1/1], Step [45/250], Loss: 0.7122\n",
    "Epoch [1/1], Step [90/250], Loss: 0.7140\n",
    "Epoch [1/1], Step [135/250], Loss: 0.6386\n",
    "Epoch [1/1], Step [180/250], Loss: 0.5746\n",
    "Epoch [1/1], Step [225/250], Loss: 0.5587\n",
    "Validation Accuracy: 64.60%\n",
    "\n",
    " Some objects from the trial: \n",
    "\n",
    "transform: Compose(\n",
    "    Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)\n",
    "    ToTensor()\n",
    ")\n",
    "\n",
    " model: FlexibleCNN(\n",
    "  (features): ModuleList(\n",
    "    (0): Sequential(\n",
    "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (2): ReLU()\n",
    "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    )\n",
    "    (1): Sequential(\n",
    "      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (2): ReLU()\n",
    "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    )\n",
    "  )\n",
    "  (classifier): Sequential(\n",
    "    (0): Dropout(p=0.3, inplace=False)\n",
    "    (1): Linear(in_features=2048, out_features=128, bias=True)\n",
    "    (2): ReLU()\n",
    "    (3): Dropout(p=0.3, inplace=False)\n",
    "    (4): Linear(in_features=128, out_features=2, bias=True)\n",
    "  )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e162b2d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 0,
    "title": "# TEST EXERCISE 3"
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "# Note: It takes approximately 1 minute to run\n",
    "unittests.exercise_3(objective_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc72371e",
   "metadata": {
    "title": "Section 5: Navigating the Hyperparameter Landscape"
   },
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Navigating the Hyperparameter Landscape: Running the Optuna Study\n",
    "\n",
    "With the objective function ready, it's time for you to embark on the journey of hyperparameter optimization using Optuna. \n",
    "In this section, you'll load an Optuna study to continue the hyperparameter search process.\n",
    "\n",
    "**Note**: Running the study can be time-consuming, so a pre-existing study is available to expedite this process. \n",
    "You can still add additional trials to further explore and refine the hyperparameter space.\n",
    "*If you wish to create a new study from scratch, you can use `optuna.create_study` instead of `optuna.load_study`.* \n",
    "However, **be aware that doing so may affect the expected functionality of the rest of the notebook, as it relies on the existing study setup.**\n",
    "\n",
    "In the following code, you will load the Optuna study and run additional trials to broaden your exploration of the hyperparameter landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313bb83f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "storage = \"sqlite:///example.db\"\n",
    "study_name = \"AIvsReal_optimization\"\n",
    "\n",
    "# Load the study\n",
    "study = optuna.load_study(study_name=study_name, storage=storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bf0b7d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# continue with the study, with 2 more trials (about 9 minutes to run)\n",
    "n_epochs = 3\n",
    "study.optimize(lambda trial: objective_function(trial, n_epochs=n_epochs, device=DEVICE, dataset_path=AIvsReal_path), n_trials=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5837c9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<a name='5-1'></a>\n",
    "### 5.1 - Analyzing the Results: Evaluating the Best Models\n",
    "\n",
    "After running an Optuna study, you'll obtain a collection of trials, each containing different hyperparameter configurations and their corresponding performance metrics.\n",
    "You will now assess these results and pinpoint the top-performing models based on their validation accuracy.\n",
    "\n",
    "To achieve this, you will use the `trials_dataframe()` method from Optuna, which converts the results of all trials into a Pandas DataFrame. \n",
    "The DataFrame is sorted by the \"value\" column, which represents validation accuracy, to identify the `k` top-performing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8bcd3b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# check the k best trials\n",
    "df_trials = study.trials_dataframe()\n",
    "\n",
    "# Sort the trials by value (accuracy) in descending order\n",
    "df_trials.sort_values(by=\"value\", ascending=False, inplace=True)\n",
    "\n",
    "df_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2dd3a7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "k = 5\n",
    "# Get the top k trials\n",
    "best_k_trials = df_trials.head(k)\n",
    "best_k_trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b708a6ad",
   "metadata": {},
   "source": [
    "<a name='ex-4'></a>\n",
    "### Exercise 4 - get_trainable_params\n",
    "\n",
    "Now that you have selected the top-performing models based on accuracy, the next step is to analyze their efficiency. \n",
    "One possible metric for this evaluation is the number of trainable parameters in each model. \n",
    "While you could consider other metrics, such as inference time, this analysis will focus on trainable parameters count for simplicity.\n",
    "\n",
    "**Your task**:\n",
    "\n",
    "Complete the implementation of `the get_trainable_params` function, which computes the total number of trainable parameters in a given model.\n",
    "- Retrieve all the model parameters.\n",
    "- Iterate through the model parameters to check if each parameter requires gradients (therefore is trainable).\n",
    "- For each trainable parameter, add the number of elements in the parameter to `total_trainable_params`.\n",
    "\n",
    "<details>\n",
    "<summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
    "\n",
    "If you are looking for more specific guidance, here is the step by step process.\n",
    "\n",
    "* **Get model parameters**: You can get all parameters of a model by calling its `.parameters()` method.\n",
    "    * > `model_parameters = call the parameters method on the model`\n",
    "* **Iterate through parameters**: A standard `for` loop will work perfectly here.\n",
    "* **Check for trainability**: Inside the loop, you can check if a parameter is trainable by accessing its `.requires_grad` attribute. This is a boolean property.\n",
    "    * > `if the parameter's requires_grad attribute is True:`\n",
    "* **Count elements**: For each trainable parameter, you can get its total number of elements by calling its `.numel()` method. You then add this to your running total.\n",
    "    * > `total_trainable_params += call the numel() method on the parameter`\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefbc85a",
   "metadata": {
    "deletable": false,
    "lines_to_next_cell": 0,
    "tags": [
     "graded"
    ],
    "title": "SOLUTION, EXERCISE 4: get_trainable_params"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: get_trainable_params\n",
    "\n",
    "def get_trainable_params(model):\n",
    "    \"\"\"\n",
    "    Calculate the total number of trainable parameters in the model.\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model.\n",
    "    Returns:\n",
    "        int: Total number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    total_trainable_params = 0\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Get the model parameters\n",
    "    model_parameters = model.parameters()\n",
    "    # Iterate through the model parameters\n",
    "    for param in model_parameters:\n",
    "        # check if the parameter requires gradient\n",
    "        if param.requires_grad:\n",
    "            # Add the number of elements in the parameter to the total\n",
    "            total_trainable_params += param.numel()\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return total_trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e125f8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "n_layers = 3\n",
    "n_filters = [16, 32, 64]\n",
    "kernel_sizes = [3, 3, 3]\n",
    "dropout_rate = 0.5\n",
    "fc_size = 128\n",
    "\n",
    "model = FlexibleCNN(\n",
    "    n_layers=n_layers,\n",
    "    n_filters=n_filters,\n",
    "    kernel_sizes=kernel_sizes,\n",
    "    dropout_rate=dropout_rate,\n",
    "    fc_size=fc_size,\n",
    ")\n",
    "\n",
    "print(\"Total trainable parameters:\", get_trainable_params(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0035c6",
   "metadata": {
    "title": "# EXPECTED OUTPUT, EXERCISE 4"
   },
   "source": [
    "#### Expected Output:\n",
    "```\n",
    "Total trainable parameters: 23808\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb41dbf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Test your code! \n",
    "unittests.exercise_4(get_trainable_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9242d7-4841-4805-8720-64be2d94bd46",
   "metadata": {},
   "source": [
    "---\n",
    "# Submission Note\n",
    "\n",
    "Congratulations! You've completed the final graded exercise of this assignment.\n",
    "\n",
    "If you've successfully passed all the unit tests above, you've completed the core requirements of this assignment. Feel free to [submit](#submission) your work now. The grading process runs in the background, so it will not disrupt your progress and you can continue on with the rest of the material.\n",
    "\n",
    "**ðŸš¨ IMPORTANT NOTE** If you have passed all tests within the notebook, but the autograder shows a system error after you submit your work:\n",
    "\n",
    "<div style=\"background-color: #1C1C1E; border: 1px solid #444444; color: #FFFFFF; padding: 15px; border-radius: 5px;\">\n",
    "    <p><strong>Grader Error: Grader feedback not found</strong></p>\n",
    "    <p>Autograder failed to produce the feedback...</p>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "This is typically a temporary system glitch. The most common solution is to resubmit your assignment, as this often resolves the problem. Occasionally, it may be necessary to resubmit more than once. \n",
    ">\n",
    "If the error persists, please reach out for support in the [DeepLearning.AI Community Forum](https://community.deeplearning.ai/c/course-q-a/pytorch-for-developers/pytorch-techniques-and-ecosystem-tools/561).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52832318",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "You can now use the `get_trainable_params` function to add the number of trainable parameters to the DataFrame containing the results of the Optuna study. The following function, `add_efficiency_metrics`, will add a new column to the DataFrame, displaying the number of trainable parameters for each model.   \n",
    "Finally, a plot will be generated to visualize the relationship between validation accuracy and the number of trainable parameters for the top-performing models. \n",
    "*Notice that you may find models with similar validation accuracy but significantly different numbers of trainable parameters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4203cb9e-3bbe-4710-9fff-59e86508b904",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_efficiency_metrics(study, best_k_trials):\n",
    "    \"\"\"\n",
    "    Calculates efficiency metrics for the top K trials from a study.\n",
    "\n",
    "    Args:\n",
    "        study: The Optuna study object containing all trial information.\n",
    "        best_k_trials: A DataFrame or similar object containing the best K trials.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the calculated efficiency metrics for each trial.\n",
    "    \"\"\"\n",
    "    # Get the indices of the best k trials\n",
    "    idx_trials = best_k_trials.index.tolist()\n",
    "\n",
    "    # Initialize a dictionary to store the results\n",
    "    results = {}\n",
    "\n",
    "    # Iterate over the indices of the trials\n",
    "    for i in idx_trials:\n",
    "        # Get the model for the corresponding trial\n",
    "        trial = study.get_trials(deepcopy=True)[i]\n",
    "        # Get the parameters of the model\n",
    "        params_model = trial.params\n",
    "\n",
    "        # Extract the corresponding parameters from the trial\n",
    "        n_layers = params_model[\"n_layers\"]\n",
    "        # Extract the number of filters for each layer\n",
    "        n_filters = [params_model[f\"n_filters_layer{i}\"] for i in range(n_layers)]\n",
    "        # Extract the kernel sizes for each layer\n",
    "        kernel_sizes = [params_model[f\"kernel_size_layer{i}\"] for i in range(n_layers)]\n",
    "        # Extract the dropout rate\n",
    "        dropout_rate = params_model[\"dropout_rate\"]\n",
    "        # Extract the size of the fully connected layer\n",
    "        fc_size = params_model[\"fc_size\"]\n",
    "\n",
    "        # Create a new model instance with the extracted parameters\n",
    "        model_trial =  FlexibleCNN(\n",
    "            n_layers=n_layers,\n",
    "            n_filters=n_filters,\n",
    "            kernel_sizes=kernel_sizes,\n",
    "            dropout_rate=dropout_rate,\n",
    "            fc_size=fc_size,\n",
    "            num_classes=2\n",
    "        )\n",
    "\n",
    "        # Get the efficiency metrics for the model\n",
    "        total_trainable_params = get_trainable_params(model_trial)\n",
    "\n",
    "        # Get the accuracy of the model from the trial's value\n",
    "        accuracy = trial.value\n",
    "\n",
    "        # Store the trial's metrics in the results dictionary\n",
    "        results[i] = {\n",
    "            'trial': i,\n",
    "            \"model_size\": total_trainable_params,\n",
    "            \"accuracy\": accuracy,\n",
    "        }\n",
    "\n",
    "    # Return the dictionary of results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7d1701",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_model_metrics(results_df):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        results_df: A pandas DataFrame containing the results, with columns for 'accuracy',\n",
    "                    'model_size', and a unique identifier for each trial.\n",
    "    \"\"\"\n",
    "    # Define the column to be used for labeling points\n",
    "    label_column = 'trial'\n",
    "    # Create a new figure and axes for the plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Get the unique identifiers for each data point\n",
    "    unique_labels = results_df[label_column].unique()\n",
    "    \n",
    "    # Get a color palette from matplotlib\n",
    "    base_colors = plt.get_cmap(\"tab10\").colors\n",
    "    # Create a list of colors, cycling through the palette if needed\n",
    "    colors = [base_colors[i % len(base_colors)] for i in range(len(unique_labels))]\n",
    "    # Map each unique label to a specific color\n",
    "    label_color_map = dict(zip(unique_labels, colors))\n",
    "\n",
    "    # Iterate through the DataFrame to plot each data point\n",
    "    for _, row in results_df.iterrows():\n",
    "        # Plot a single point with its accuracy and model size\n",
    "        ax.scatter(\n",
    "            row[\"accuracy\"],\n",
    "            row[\"model_size\"],\n",
    "            color=label_color_map[row[label_column]],\n",
    "            label=row[label_column]\n",
    "        )\n",
    "\n",
    "    # Set the title of the plot\n",
    "    ax.set_title(\"Accuracy vs Model Size\")\n",
    "    # Set the label for the y-axis\n",
    "    ax.set_ylabel(\"Model Size\")\n",
    "    # Set the label for the x-axis\n",
    "    ax.set_xlabel(\"Accuracy\")\n",
    "\n",
    "    # Add a legend to the plot with a single entry for each label\n",
    "    handles = []\n",
    "    # Create a set to track labels that have been added to the legend\n",
    "    added_labels = set()\n",
    "    # Iterate through the unique labels and their colors to create legend handles\n",
    "    for label, color in label_color_map.items():\n",
    "        # Create a proxy artist for the legend entry\n",
    "        handles.append(plt.Line2D([0], [0], marker='o', color='w',\n",
    "                                  markerfacecolor=color, label=label))\n",
    "    # Display the legend on the axes\n",
    "    ax.legend(handles=handles, title=label_column, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    # Adjust plot parameters for a tight layout\n",
    "    plt.tight_layout()\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c42db75",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "results = add_efficiency_metrics(study, best_k_trials)\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df.astype({'trial': 'int64', 'model_size': 'int64'})\n",
    "\n",
    "plot_model_metrics(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a06c39",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - (Optional) Evaluating the model with alternative metrics\n",
    "\n",
    "You are now a seller of authentic digital art and are using the AI Explorer system to assist in reselling available artwork, which includes both real and fake pieces. Your aim is to ensure that the art you resell is authentic by accurately classifying each piece as real (labeled as 1) or fake (labeled as 0).\n",
    "\n",
    "In this setting, you face two significant risks:\n",
    "\n",
    "- **Reputation Issue**: Selling fake art as real, which eventually leads to a loss of reputation or discourages people from buying your art. This can be described as a False Positive (FP) error. Here, the model incorrectly classifies fake art as real, labeling it as 1 when it should be 0.\n",
    "- **Profit Issue**: Not selling real art because you classify it as fake, which leads to a loss of potential profit. This corresponds to a False Negative (FN) error, where real art is incorrectly labeled as fake, assigning it a 0 instead of the correct label 1.\n",
    "\n",
    "To evaluate the performance of your system in terms of both these issues, you can employ metrics like Recall and Precision. \n",
    "\n",
    "- **Recall** helps ensure real art is correctly identified, minimizing FN errors and increasing the number of available pieces to sell. It is the percentage of actual real art pieces that are correctly classified. Recall values range between 0 and 1, with values closer to 1 indicating fewer FN errors, meaning more real art is correctly identified and available for sale.\n",
    "\n",
    "- **Precision** focuses on correctly identifying fake pieces, reducing FP errors and protecting your reputation. It is the percentage of identified real art pieces that are actually real. Precision also ranges between 0 and 1, with values closer to 1 indicating fewer FP errors, thereby enhancing your credibility by ensuring fake art is not being sold as real.\n",
    "\n",
    "By analyzing your system with these metrics in mind, you can balance maintaining your credibility and maximizing your sales potential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9d5249",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "Description of extra things of optuna and torchmetrics to be used"
   },
   "source": [
    "<a name='6-1'></a>\n",
    "### 6.1 - Precision and Recall\n",
    "\n",
    "In this section, you will explore how to calculate additional metrics such as Precision and Recall for the best k trials from the Optuna study.\n",
    "This will help you understand the trade-offs between different metrics and how they can be used to evaluate the performance of your models in a more nuanced way.\n",
    "\n",
    "You will use the `torchmetrics` library to calculate these metrics, which provides a simple and efficient way to compute various evaluation metrics for PyTorch models.\n",
    "In this final optional section, you will retrain the best k models from the Optuna study and evaluate them using Accuracy, Precision and Recall metrics. The function `train_with_alternative_metrics` contains the logic to extract the model parameters from the Optuna trial, create a new model with those parameters, and then train and evaluate the model using the specified metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54776201",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 0,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_with_alternative_metrics(study, best_k_trials, device, dataset_path):\n",
    "    \"\"\"\n",
    "    Trains a selection of models and evaluates them using multiple metrics.\n",
    "\n",
    "    Args:\n",
    "        study: The Optuna study object.\n",
    "        best_k_trials: A DataFrame of the best K trials to be re-evaluated.\n",
    "        device: The device (e.g., 'cpu' or 'cuda') to perform training and evaluation on.\n",
    "        dataset_path: The file path to the dataset.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the evaluation metrics for each trial.\n",
    "    \"\"\"\n",
    "    # Extract the indices of the best k trials\n",
    "    idx_trials = best_k_trials.index.tolist()\n",
    "    # Initialize a dictionary to store the results\n",
    "    results = {}\n",
    "\n",
    "    # Iterate over the indices of the best trials\n",
    "    for i in idx_trials:\n",
    "        # Get the trial object from the study\n",
    "        trial = study.get_trials(deepcopy=True)[i]\n",
    "        # Extract the model parameters from the trial\n",
    "        params_model = trial.params\n",
    "\n",
    "        # Extract model parameters from the trial's parameters\n",
    "        n_layers = params_model[\"n_layers\"]\n",
    "        # Create a list of the number of filters for each convolutional layer\n",
    "        n_filters = [params_model[f\"n_filters_layer{j}\"] for j in range(n_layers)]\n",
    "        # Create a list of the kernel sizes for each convolutional layer\n",
    "        kernel_sizes = [params_model[f\"kernel_size_layer{j}\"] for j in range(n_layers)]\n",
    "        # Extract the dropout rate\n",
    "        dropout_rate = params_model[\"dropout_rate\"]\n",
    "        # Extract the size of the fully connected layer\n",
    "        fc_size = params_model[\"fc_size\"]\n",
    "\n",
    "        # Instantiate a new model with the extracted parameters\n",
    "        model_trial = FlexibleCNN(\n",
    "            n_layers=n_layers,\n",
    "            n_filters=n_filters,\n",
    "            kernel_sizes=kernel_sizes,\n",
    "            dropout_rate=dropout_rate,\n",
    "            fc_size=fc_size,\n",
    "            num_classes=2\n",
    "        )\n",
    "\n",
    "        # Initialize the optimizer for the model\n",
    "        optimizer = optim.Adam(model_trial.parameters(), lr=params_model['learning_rate'])\n",
    "        # Define the loss function\n",
    "        loss_fcn = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Get the data loaders for training and validation\n",
    "        train_loader, val_loader = get_data_loaders(\n",
    "            transforms.Compose([\n",
    "                transforms.Resize((params_model[\"resolution\"], params_model[\"resolution\"])),\n",
    "                transforms.ToTensor(),\n",
    "            ]),\n",
    "            params_model[\"batch_size\"],\n",
    "            AIvsReal_path=dataset_path\n",
    "        )\n",
    "\n",
    "        # Set the number of training epochs\n",
    "        n_epochs = 3\n",
    "        # Move the model to the specified device\n",
    "        model_trial = model_trial.to(device)\n",
    "\n",
    "        # Begin the training loop\n",
    "        for epoch in range(n_epochs):\n",
    "            # Run a single training epoch\n",
    "            _ = training_epoch(\n",
    "                model_trial,\n",
    "                train_loader,\n",
    "                optimizer,\n",
    "                loss_fcn,\n",
    "                device,\n",
    "                epoch,\n",
    "                n_epochs,\n",
    "                silent=False,\n",
    "            )\n",
    "\n",
    "        # Initialize the evaluation metrics\n",
    "        accuracy_metric = Accuracy(task=\"binary\").to(device)\n",
    "        precision_metric = Precision(task=\"binary\").to(device)\n",
    "        recall_metric = Recall(task=\"binary\").to(device)\n",
    "\n",
    "        # Set the model to evaluation mode\n",
    "        model_trial.eval()\n",
    "        # Disable gradient calculations for evaluation\n",
    "        with torch.no_grad():\n",
    "            # Iterate through the validation data\n",
    "            for inputs, labels in val_loader:\n",
    "                # Move data to the specified device\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                # Perform a forward pass\n",
    "                outputs = model_trial(inputs)\n",
    "                # Get the predicted class\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "                # Update the metrics with the current batch's predictions and labels\n",
    "                accuracy_metric.update(preds, labels)\n",
    "                precision_metric.update(preds, labels)\n",
    "                recall_metric.update(preds, labels)\n",
    "\n",
    "        # Compute the final metric scores and store them in the results dictionary\n",
    "        results[i] = {\n",
    "            'trial': i,\n",
    "            'accuracy': accuracy_metric.compute().item(),\n",
    "            'precision': precision_metric.compute().item(),\n",
    "            'recall': recall_metric.compute().item(),\n",
    "        }\n",
    "\n",
    "    # Return the dictionary of results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f31db2-0dbf-4dbf-8650-369b9d83c015",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# NOTE: It takes about 16 minutes to run\n",
    "results_alt = train_with_alternative_metrics(study=study, best_k_trials=best_k_trials, device=DEVICE, dataset_path=AIvsReal_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84a32bc-c64c-4c95-a370-0ac90733e97a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_metric_scatter(df, x_col=\"accuracy\", y_col=\"precision\", color_col=\"recall\", label_col=\"trial\"):\n",
    "    \"\"\"\n",
    "    Creates a scatter plot to visualize the relationship between two metrics,\n",
    "    with a third metric represented by color.\n",
    "\n",
    "    Args:\n",
    "        df: A pandas DataFrame containing the data to plot.\n",
    "        x_col: The name of the column to use for the x-axis.\n",
    "        y_col: The name of the column to use for the y-axis.\n",
    "        color_col: The name of the column to use for point colors.\n",
    "        label_col: The name of the column to use for labeling each point.\n",
    "    \"\"\"\n",
    "    # Create a new figure and axes for the plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Normalize color values and define the colormap\n",
    "    color_values = df[color_col]\n",
    "    # Create a normalization object for the color range\n",
    "    norm = mpl.colors.Normalize(vmin=color_values.min(), vmax=color_values.max())\n",
    "    # Define the colormap to be used\n",
    "    cmap = plt.cm.viridis\n",
    "\n",
    "    # Create the scatter plot\n",
    "    scatter = ax.scatter(\n",
    "        df[x_col],\n",
    "        df[y_col],\n",
    "        c=color_values,\n",
    "        cmap=cmap,\n",
    "        norm=norm,\n",
    "        s=100,\n",
    "        edgecolor='k',\n",
    "        alpha=0.8\n",
    "    )\n",
    "\n",
    "    # Add a colorbar to the plot\n",
    "    cbar = plt.colorbar(scatter, ax=ax)\n",
    "    # Set the label for the colorbar\n",
    "    cbar.set_label(color_col.capitalize())\n",
    "\n",
    "    # Add text labels to each point on the plot\n",
    "    for _, row in df.iterrows():\n",
    "        ax.text(\n",
    "            row[x_col],\n",
    "            row[y_col],\n",
    "            str(row[label_col]),\n",
    "            fontsize=9,\n",
    "            ha='center',\n",
    "            va='center',\n",
    "            color='white',\n",
    "        )\n",
    "\n",
    "    # Set the labels and title of the plot\n",
    "    ax.set_xlabel(x_col.capitalize())\n",
    "    ax.set_ylabel(y_col.capitalize())\n",
    "    ax.set_title(f\"{y_col.capitalize()} vs {x_col.capitalize()} (colored by {color_col.capitalize()})\")\n",
    "    # Adjust plot parameters for a tight layout\n",
    "    plt.tight_layout()\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc866979",
   "metadata": {},
   "source": [
    "Now you can observe how good your models are in terms of accuracy, precision (a.k.a. reputation) and recall (a.k.a. profit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a5b490",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "results_df_metrics = pd.DataFrame(results_alt).T\n",
    "results_df_metrics = results_df_metrics.astype({'trial': 'Int64'})\n",
    "\n",
    "print(results_df_metrics)\n",
    "\n",
    "# get the best model based on precision\n",
    "best_precision_trial = results_df_metrics.loc[results_df_metrics['precision'].idxmax()]\n",
    "print(f\"Best model based on precision: Trial {best_precision_trial['trial']} with Precision: {best_precision_trial['precision']:.4f}, Recall: {best_precision_trial['recall']:.4f}, Accuracy: {best_precision_trial['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda6e6eb-c480-4c72-a134-dd02f7d66845",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "plot_metric_scatter(results_df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e8ed7f",
   "metadata": {
    "title": "Takeaways"
   },
   "source": [
    "So, if your emphasis is on reputation, you can choose the model with the highest precision.\n",
    "For selecting a model with a good balance of precision and accuracy, consider exploring the region toward the right corner.\n",
    "\n",
    "You have now added the ability to evaluate your models using alternative metrics such as Precision and Recall, which can help you make more informed decisions about which models to deploy based on your specific business needs.\n",
    "This concludes the optional section on alternative metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837445ce-8cd7-4551-a622-4fa39354e2cb",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Excellent work, AI Explorer! You have successfully completed your mission to build the FakeFinder, mastering a powerful and modern AutoML workflow from start to finish. You engineered a dynamic `FlexibleCNN`, designed a sophisticated search space with Optuna, and implemented the objective function that guided the automated discovery process. By going beyond simple accuracy to analyze model efficiency with metrics like trainable parameters, precision, and recall, you have learned to balance performance with practical, real world constraints.\n",
    "\n",
    "The skills you've developed are at the forefront of AI, where automating model discovery is an essential technique for solving complex problems. You are now well equipped with the tools and mindset to build and evaluate models based on specific business goals, a capability that will be invaluable as you tackle even more ambitious challenges. Congratulations on a successful expedition, and best of luck on your next adventure! "
   ]
  }
 ],
 "metadata": {
  "grader_version": "1",
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "pytorch-dlai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
