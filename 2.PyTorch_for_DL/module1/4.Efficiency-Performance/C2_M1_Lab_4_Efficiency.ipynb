{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acbb0d1e",
   "metadata": {},
   "source": [
    "# Efficiency vs Performance Metrics\n",
    "\n",
    "Welcome! In this hands on lab, you'll explore the important balance between a model's performance and its efficiency. As you've learned, building effective machine learning solutions involves more than just aiming for the highest accuracy. Applications in the real world often have strict limits on model size, inference time, and memory usage that you must consider alongside predictive power.\n",
    "\n",
    "In this notebook, you will take a systematic approach to compare and select models by combining both performance and efficiency standards.\n",
    "\n",
    "Specifically, you’ll learn to:\n",
    "\n",
    "* **Train diverse models**: You will train two different convolutional neural networks on the CIFAR10 dataset: a compact, lightweight CNN and the deeper ResNet34 architecture.\n",
    "\n",
    "* **Evaluate key efficiency metrics**: You will use provided utility functions to measure and analyze each model's size, inference speed, and memory footprint.\n",
    "\n",
    "* **Apply model selection strategies**: You will experiment with two methods for choosing the best model: a weighted scoring system that balances multiple factors and a constraint based approach that filters models based on hard limits.\n",
    "\n",
    "By practicing these strategies, you'll gain practical experience in balancing model complexity with predictive quality. These skills are essential for designing and deploying robust, efficient deep learning systems in production environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c95542d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21684679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import helper_utils\n",
    "import model_architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53633405-b7bb-438a-acf1-858cb3742814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9074cf66",
   "metadata": {},
   "source": [
    "## Beyond Performance: Considering Efficiency\n",
    "\n",
    "Model selection in deep learning should not be based solely on standard performance metrics such as accuracy or F1 score. Especially in real-world applications, it is important to account for efficiency metrics that impact model deployment and usability. \n",
    "\n",
    "**Efficiency metrics** include but are not limited to:\n",
    "- **Model size:** The memory footprint of the model, often measured in megabytes.\n",
    "- **Inference time:** The average time required for the model to make predictions on new data.\n",
    "\n",
    "Evaluating these metrics allows for a more comprehensive comparison between models, helping to balance predictive performance with practical requirements such as speed and resource constraints.\n",
    "\n",
    "### Comparing Two Models\n",
    "\n",
    "In the following sections, two models are trained and evaluated on the CIFAR10 dataset:\n",
    "- **OptimizedCNN:** A compact convolutional neural network composed of a small number of convolutional layers.\n",
    "- **ResNet34:** A deeper and more complex architecture, consisting of 34 layers designed for high performance.\n",
    "\n",
    "Both models will be compared not only in terms of accuracy but also with respect to model size and inference time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c2285b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OptimizedCNN\n",
    "model_CNN = model_architectures.OptimizedCNN()\n",
    "\n",
    "# ResNet-34\n",
    "model_resnet = model_architectures.ResNet34()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c882f6",
   "metadata": {},
   "source": [
    "### Training and Evaluation\n",
    "\n",
    "The following code demonstrates how to train and evaluate two different models on the CIFAR10 dataset. \n",
    "\n",
    "- The `get_data_loaders_with_validation` function is used to create DataLoader instances for the training, validation, and test splits. The training and validation loaders are used during model training and hyperparameter tuning, while the test loader is reserved for final model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c130cb4-7d17-4aa9-ac36-9b3a0aa24c12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data_loaders_with_validation(batch_size, val_fraction=0.1):\n",
    "    \"\"\"Creates and returns data loaders for training, validation, and testing.\n",
    "\n",
    "    Args:\n",
    "        batch_size: The number of samples per batch in each data loader.\n",
    "        val_fraction: The fraction of the training data to use for validation.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the training, validation, and test data loaders.\n",
    "    \"\"\"\n",
    "    # Define the transformations for the training data, including augmentation.\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    # Define the transformations for the validation and test data (no augmentation).\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    # Load the full CIFAR-10 training dataset.\n",
    "    full_trainset = datasets.CIFAR10(root='./cifar10', train=True, download=True, transform=transform_train)\n",
    "    # Calculate the number of samples for the training and validation sets.\n",
    "    total_train = len(full_trainset)\n",
    "    val_size = int(val_fraction * total_train)\n",
    "    train_size = total_train - val_size\n",
    "\n",
    "    # Split the full training set into separate training and validation sets.\n",
    "    train_set, val_set = random_split(full_trainset, [train_size, val_size])\n",
    "\n",
    "    # Apply the non-augmented test transform to the validation set.\n",
    "    val_set.dataset.transform = transform_test\n",
    "\n",
    "    # Load the CIFAR-10 test dataset.\n",
    "    test_set = datasets.CIFAR10(root='./cifar10', train=False, download=True, transform=transform_test)\n",
    "\n",
    "    # Create DataLoader instances for each dataset split.\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    # Return the created data loaders.\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066c1cf2-5577-4739-82a5-6ae2ee0ac777",
   "metadata": {},
   "source": [
    "- The `train_and_evaluate` function performs the training loop for a specified number of epochs, evaluating model performance on the validation set at the end of each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5bae5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, optimizer, scheduler, device, n_epochs, train_loader, val_loader):\n",
    "    \"\"\"Manages the main training and evaluation loop for a model.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model to train.\n",
    "        optimizer: The optimizer for updating model weights.\n",
    "        scheduler: The learning rate scheduler (can be None).\n",
    "        device: The device ('cpu' or 'cuda') to perform computations on.\n",
    "        n_epochs: The total number of epochs to train for.\n",
    "        train_loader: The DataLoader for the training data.\n",
    "        val_loader: The DataLoader for the validation data.\n",
    "    \"\"\"\n",
    "    # Move the model to the specified compute device.\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define the loss function.\n",
    "    loss_fcn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initialize the progress bar for visualizing the training process.\n",
    "    pbar = helper_utils.NestedProgressBar(total_epochs=n_epochs, total_batches=len(train_loader), epoch_message_freq=1)\n",
    "\n",
    "    # Loop through the specified number of epochs.\n",
    "    for epoch in range(n_epochs):\n",
    "        # Update the epoch-level progress bar for the new epoch.\n",
    "        pbar.update_epoch(epoch+1)\n",
    "\n",
    "        # Perform one epoch of training and get the average training loss.\n",
    "        train_loss, _ = helper_utils.train_epoch(model, train_loader, optimizer, loss_fcn, device, pbar)\n",
    "        \n",
    "        # Evaluate the model's accuracy on the validation set.\n",
    "        val_acc = helper_utils.evaluate_accuracy(model, val_loader, device)\n",
    "\n",
    "        # Log the training loss and validation accuracy for the current epoch.\n",
    "        pbar.maybe_log_epoch(epoch+1, message=f\"Epoch {epoch+1} - Train Loss: {train_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Check if a learning rate scheduler is being used.\n",
    "        if scheduler is not None:\n",
    "            # Handle the ReduceLROnPlateau scheduler, which requires a metric.\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(val_acc)\n",
    "            # For other schedulers, step without a metric.\n",
    "            else:\n",
    "                scheduler.step()\n",
    "\n",
    "    # Close the progress bar and print a final completion message.\n",
    "    pbar.close(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6e33fe",
   "metadata": {},
   "source": [
    "Now you will train the two models and evaluate them.\n",
    "Two different optimizers are used: `Adam` for `OptimizedCNN` and `SGD` for `ResNet34`.\n",
    "You will also use a learning rate scheduler to adjust the learning rate during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc2add71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:54<00:00, 3.11MB/s] \n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 128\n",
    "train_loader, val_loader, test_loader = get_data_loaders_with_validation(batch_size=batch_size, val_fraction=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4f4b17f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5d48ae914c4820a3d419201901d533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2490615326844fca3b35a18d532f719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current Batch:   0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05251f22a6f94cfc8ea7149414621b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 1.4267, Val Acc: 0.5588\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7856051a2a140ccae88fd736fd516d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train Loss: 1.0014, Val Acc: 0.6456\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa4833a66dd4c4984a223eb6bc90037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train Loss: 0.8596, Val Acc: 0.6980\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63cac60745414cd08bd9e08f5e73f045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train Loss: 0.7669, Val Acc: 0.7208\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432b7867523845d28c942fc0e215221d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train Loss: 0.6966, Val Acc: 0.7308\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5554a445d4408aaa6c0d7ebc918d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train Loss: 0.6307, Val Acc: 0.7276\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cab3d11ae5840e99b3707018efe150e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Train Loss: 0.5781, Val Acc: 0.7398\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcafbafe1103482c9bae8eb4c8a9c6e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Train Loss: 0.5325, Val Acc: 0.7630\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7557f819f07d44c2b8451c3a42249ab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Train Loss: 0.4849, Val Acc: 0.7658\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b8782b38b194521bd3b20924264b328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Train Loss: 0.4432, Val Acc: 0.7646\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "optimizer_CNN = optim.Adam(model_CNN.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler_CNN = optim.lr_scheduler.ReduceLROnPlateau(optimizer_CNN, 'max', patience=3, factor=0.5)\n",
    "\n",
    "# Train and evaluate the OptimizedCNN model\n",
    "train_and_evaluate(model_CNN, optimizer_CNN, scheduler_CNN, device, n_epochs, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fc77b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b5bd222dc442a3adbcbc037b367788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e337a53c9ce647cd8d9c09835fe33329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current Batch:   0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be665f47e37948c1ab70c5eb7a59f8dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 2.2633, Val Acc: 0.2768\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8892d8fc209c4805b486fb391546d9eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train Loss: 1.6885, Val Acc: 0.4376\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43866c340b6d4c4e923c33424891993b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train Loss: 1.4558, Val Acc: 0.5108\n"
     ]
    }
   ],
   "source": [
    "optimizer_resnet = optim.SGD(model_resnet.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler_resnet = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_resnet, T_max=200)\n",
    "\n",
    "# Train and evaluate the ResNet-34 model\n",
    "train_and_evaluate(model_resnet, optimizer_resnet, scheduler_resnet, device, n_epochs, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b93b45",
   "metadata": {},
   "source": [
    "## Evaluating Model Efficiency\n",
    "\n",
    "When comparing machine learning models, it is important to consider their efficiency in addition to their predictive performance. \n",
    "In practical scenarios, factors such as model size and inference time may significantly affect the choice of model, especially when deploying on resource-constrained devices.\n",
    "\n",
    "In this section, efficiency metrics will be computed for the trained models. By calculating these metrics, one can compare models not just by accuracy but also by how efficiently they use computational resources.\n",
    "\n",
    "- `get_model_size`: Calculates the total memory size of the model in megabytes (MB), considering both trainable parameters and non-trainable buffers.\n",
    "    - The function iterates over all parameters using `model.parameters()`. For each parameter, it multiplies the number of elements (`param.nelement()`) by the size in bytes of each element (`param.element_size()`), accumulating the total in `param_size`.\n",
    "    - It repeats this process for non-trainable buffers, accessed via `model.buffers()`, accumulating their total size in `buffer_size`.\n",
    "    - The final model size in MB is computed as `(param_size + buffer_size) / 1024**2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f8ff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model):\n",
    "    \"\"\"Calculates the total size of a PyTorch model in megabytes (MB).\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model (nn.Module) whose size is to be calculated.\n",
    "\n",
    "    Returns:\n",
    "        The total size of the model in megabytes (MB).\n",
    "    \"\"\"\n",
    "    # Initialize a variable to store the size of trainable parameters.\n",
    "    param_size = 0\n",
    "    # Iterate over all model parameters.\n",
    "    for param in model.parameters():\n",
    "        # Add the size of the current parameter to the total.\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "\n",
    "    # Initialize a variable to store the size of non-trainable buffers.\n",
    "    buffer_size = 0\n",
    "    # Iterate over all model buffers.\n",
    "    for buffer in model.buffers():\n",
    "        # Add the size of the current buffer to the total.\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "    # Calculate the total size in megabytes.\n",
    "    size_in_mb = (param_size + buffer_size) / 1024**2\n",
    "    # Return the final calculated size.\n",
    "    return size_in_mb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2e825d-d56e-4c8d-92a1-b0560dc09666",
   "metadata": {},
   "source": [
    "- `measure_inference_time`: \n",
    "Estimates the average time taken by the model to perform inference on a batch of input data.\n",
    "    - The model is set to evaluation mode using `model.eval()`.\n",
    "    - The input data is moved to the same device as the model with `input_data.to(device)`.\n",
    "    - A short warmup loop is run to ensure accurate timing.\n",
    "    - The inference time is measured by running the model forward pass `num_iterations` times inside a `torch.no_grad()` block and computing the elapsed time.\n",
    "    - The function returns the average inference time per pass, converted from seconds to milliseconds (`ms`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4510a058-242b-4f8f-84b4-a99f3e9ce6a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def measure_inference_time(model, input_data, num_iterations=100):\n",
    "    \"\"\"Measures the average inference time of a model for a given input.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model (nn.Module) to be benchmarked.\n",
    "        input_data: A sample input tensor for the model.\n",
    "        num_iterations: The number of times to run inference for averaging.\n",
    "\n",
    "    Returns:\n",
    "        The average inference time in milliseconds (ms).\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode to disable layers like dropout.\n",
    "    model.eval()\n",
    "\n",
    "    # Get the device the model is currently on.\n",
    "    device = next(model.parameters()).device\n",
    "    # Move the input data to the same device as the model.\n",
    "    input_data = input_data.to(device)\n",
    "\n",
    "    # Perform a few warmup runs to allow for GPU caching and other setup.\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            _ = model(input_data)\n",
    "\n",
    "    # Record the start time before the main measurement loop.\n",
    "    start_time = time.time()\n",
    "    # Run inference for the specified number of iterations without calculating gradients.\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_iterations):\n",
    "            _ = model(input_data)\n",
    "    # Record the end time after the loop finishes.\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate the average time per iteration.\n",
    "    avg_time = (end_time - start_time) / num_iterations\n",
    "    # Convert the average time from seconds to milliseconds and return it.\n",
    "    return avg_time * 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6286ab",
   "metadata": {},
   "source": [
    "### Model Efficiency and Performance Comparison\n",
    "\n",
    "The `evaluate_efficiency` function provides a comprehensive summary of a model's key efficiency and performance metrics. \n",
    "- The function calls `get_model_size(model)` to compute the total memory required to store the model's parameters and buffers. \n",
    "\n",
    "- A batch of input data is extracted from the test loader, and a representative sample (`sample_input`) is created by selecting a single input and moving it to the correct device. \n",
    "The function then uses `measure_inference_time(model, sample_input)` to determine the average time (in milliseconds) needed for the model to process a single input.\n",
    "\n",
    "- The model's accuracy is evaluated using the `helper_utils.evaluate_accuracy(model, test_loader, device)` function, which computes the ratio of correct predictions to total samples in the test set.\n",
    "\n",
    "The results are packaged in a dictionary with keys for `\"accuracy\"`, `\"model_size_mb\"`, and `\"inference_time_ms\"`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b38513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_efficiency(model, test_loader, device):\n",
    "    \"\"\"Evaluates the efficiency of a model based on several key metrics.\n",
    "\n",
    "    This function calculates the model's accuracy on a test set, its total\n",
    "    size in megabytes, and its average inference time in milliseconds.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model (nn.Module) to be evaluated.\n",
    "        test_loader: The DataLoader providing the test dataset.\n",
    "        device: The device ('cpu' or 'cuda') on which to perform the evaluation.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the model's 'accuracy', 'model_size_mb',\n",
    "        and 'inference_time_ms'.\n",
    "    \"\"\"\n",
    "    # Calculate the model's size in megabytes.\n",
    "    model_size = get_model_size(model)\n",
    "\n",
    "    # Create an iterator to get a single batch for inference time measurement.\n",
    "    data_iter = iter(test_loader)\n",
    "    batch = next(data_iter)\n",
    "    # Extract the input tensors from the batch.\n",
    "    inputs = batch[0]\n",
    "    # Create a single-sample batch and move it to the correct device.\n",
    "    sample_input = inputs[:1].to(device)\n",
    "    # Measure the average inference time using the sample input.\n",
    "    inf_time = measure_inference_time(model, sample_input)\n",
    "\n",
    "    # Calculate the model's accuracy on the entire test set.\n",
    "    test_accuracy = helper_utils.evaluate_accuracy(model, test_loader, device)\n",
    "\n",
    "    # Return the collected efficiency metrics in a dictionary.\n",
    "    return {\n",
    "        \"accuracy\": test_accuracy,\n",
    "        \"model_size_mb\": model_size,\n",
    "        \"inference_time_ms\": inf_time,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3530218",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Optimized_CNN\": model_CNN,\n",
    "    \"ResNet34\": model_resnet\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    results[name] = evaluate_efficiency(model, test_loader, device)\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5120353d-0466-4c9c-8c08-6a087475895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.plot_efficiency_analysis(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16df89d4",
   "metadata": {},
   "source": [
    "### Model Selection based on constraints and weighted criteria\n",
    "\n",
    "When considering both performance and efficiency, model selection becomes a multi-objective decision. \n",
    "Instead of choosing a model solely based on accuracy, you may want to account for additional factors such as model size and inference time.\n",
    "\n",
    "In this section, you are provided with a table containing several ResNet models.  \n",
    "Each model has different values for size, inference time, and accuracy.  \n",
    "Although these models are not trained within this notebook (the values are hypothetical and might correspond to results after 100 epochs), you can apply the same selection methods to any set of models, just like you did for `OptimizedCNN` and `ResNet34`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8669a0-ae08-45fc-ae1d-1f47624fd658",
   "metadata": {},
   "source": [
    "In the cells bellow you will find two different approaches for model selection `select_best_model_weighted` and `select_best_model_constraint_based`.\n",
    "\n",
    "These functions encapsulate the main ingredients of a robust model selection pipeline: conversion and normalization of metrics, flexible weighting, hard constraint enforcement, and clear reporting of results. The structured approach ensures models are chosen according to both application requirements and available resources.\n",
    "\n",
    "- `select_best_model_weighted`: Systematically selects the optimal model by combining normalized model metrics into a single score using customizable weights.\n",
    "    - The input `results` is first converted to a dictionary using `results.to_dict(orient=\"index\")`, enabling direct access to model entries.\n",
    "    - Metric weights can be specified (e.g., `weights = {\"accuracy\": 0.6, \"model_size_mb\": 0.2, \"inference_time_ms\": 0.2}`) or left as defaults to prioritize accuracy.\n",
    "    - Normalization of each metric occurs across all models: \n",
    "        - For `\"accuracy\"`, normalization is performed so that higher scores are better using  \n",
    "          `norm_value = (value - min_val) / range_val`.\n",
    "        - For `\"model_size_mb\"` and `\"inference_time_ms\"`, normalization rewards smaller values via  \n",
    "          `norm_value = 1 - (value - min_val) / range_val`.\n",
    "    - The weighted sum for each model is computed as  \n",
    "      `sum(weights[metric] * normalized[name][metric] for metric in metrics)`, aggregating the influence of all metrics based on their importance.\n",
    "    - The model with the highest combined score is chosen with  \n",
    "      `best_model = max(scores.items(), key=lambda x: x[1])`.\n",
    "    - The function returns both the name of the best model and the scores for all evaluated models, providing transparency and flexibility for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5080061e-7445-406e-a631-7b26ac481b66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_best_model_weighted(results, weights=None):\n",
    "    \"\"\"Selects the best model from a set of results using a weighted scoring system.\n",
    "\n",
    "    Args:\n",
    "        results: A pandas DataFrame containing model performance metrics,\n",
    "                 with model names as the index.\n",
    "        weights: An optional dictionary where keys are metric names (e.g.,\n",
    "                 'accuracy') and values are their corresponding weights.\n",
    "                 If None, default weights are used.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the name of the best model and a dictionary of\n",
    "        all models' weighted scores.\n",
    "    \"\"\"\n",
    "    # Convert the DataFrame to a dictionary for easier processing.\n",
    "    results = results.to_dict(orient=\"index\")\n",
    "    \n",
    "    # If no weights are provided, define a default set that prioritizes accuracy.\n",
    "    if weights is None:\n",
    "        weights = {\"accuracy\": 0.5, \"model_size_mb\": 0.2, \"inference_time_ms\": 0.3}\n",
    "\n",
    "    # Get the list of metrics to be considered from the weights dictionary.\n",
    "    metrics = list(weights.keys())\n",
    "    # Initialize a dictionary to store the normalized metric values for each model.\n",
    "    normalized = {name: {} for name in results}\n",
    "\n",
    "    # Loop through each metric to normalize its values across all models.\n",
    "    for metric in metrics:\n",
    "        # Extract all values for the current metric to find the min and max.\n",
    "        values = [res[metric] for res in results.values()]\n",
    "        min_val, max_val = min(values), max(values)\n",
    "        # Calculate the range of values, avoiding division by zero.\n",
    "        range_val = max_val - min_val if max_val != min_val else 1.0\n",
    "\n",
    "        # Iterate through each model's results to calculate its normalized score.\n",
    "        for name, res in results.items():\n",
    "            value = res[metric]\n",
    "            # Check the metric type to determine the normalization direction.\n",
    "            if metric == \"accuracy\":\n",
    "                # For accuracy, higher values are better, so normalize directly.\n",
    "                norm_value = (value - min_val) / range_val\n",
    "            else:\n",
    "                # For size and time, lower values are better, so invert the normalization.\n",
    "                norm_value = 1 - (value - min_val) / range_val\n",
    "            # Store the calculated normalized value.\n",
    "            normalized[name][metric] = norm_value\n",
    "\n",
    "    # Calculate the final weighted score for each model.\n",
    "    scores = {\n",
    "        name: sum(weights[metric] * normalized[name][metric] for metric in metrics)\n",
    "        for name in results\n",
    "    }\n",
    "\n",
    "    # Find the model with the highest overall score.\n",
    "    best_model = max(scores.items(), key=lambda x: x[1])\n",
    "    # Return the name of the best model and the dictionary of all scores.\n",
    "    return best_model[0], scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dbb3f5-021a-4752-b25a-299f6fa8adf9",
   "metadata": {},
   "source": [
    "- `select_best_model_constraint_based`: Selects the most appropriate model by strictly enforcing resource constraints before considering performance.\n",
    "    - Input `results` is also converted to a dictionary using `results.to_dict(orient=\"index\")` for direct filtering.\n",
    "    - The code applies dual constraints, retaining models only if  \n",
    "      `metrics[\"model_size_mb\"] <= max_size_mb` and  \n",
    "      `metrics[\"inference_time_ms\"] <= max_inference_ms`.\n",
    "    - If no models satisfy both constraints, a message is printed to inform the user and the function returns `None`.\n",
    "    - Among the viable models, selection is based on accuracy using  \n",
    "      `best_model = max(viable_models.items(), key=lambda x: x[1][\"accuracy\"])`.\n",
    "    - The function returns the highest-accuracy model meeting the constraints and a collection of all viable options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ee15db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_model_constraint_based(results, max_size_mb, max_inference_ms):\n",
    "    \"\"\"Selects the best model based on hard constraints for size and inference time.\n",
    "\n",
    "    Args:\n",
    "        results: A pandas DataFrame of model performance metrics.\n",
    "        max_size_mb: The maximum allowable model size in megabytes.\n",
    "        max_inference_ms: The maximum allowable inference time in milliseconds.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the name of the best model and a dictionary of all\n",
    "        viable models. Returns None if no models meet the constraints.\n",
    "    \"\"\"\n",
    "    # Convert the DataFrame to a dictionary for easier processing.\n",
    "    results = results.to_dict(orient=\"index\")\n",
    "    \n",
    "    # Filter the results to include only models that satisfy both constraints.\n",
    "    viable_models = {\n",
    "        name: metrics for name, metrics in results.items()\n",
    "        if metrics[\"model_size_mb\"] <= max_size_mb and\n",
    "           metrics[\"inference_time_ms\"] <= max_inference_ms\n",
    "    }\n",
    "\n",
    "    # Check if any models met the constraints.\n",
    "    if not viable_models:\n",
    "        # If no models are viable, notify the user.\n",
    "        print(\"No models meet all constraints. Consider relaxing constraints.\")\n",
    "        return None\n",
    "\n",
    "    # From the filtered list, select the model with the highest accuracy.\n",
    "    best_model = max(viable_models.items(), key=lambda x: x[1][\"accuracy\"])\n",
    "    # Return the name of the best model and all models that met the constraints.\n",
    "    return best_model[0], viable_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c5e7d6",
   "metadata": {},
   "source": [
    "Now, run the two function to select the best of the ResNet models based on the given constraints and criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661575c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_results = helper_utils.load_resnet_table()\n",
    "resnet_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5af195-d0eb-4ae2-8ba4-8c9a0c7d71e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.plot_efficiency_analysis(resnet_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c814a2",
   "metadata": {},
   "source": [
    "You can see that as the model depth increases (from ResNet-18 to ResNet-152), both the model size and inference time increase. \n",
    "In other words, deeper models tend to be larger and slower.\n",
    "Depending on the application constraints (e.g., real-time inference vs. accuracy needs), selecting the appropriate model requires balancing model complexity and efficiency.\n",
    "\n",
    "Now you can run the two functions to select the best model based on the given constraints and criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e7b77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_weighted, weighted_scores = select_best_model_weighted(resnet_results)\n",
    "\n",
    "print(f\"Best model by weighted criteria: {best_model_weighted}\")\n",
    "print(\"Weighted scores:\")\n",
    "pprint(weighted_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7458ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_constrains, viable_models = select_best_model_constraint_based(resnet_results, max_size_mb=100, max_inference_ms=4)\n",
    "\n",
    "print(f\"Best model by constraints: {best_model_constrains}\")\n",
    "print(\"Viable models:\")\n",
    "pprint(viable_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb97a825",
   "metadata": {},
   "source": [
    "Observe that depending on the constraints and criteria, the selected model may vary.\n",
    "For the weighted criteria, even if the ResNet-152 has the highest accuracy, it is not the one with the best score (due to its larger size and longer inference time)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc335fd8",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations on reaching the conclusion of this practical tour of model selection and efficiency evaluation in PyTorch! This notebook has illustrated not only how to train and assess powerful neural network architectures, but also how to weigh critical efficiency metrics that directly affect model deployment and usability.\n",
    "\n",
    "Key steps in this workflow included training compact and complex models, calculating resource costs.\n",
    "Moreover, automated model selection was explored through both weighted objective functions and strict constraint filtering, highlighting the trade-offs between accuracy, speed, and memory footprint across different scenarios.\n",
    "\n",
    "By working through these tasks, you'll gain a richer, more practical understanding of what it takes to select the most suitable model for a given application. \n",
    "These techniques empower practitioners to make informed, data-driven decisions tailored to real-world technological and operational constraints, laying a strong foundation for robust, efficient machine learning system design."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "pytorch-dlai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
