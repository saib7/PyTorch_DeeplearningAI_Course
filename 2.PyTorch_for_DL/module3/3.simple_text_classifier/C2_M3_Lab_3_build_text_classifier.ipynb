{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe110568-9e4f-4b87-8f7e-c07c35518585",
   "metadata": {},
   "source": [
    "# Building a Simple Text Classifier in PyTorch\n",
    "\n",
    "Having explored the foundational concepts of converting text from human language into machine-friendly tensors, you are now ready to apply that knowledge to a practical challenge. You have seen how raw text is tokenized, transformed into numerical IDs, and then represented by meaningful vectors called embeddings that capture semantic relationships between words. This lab will guide you through the process of building a complete text classification pipeline from scratch using PyTorch. The goal is to train a model that can read the title of a recipe and classify it as either fruit-based or vegetable-based.\n",
    "\n",
    "This hands-on lab will solidify your understanding of the entire workflow, from initial data preparation to final model evaluation. You will put theory into practice and tackle common challenges in natural language processing.\n",
    "\n",
    "Specifically, in this lab, you'll learn to:\n",
    "\n",
    "* **Prepare and preprocess raw text data**, which includes cleaning, tokenizing, and building a vocabulary from a training dataset to avoid data leakage.\n",
    "* **Construct custom PyTorch `Dataset` and `DataLoader` objects** and implement custom `collate` functions to efficiently handle batches of variable-length text sequences.\n",
    "* **Build, train, and compare multiple model architectures**. This includes an efficient model using `nn.EmbeddingBag` and others that perform manual pooling operations like mean, max, and sum.\n",
    "* **Address a class imbalance problem** in the dataset by applying class weights to the loss function, a vital technique for training fair and accurate models on skewed data.\n",
    "* **Evaluate your trained models** to select the best-performing one based on the F1 score and then test its predictive power on new, unseen recipe titles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1500a619-5745-42da-8849-d1e4d85ada64",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcf93e2-2c1c-4aa4-a05d-c3f89c586bb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from collections import Counter\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "import helper_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f808121-f0bd-4dc6-a91f-2189c50db2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75606bd-f0df-4b8c-9a20-a3b71a600d02",
   "metadata": {},
   "source": [
    "## The Recipe Dataset\n",
    "\n",
    "For this lab, you will work with a specially prepared dataset derived from a large collection of recipes. You will load the prepared dataset and get it ready for the modeling process. This involves transforming the recipe titles and labels into a numerical format that a PyTorch model can understand.\n",
    "\n",
    "**The Food.com Recipe Collection**\n",
    "\n",
    "Your data originates from the [Food.com Recipes and User Interactions](https://www.kaggle.com/datasets/shuyangli94/food-com-recipes-and-user-interactions) dataset, a vast collection of over 230,000 recipes gathered over 18 years. This rich dataset contains everything from recipe names and ingredients to cooking steps and nutritional information. For the purposes of this lab, a subset of this data has been pre-created, focusing solely on recipes that are either fruit-based or vegetable-based.\n",
    "\n",
    "**How the Subset Was Created**\n",
    "\n",
    "The `recipes_fruit_veg.csv` file you'll use was generated by a script that filtered the original dataset. In short, the script performed the following actions:\n",
    "\n",
    "* It scanned each recipe’s ingredients for a predefined list of common fruit and vegetable keywords.\n",
    "* To create distinct categories, it only kept recipes that contained fruit keywords but no vegetable keywords, or vice versa.\n",
    "* Any recipe that contained a mix of both categories, or neither, was excluded.\n",
    "\n",
    "This process ensures the dataset has two mutually exclusive classes, which is ideal for this classification task. If you're interested, you can explore the exact logic in the `filter_recipe_dataset` function within the `helper_utils.py` file.\n",
    "\n",
    "* Now, run the cell below to load the dataset. This will read the data from the `recipes_fruit_veg.csv` file into a pandas DataFrame called `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27739bf-9269-450a-acb8-9b9b9f34227c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the filtered dataset into a pandas DataFrame\n",
    "df = pd.read_csv(\"recipes_fruit_veg.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad0fe99-bd00-41d3-9ce5-5ee1396a28a8",
   "metadata": {},
   "source": [
    "### Preparing Inputs and Labels\n",
    "\n",
    "Begin by inspecting the first ten rows of your DataFrame to understand its structure. This will help you see the raw text data you will be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3863aa52-343b-41f8-a013-5a34433e01ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 10 rows of the DataFrame\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c47fa48-629c-468c-a7a2-d9063f311157",
   "metadata": {},
   "source": [
    "From the table above, you can see the different columns you have available. For your classification task, **your goal is to predict the category based on the recipe name**.\n",
    "\n",
    "* **Model Input**: The `name` column will serve as your input feature. This is the text you will train the model to understand.\n",
    "\n",
    "* **Model Labels**: The `category` column contains the labels ('fruit' or 'vegetable'). However, machine learning models require numerical data, not text. Your next step is to convert these string labels into numbers (e.g., 0 and 1) so the model can process them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9c7dde-837e-4239-a13a-61a97aee303a",
   "metadata": {},
   "source": [
    "* Perform this conversion by creating a new `label` column and setting the default value of **1 (for 'vegetable')** and then updating the label to **0 for all 'fruit'** recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8d488a-367d-4da1-a695-03c28f814ae8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the new 'label' column and set a default value.\n",
    "# Set everything to 1 (the label for 'vegetable').\n",
    "df['label'] = 1\n",
    "\n",
    "# Use boolean indexing to find all rows where the 'category' is 'fruit'\n",
    "# Update the 'label' in those specific rows to 0.\n",
    "df.loc[df['category'] == 'fruit', 'label'] = 0\n",
    "\n",
    "# Display the first few rows to confirm the new column is correct\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0986037-3290-4b23-a469-849e6fc60a44",
   "metadata": {},
   "source": [
    "With the numerical `label` column now ready, you can extract the data into its final format. \n",
    "\n",
    "* Create two separate Python lists:\n",
    "    * `texts`: A list containing the recipe names from the `name` column.\n",
    "    * `labels`: A list containing the numerical labels (0 or 1) from the `label` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c780e56c-f915-4dc6-bc52-271ed815c9f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Keep only rows with a recipe name.\n",
    "df_clean = df.dropna(subset=['name'])\n",
    "\n",
    "# Get recipe names as a list.\n",
    "texts = df_clean['name'].tolist()\n",
    "\n",
    "# Get matching labels as a list.\n",
    "labels = df_clean['label'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eee709d-a231-4870-b96b-94985be35e4e",
   "metadata": {},
   "source": [
    "* Run the next cell to see a breakdown of the samples you'll be working with.\n",
    "    * **Note**: Pay close attention to the output. You'll notice there are significantly fewer fruit recipes than vegetable recipes. This is a common issue known as data imbalance, and it's important to keep this in mind as you will address it later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b8cf8c-246f-4006-8c1b-95015eea56e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the final dataset size and the class distribution.\n",
    "print(f\"Total samples for classification:\\t{len(texts)}\")\n",
    "print(f\"Fruit recipes:\\t\\t\\t\\t{labels.count(0)}, {round(labels.count(0)/(labels.count(0) + labels.count(1)) *100,1)} %\")\n",
    "print(f\"Vegetable recipes:\\t\\t\\t{labels.count(1)}, {round(labels.count(1)/(labels.count(0) + labels.count(1)) *100,1)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa22df19-86ad-46b5-ac11-13da58e95d91",
   "metadata": {},
   "source": [
    "#### Previewing the Input and Labels\n",
    "\n",
    "Your data is now structured with the `name` column as the model's input and the `label` column as its output.\n",
    "\n",
    "* Run the cell below to review a random sample of these training pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae444a90-9d03-4b8d-97b6-cfd64eb45318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of random samples to display.\n",
    "num_samples = 10\n",
    "\n",
    "# Display a sample of name and label pairs.\n",
    "display(df[['name', 'label']].sample(num_samples, random_state=25).style.hide(axis=\"index\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e771b05c-527c-465a-94b3-074e4d13f898",
   "metadata": {},
   "source": [
    "## Splitting the Raw Data\n",
    "\n",
    "To accurately judge your model's performance, you need to test it on data it has **never seen before**. This means your first step before any text processing or vocabulary building is to separate your raw texts and labels lists into two distinct groups.\n",
    "\n",
    "The reason you do this now is to prevent **data leakage**. If you were to build your vocabulary from the **entire** dataset, the model would have already been exposed to the words in your validation data. This would not be a fair test of how it performs on new, real-world information.\n",
    "\n",
    "By splitting the raw data first, you can build the vocabulary using only the words from one group (the training data). The other group remains completely untouched, ensuring it is a truly \"unseen\" set of data for your final evaluation. You perform this division as a clean, preliminary step before any PyTorch specific formatting.\n",
    "\n",
    "* Use the <code>[train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)</code> function from **scikit-learn** to handle this.\n",
    "    * `texts`: The list of recipe titles you want to split.\n",
    "    * `labels`: The list of corresponding labels for each recipe title.\n",
    "    * `test_size=0.2`: Specifies that 20% of the data should be used for the validation set.\n",
    "    * `stratify=labels`: Ensures the training and validation sets have the same class proportions as the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96877fe6-9445-45a0-b5ce-0d28fc9208f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    stratify=labels\n",
    ")\n",
    "\n",
    "# Print the number of samples in each set to verify the split.\n",
    "print(f\"Training samples: {len(train_texts)}\")\n",
    "print(f\"Validation samples: {len(val_texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105cb240-36cf-400a-8771-c1ce7a8ba9e9",
   "metadata": {},
   "source": [
    "### Text Preprocessing and Vocabulary Building\n",
    "\n",
    "With your data now split into separate groups, it's time to convert the recipe titles into a numerical format that your model can understand. This involves cleaning the text and then building a vocabulary to map each unique word to a number.\n",
    "\n",
    "* Define the `preprocess_text` function and apply it to your recipe titles.\n",
    "    * It takes a raw text string as its input, cleans the text by converting it to lowercase, and removes any characters that are not letters or spaces.\n",
    "    * It then tokenizes the string by splitting it into a list of individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8bbf0e-03b9-4930-8bc1-71288ffce8b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Cleans and tokenizes a raw text string.\n",
    "\n",
    "    Args:\n",
    "        text (str): The raw text to be processed.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of cleaned words (tokens).\n",
    "    \"\"\"\n",
    "    # Convert the entire text to lowercase.\n",
    "    text = text.lower()\n",
    "    # Remove all characters that are not letters or whitespace.\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Split the cleaned string into a list of words.\n",
    "    words = text.split()\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d5d80f-15ef-48e8-a99f-1871d739c53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the training and validation texts separately.\n",
    "processed_train_texts = [preprocess_text(text) for text in train_texts]\n",
    "processed_val_texts = [preprocess_text(text) for text in val_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de95c24-ffff-43e5-ad2e-c43de5db6994",
   "metadata": {},
   "source": [
    "* Define the `Vocabulary` class, which creates and manages the mapping between words and unique numerical IDs.\n",
    "    * `__init__`: Initializes the vocabulary with special tokens for padding (`<pad>`) and unknown words (`<unk>`). It also sets a minimum frequency (`min_freq`) for a word to be included.\n",
    "    * `build_vocab`: Scans all the processed texts, counts the frequency of every word, and adds words that meet the `min_freq` threshold to the vocabulary mapping.\n",
    "    * `encode`: Takes a list of words and converts it into the corresponding list of numerical IDs. It uses the `<unk>` token's ID for any word it doesn't recognize.\n",
    "    * `__len__`: Allows you to find the total number of unique items in the vocabulary by calling `len()` on the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943e908d-5ccf-4454-9bd2-9473438e9c81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"Builds and manages a word-to-index vocabulary from text.\n",
    "\n",
    "    Attributes:\n",
    "        word2idx (dict): Maps words to unique integers.\n",
    "        idx2word (dict): Maps integers back to words.\n",
    "        min_freq (int): Minimum word frequency for inclusion.\n",
    "    \"\"\"\n",
    "    def __init__(self, min_freq=1):\n",
    "        \"\"\"Initializes the vocabulary.\n",
    "\n",
    "        Args:\n",
    "            min_freq (int): Minimum word frequency to be included.\n",
    "        \"\"\"\n",
    "        # Mappings for word-to-index and index-to-word with special tokens.\n",
    "        self.word2idx = {'<pad>': 0, '<unk>': 1}\n",
    "        self.idx2word = {0: '<pad>', 1: '<unk>'}\n",
    "        self.min_freq = min_freq\n",
    "\n",
    "    def build_vocab(self, texts):\n",
    "        \"\"\"Builds the vocabulary from a corpus of tokenized texts.\n",
    "\n",
    "        Args:\n",
    "            texts (list of list of str): A corpus of tokenized sentences.\n",
    "        \"\"\"\n",
    "        # Count the frequency of all words in the corpus.\n",
    "        word_counts = Counter(word for text in texts for word in text)\n",
    "        \n",
    "        # Add words to the vocabulary if they meet the minimum frequency.\n",
    "        for word, count in word_counts.items():\n",
    "            if count >= self.min_freq:\n",
    "                idx = len(self.word2idx)\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word[idx] = word\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Converts a tokenized text to a sequence of indices.\n",
    "\n",
    "        Args:\n",
    "            text (list of str): Tokenized text to encode.\n",
    "\n",
    "        Returns:\n",
    "            list of int: Sequence of corresponding indices.\n",
    "        \"\"\"\n",
    "        # Use the <unk> token for words not in the vocabulary.\n",
    "        return [self.word2idx.get(word, self.word2idx['<unk>']) for word in text]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the vocabulary size.\"\"\"\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea5a32a-45bb-47c1-b1e8-5a13777da9b4",
   "metadata": {},
   "source": [
    "* Create a new vocabulary object to manage your word-to-number mappings.\n",
    "    * `min_freq=2`: Ensures that only words appearing at least twice in your texts will be added to the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d310ba1-6fa2-4c0f-9f34-3c9621424486",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary(min_freq=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccc5e99-aa67-4f79-9d37-020e97d6dff5",
   "metadata": {},
   "source": [
    "* Execute the main vocabulary building process. **The vocabulary will be built using only the training data**.\n",
    "    * It scans all of the `processed_texts`, counts the frequency of every word, and creates the final word-to-number mapping for all words that meet the `min_freq` rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb238b3-99da-4606-a965-b7a8e20a4de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the vocabulary using ONLY the processed training texts.\n",
    "vocab.build_vocab(processed_train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82be597-4804-4d4d-a03e-8e443af875d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of words in the vocabulary:\", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4c26b1-b348-4d60-95bd-d0298abf8d80",
   "metadata": {},
   "source": [
    "* Use the `vocab.encode()` method to convert your preprocessed training texts into lists of numerical IDs.\n",
    "* Repeat the exact same process for your validation texts, using the same vocabulary you built from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092c584f-afd4-4179-9806-60e9298de086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode both the training and validation texts using the vocabulary.\n",
    "indexed_train_texts = [vocab.encode(text) for text in processed_train_texts]\n",
    "indexed_val_texts = [vocab.encode(text) for text in processed_val_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969b171f-e137-40a4-bbdc-6c1377a71c20",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Preparing Data for Training\n",
    "\n",
    "Now that you have separate, encoded lists for your training and validation data, you can create custom PyTorch `Dataset` objects for each to encapsulate your \"indexed texts\" and \"labels\".\n",
    "\n",
    "#### Building Your Custom `TextDataset`\n",
    "\n",
    "* Define a custom dataset by inheriting from PyTorch's `Dataset` class.\n",
    "    * `__init__`: Stores the lists of indexed texts and numerical labels. It also creates a `.classes` attribute by finding and sorting the unique values from the labels list.\n",
    "    * `__len__`: Returns the total number of samples (the length of your texts list) in the dataset.\n",
    "    * `__getitem__`: Retrieves a single data sample. Given an index idx, it fetches the corresponding text and label, converts them into PyTorch tensors, and returns them in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a61681-b771-4297-9733-8d1d72c62caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom PyTorch Dataset for handling text and label data.\n",
    "\n",
    "    This class encapsulates a dataset of texts and their corresponding labels,\n",
    "    making it compatible with PyTorch's DataLoader.\n",
    "    \"\"\"\n",
    "    def __init__(self, texts, labels):\n",
    "        \"\"\"\n",
    "        Initializes the TextDataset object.\n",
    "\n",
    "        Args:\n",
    "            texts: A list or array of numericalized text sequences.\n",
    "            labels: A list or array of corresponding labels.\n",
    "        \"\"\"\n",
    "        # Store the collection of texts.\n",
    "        self.texts = texts\n",
    "        # Store the collection of labels.\n",
    "        self.labels = labels\n",
    "        # Find unique class labels and store them\n",
    "        self.classes = sorted(list(set(labels)))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        # Return the size of the dataset based on the number of texts.\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves a single sample from the dataset at a given index.\n",
    "\n",
    "        Args:\n",
    "            idx: The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing the text and label as PyTorch tensors.\n",
    "        \"\"\"\n",
    "        # Create a dictionary for the sample at the specified index.\n",
    "        sample = {\n",
    "            'text': torch.tensor(self.texts[idx], dtype=torch.long),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        # Return the sample dictionary.\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7967c785-29aa-4d4e-94aa-6d70fa3f04b0",
   "metadata": {},
   "source": [
    "* Create instances of your `TextDataset` for training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b2635d-cddb-46c6-97ac-7bc8e09cfabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training and validation datasets directly from the split data.\n",
    "train_dataset = TextDataset(indexed_train_texts, train_labels)\n",
    "val_dataset = TextDataset(indexed_val_texts, val_labels)\n",
    "\n",
    "# Print the number of samples in each set to verify.\n",
    "print(f\"Training samples:   {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ffe2b9-f388-4b39-8c63-60b25d0ac723",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The Collate Function and DataLoaders\n",
    "\n",
    "In all of your previous modules on computer vision, you saw that every image was preprocessed into a **fixed**, **uniform size** (e.g., 224x224 pixels). Since each image tensor had the exact same dimensions, the `DataLoader` could easily combine them into a single batch.\n",
    "\n",
    "Text, however, is naturally variable. For example:\n",
    "\n",
    "* `\"apple pie\"` becomes a tensor of length 2.\n",
    "\n",
    "* `\"roasted broccoli and garlic\"` becomes a tensor of length 4.\n",
    "\n",
    "This is where you'll run into an issue with the `DataLoader`'s **default configuration**. By default, it tries to create a batch by stacking the individual tensors. This operation will fail because your text tensors have different lengths, and they can't be stacked into a single, uniform tensor. This is the new challenge you must solve.\n",
    "\n",
    "**The Solution: The `collate_fn` Parameter**\n",
    "\n",
    "To solve this, you'll use an optional but powerful parameter of the `DataLoader` class: `collate_fn`.\n",
    "\n",
    "This parameter lets you provide your own custom function that defines exactly how to take a list of samples from your `Dataset` and combine them into a single batch. Instead of using the default behavior, the `DataLoader` will execute your custom logic.\n",
    "\n",
    "```python\n",
    "DataLoader(dataset=...,\n",
    "           batch_size=...,\n",
    "           shuffle=...,\n",
    "           collate_fn=... # <-- Your custom function goes here\n",
    "          )\n",
    "```\n",
    "\n",
    "You are essentially telling the `DataLoader`, \"Don't use your default stacking method; use my specific instructions to form a batch.\"\n",
    "\n",
    "This practice of using a `collate_fn` is also the standard and most efficient method. While you could pad all sentences from the start, that approach is inefficient, leading to **significant memory and computational waste**. By using a `collate_fn` to perform \"dynamic padding,\" each batch is only padded to the length of the longest sentence within that batch, which saves resources and speeds up training.\n",
    "\n",
    "You will define two such functions, one for each of the model architectures that will be covered in the next section.\n",
    "\n",
    "* First, define `collate_batch_embeddingbag`, which takes a list of samples and prepares a batch specifically formatted for PyTorch's efficient `nn.EmbeddingBag` layer (this will be covered in the next section).\n",
    "    * It first extracts all the labels and text tensors from the batch.\n",
    "    * It then creates a single, long 1D tensor, `flattened_text`, by concatenating all the individual text tensors together.\n",
    "    * It computes an `offsets` tensor. This tensor marks the starting index of each original recipe title within the `flattened_text` tensor.\n",
    "    * Finally, it returns the flattened text, the offsets, and the labels, all moved to the active device (e.g., your GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff30d9c2-e573-4f14-b4ff-55df2694f3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch_embeddingbag(batch):\n",
    "    \"\"\"\n",
    "    Formats a batch for nn.EmbeddingBag by flattening texts and creating offsets.\n",
    "\n",
    "    Args:\n",
    "        batch (list of dict): A list of samples from the Dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple of (flattened_text, offsets, labels) tensors.\n",
    "    \"\"\"\n",
    "    # Extract labels from each item and create a single tensor.\n",
    "    labels = torch.tensor([item['label'] for item in batch])\n",
    "    # Extract the individual text tensors from the batch.\n",
    "    texts = [item['text'] for item in batch]\n",
    "    # Create a list of the lengths of each text, prepended with 0.\n",
    "    offsets = [0] + [len(text) for text in texts]\n",
    "    # Convert to a tensor of offsets representing the starting index of each sequence.\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    # Concatenate all text tensors into a single, long 1D tensor.\n",
    "    flattened_text = torch.cat(texts)\n",
    "    \n",
    "    # Return the three tensors required for the EmbeddingBag model.\n",
    "    return flattened_text.to(device), offsets.to(device), labels.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3d1e1b-042e-410b-8a2b-428cd5111aa2",
   "metadata": {},
   "source": [
    "* Next, define `collate_batch_manual`, which prepares a batch using the more common technique of **padding**.\n",
    "    * It starts by extracting all the labels and text tensors from the batch.\n",
    "    * It then finds the length of the longest sequence (`max_len`) **within the current batch**.\n",
    "    * A new tensor of zeros, `padded_texts`, is created with the shape `(batch_size, max_len)`. This acts as a template for the batch.\n",
    "    * It then loops through and copies each text sequence into the `padded_texts` tensor, leaving the remaining space as zero-padding for shorter sequences.\n",
    "    * Finally, it returns the single, rectangular `padded_texts` tensor and the `labels` tensor, both moved to the active device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36ff5d4-d614-42b0-bb2f-6e88c85993aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch_manual(batch):\n",
    "    \"\"\"\n",
    "    Formats a batch by padding texts to the same length.\n",
    "\n",
    "    Args:\n",
    "        batch (list of dict): A list of samples from the Dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple of (padded_texts, labels) tensors.\n",
    "    \"\"\"\n",
    "    # Extract labels from each item and create a single tensor.\n",
    "    labels = torch.tensor([item['label'] for item in batch])\n",
    "    # Extract the individual text tensors from the batch.\n",
    "    texts = [item['text'] for item in batch]\n",
    "    # Find the length of the longest sequence in this specific batch.\n",
    "    max_len = max(len(text) for text in texts)\n",
    "    # Create a tensor of zeros to hold the padded batch.\n",
    "    padded_texts = torch.zeros(len(texts), max_len, dtype=torch.long)\n",
    "    # Copy each text sequence into the corresponding row of the padded tensor.\n",
    "    for i, text in enumerate(texts):\n",
    "        padded_texts[i, :len(text)] = text\n",
    "        \n",
    "    # Return the padded texts and labels, moved to the active device.\n",
    "    return padded_texts.to(device), labels.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258bc0b2-c852-4504-bdb0-d9e78c3dfd9c",
   "metadata": {},
   "source": [
    "* Create two `DataLoader` instances, `train_loader_embag` and `val_loader_embag`.\n",
    "    * `collate_fn=collate_batch_embeddingbag`: Passing your custom `collate_batch_embeddingbag` function to both Dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b6a71b-5424-4cb8-8bf0-6b6ff7265873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of samples to process in each batch.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for the training set with `collate_batch_embeddingbag`\n",
    "train_loader_embag = DataLoader(train_dataset, \n",
    "                                batch_size=batch_size, \n",
    "                                shuffle=True, \n",
    "                                collate_fn=collate_batch_embeddingbag\n",
    "                               )\n",
    "\n",
    "# Create the DataLoader for the validation set with `collate_batch_embeddingbag`\n",
    "val_loader_embag = DataLoader(val_dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=False, \n",
    "                              collate_fn=collate_batch_embeddingbag\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31404b5f-df27-40b5-90d6-ffd997631dcd",
   "metadata": {},
   "source": [
    "* Now, create two `DataLoader` instances, `train_loader_manual` and `val_loader_manual`.\n",
    "    * `collate_fn=collate_batch_manual`: Passing your custom `collate_batch_manual` function to both Dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05000482-f160-44f1-89e4-d5499e348e16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the DataLoader for the training set with `collate_batch_manual`\n",
    "train_loader_manual = DataLoader(train_dataset, \n",
    "                                 batch_size=batch_size, \n",
    "                                 shuffle=True, \n",
    "                                 collate_fn=collate_batch_manual\n",
    "                                )\n",
    "\n",
    "# Create the DataLoader for the validation set with `collate_batch_manual`\n",
    "val_loader_manual = DataLoader(val_dataset, \n",
    "                               batch_size=batch_size, \n",
    "                               shuffle=False, \n",
    "                               collate_fn=collate_batch_manual\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fdac1b-52a0-4300-bd0d-7cc59e82e2e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Architectures: EmbeddingBag vs. Manual Pooling\n",
    "\n",
    "So far, you've completed the entire data pipeline: you've loaded, cleaned, and tokenized the text; built a numerical vocabulary; and prepared `DataLoaders` to feed batches to a model. Now, it's time to design the models that will learn from this data.\n",
    "\n",
    "You're going to build and compare four different model architectures. The models start by converting word indices into dense embedding vectors, but they differ in the next step: how they aggregate the embeddings from a variable-length sentence into a single, fixed-size vector that can be used for classification.\n",
    "\n",
    "1. The first model uses `nn.EmbeddingBag`, a highly efficient, built-in PyTorch layer that performs a **mean** aggregation in one go.\n",
    "\n",
    "2. The next models use a **manual pooling** approach.  We will use three approaches where we will aggregate the embeddings by the **mean**, **max**, and **sum**.\n",
    "\n",
    "By building and testing these, you'll discover which of the new strategies performs better for this specific task.\n",
    "\n",
    "### EmbeddingBagClassifier\n",
    "\n",
    "Your first model, the `EmbeddingBagClassifier`, is a simple but powerful architecture built around PyTorch's efficient `nn.EmbeddingBag` layer.\n",
    "\n",
    "* `__init__`: Defines the three layers of the model:\n",
    "    * <code>[nn.EmbeddingBag](https://docs.pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html)</code>: This is the core of the model. In a single step, this layer takes your token IDs and their corresponding offsets and computes a fixed-size vector for each recipe title by averaging its word embeddings.\n",
    "    * `nn.Dropout`: This is a standard regularization layer that helps prevent the model from overfitting.\n",
    "    * `nn.Linear`: This is the final, fully connected classification layer that takes the vector from the embedding bag and outputs the raw scores for each class ('fruit' or 'vegetable').\n",
    "* The `forward` method defines the actual data flow. The data passes through the layers in this order: `EmbeddingBag` → `Dropout` → `Linear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b377dfe7-149f-4594-ae0e-805ace50c08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingBagClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple text classifier using a pre-trained nn.EmbeddingBag layer.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): The size of the vocabulary.\n",
    "        embedding_dim (int): The size of the embedding vectors.\n",
    "        num_classes (int): The number of output classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, num_classes):\n",
    "        super().__init__()\n",
    "        # The core layer that efficiently computes embeddings for variable-length sequences.\n",
    "        # 'mode=mean' specifies that it will average the embeddings of all words in a sequence.\n",
    "        self.embedding_bag = nn.EmbeddingBag(vocab_size, embedding_dim, mode='mean')\n",
    "        # A standard dropout layer for regularization to prevent overfitting.\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        # The final fully connected layer that maps the embedding to the output classes.\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, text, offsets=None):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            text (torch.Tensor): A 1D tensor of concatenated text indices.\n",
    "            offsets (torch.Tensor): A 1D tensor of starting positions for each sequence.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The raw output scores (logits) for each class.\n",
    "        \"\"\"\n",
    "        # Compute the single embedding vector for each sequence in the batch.\n",
    "        embedded = self.embedding_bag(text, offsets)\n",
    "        # Apply dropout to the embeddings.\n",
    "        embedded = self.dropout(embedded)\n",
    "        # Pass the result through the final linear layer to get class scores.\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472a8e08-0db3-434b-ac1d-b44129d654c4",
   "metadata": {},
   "source": [
    "### ManualPoolingClassifier\n",
    "\n",
    "Your other models, the `ManualPoolingClassifier`, takes a more hands-on approach where you will explicitly define the logic for aggregating the word embeddings.\n",
    "\n",
    "* `__init__`: Defines the model's layers and configuration:\n",
    "    * `nn.Embedding`: A standard embedding layer that converts your padded text's token IDs into dense vectors.\n",
    "        * `padding_idx=0` is set to ensure the padding tokens are ignored during training.\n",
    "    * `self.pooling`: This isn't a layer, but an attribute that stores the name of the pooling strategy (`'mean'`, `'max'`, or `'sum'`) you want to use.\n",
    "    * `nn.Dropout`: A standard regularization layer.\n",
    "    * `nn.Linear`: The final fully connected classification layer.\n",
    "* The `forward` method defines a more complex data flow:\n",
    "    * First, it gets the embeddings and creates a **mask** to ensure the padding tokens are zeroed out and don't affect the pooling calculations.\n",
    "    * The `if/elif` block then uses the chosen `pooling` strategy to aggregate the word embeddings into a single, fixed-size vector for each recipe title. Because of this logic, **this single class acts as a blueprint for three distinct model variations**.\n",
    "    * Finally, the resulting pooled vector is passed through `Dropout` and the final `Linear` layer to get the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd58e48-7c9d-49ad-81b4-1e576c384b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualPoolingClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A text classifier that uses an nn.Embedding layer followed by a manual\n",
    "    pooling strategy (mean, max, or sum).\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): The size of the vocabulary.\n",
    "        embedding_dim (int): The size of the embedding vectors.\n",
    "        num_classes (int): The number of output classes.\n",
    "        pooling (str, optional): The pooling strategy to use.\n",
    "                                 Options: 'mean', 'max', 'sum'.\n",
    "                                 Defaults to 'mean'.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, num_classes, pooling='mean'):\n",
    "        super().__init__()\n",
    "        # Embedding layer that maps token IDs to vectors.\n",
    "        # padding_idx=0 ensures that the padding token is ignored during training.\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        # Store the chosen pooling strategy.\n",
    "        self.pooling = pooling\n",
    "        # Final fully connected layer for classification.\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "        # Dropout layer for regularization.\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, text):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            text (torch.Tensor): A batch of padded text indices.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The raw output scores (logits) for each class.\n",
    "        \"\"\"\n",
    "        # Get embeddings for the input text.\n",
    "        # Shape: (batch_size, max_len, embedding_dim)\n",
    "        embedded = self.embedding(text)\n",
    "\n",
    "        # Create a mask to ignore padding tokens in pooling calculations.\n",
    "        # The mask will have 1s for real tokens and 0s for padding tokens.\n",
    "        mask = (text != 0).float().unsqueeze(-1)\n",
    "        # Apply the mask by element-wise multiplication.\n",
    "        embedded = embedded * mask\n",
    "\n",
    "        # Apply the chosen pooling strategy.\n",
    "        if self.pooling == 'mean':\n",
    "            # Sum embeddings and divide by the actual number of non-padded tokens.\n",
    "            # clamp(min=1) prevents division by zero for empty sequences.\n",
    "            pooled = embedded.sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
    "        elif self.pooling == 'max':\n",
    "            # Set padded positions to negative infinity so they are ignored by max().\n",
    "            embedded[mask.squeeze(-1) == 0] = float('-inf')\n",
    "            pooled, _ = embedded.max(dim=1)\n",
    "        elif self.pooling == 'sum':\n",
    "            # Sum the embeddings of all non-padded tokens.\n",
    "            pooled = embedded.sum(dim=1)\n",
    "\n",
    "        # Apply dropout and the final linear layer.\n",
    "        pooled = self.dropout(pooled)\n",
    "        return self.fc(pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d80b221-2a3f-4985-ad69-1b096bfb1519",
   "metadata": {},
   "source": [
    "### Initializing the Models\n",
    "\n",
    "* Define the hyperparameters for the models.\n",
    "    * `vocab_size`: The total number of unique words in your vocabulary, used to set the size of the embedding layer.\n",
    "    * `embedding_dim`: A hyperparameter you choose that defines the size of the vector used to represent each word.\n",
    "    * `num_classes`: The number of output categories your model will predict (in this case, 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c6cf23-c861-4f88-8f1e-94613a73f9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model Hyperparameters\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 64\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b3317e-c920-454d-8aa5-331329a29016",
   "metadata": {},
   "source": [
    "* Initialize the EmbeddingBag model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2fc019-fde4-4c15-87e3-9966d7868a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embag = EmbeddingBagClassifier(vocab_size, embedding_dim, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d0ae82-a149-4b5b-8476-59103bbb02e9",
   "metadata": {},
   "source": [
    "* Next, initialize the three variants of the `ManualPoolingClassifier`, one for each pooling strategy: `'mean'`, `'max'`, and `'sum'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7a4481-fe63-491f-babb-33cc552e47cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the 'mean' pooling variant\n",
    "model_manual_mean = ManualPoolingClassifier(vocab_size, embedding_dim, num_classes, pooling='mean')\n",
    "\n",
    "# Initialize the 'max' pooling variant\n",
    "model_manual_max = ManualPoolingClassifier(vocab_size, embedding_dim, num_classes, pooling='max')\n",
    "\n",
    "# Initialize the 'sum' pooling variant\n",
    "model_manual_sum = ManualPoolingClassifier(vocab_size, embedding_dim, num_classes, pooling='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75705a78-3eb4-44c8-9ae4-b8b3a9d56838",
   "metadata": {},
   "source": [
    "## Running the Training Experiment\n",
    "\n",
    "With your model architectures and data loaders now fully configured, it's time to begin the training process. However, before you start training, there's one crucial issue to address that was noted during the initial data exploration.\n",
    "\n",
    "Earlier, you saw that the dataset is quite **imbalanced**, with significantly more vegetable recipes than fruit recipes.\n",
    "\n",
    "If you train the model on this dataset as-is, it will see the 'vegetable' class far more often. This can cause the model to become biased, learning to simply predict the majority class ('vegetable') most of the time to achieve high accuracy, while performing poorly on the minority 'fruit' class it rarely sees.\n",
    "\n",
    "To solve this, you'll use a common and effective strategy: **class weights**. The idea is to give more importance to the under-represented class during training. By assigning a higher weight to the 'fruit' class, you tell the loss function to penalize the model more heavily for making mistakes on fruit recipes. This forces the model to pay closer attention to the minority class, leading to a more balanced and fair classifier.\n",
    "\n",
    "### Addressing Class Imbalance\n",
    "\n",
    "* The first step is to get a list of all the labels that belong to your **training set** to calculate the class weights.\n",
    "* Since you already used `train_test_split` earlier, this list is already available in the `train_labels` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf4d1b9-a7c2-4b37-ae58-378ff0ff7c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_list = train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9def398-1d50-4ae7-9258-ffbedb6b2c89",
   "metadata": {},
   "source": [
    "* Use <code>[compute_class_weight()](https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html)</code>, a helper function from the **scikit-learn library** to automatically calculate the appropriate weights to counteract your imbalanced dataset.\n",
    "    * `class_weight='balanced'`: This is the key instruction. It tells the function to automatically calculate weights that are inversely proportional to how frequently each class appears.\n",
    "    * `classes=np.unique(train_labels_list)`: This parameter provides the function with a list of all the unique class labels that exist in your data, which are `[0, 1]`.\n",
    "    * `y=train_labels_list`: This is the list of all the labels from your training set. The function **automatically discovers** which is the minority class by counting the occurrences of each label in this list. These counts are then used to calculate the final \"balanced\" weights.\n",
    "        * In a scenario with more than two classes, this process works exactly the same. The function would calculate a unique weight for each class based on its frequency. If multiple classes were minority classes, they would each receive a similarly high weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569b0698-8ff0-4f0b-9905-7fbad4a23efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use scikit-learn's utility to automatically calculate class weights.\n",
    "class_weights = compute_class_weight(\n",
    "    # The strategy for calculating weights. 'balanced' is automatic.\n",
    "    class_weight='balanced',\n",
    "    # The array of unique class labels (e.g., [0, 1]).\n",
    "    classes=np.unique(train_labels_list),\n",
    "    # The list of all training labels, used to count class frequencies.\n",
    "    y=train_labels_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1214c55e-bf80-4cda-8acd-1e21e6283e6a",
   "metadata": {},
   "source": [
    "* Convert the weights calculated by scikit-learn into the format required by PyTorch.\n",
    "* The weights are first converted from a NumPy array into a PyTorch **tensor** with a `float` data type.\n",
    "* The `.to(device)` then moves this tensor to your active training device (e.g., 'cuda')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45c8344-8e6e-4842-b48a-e4294141769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NumPy array of weights into a PyTorch tensor of type float\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Print the final weights to verify the calculation.\n",
    "print(\"Calculated Class Weights:\")\n",
    "print(f\"  - Fruit (Class 0):     {class_weights[0]:.2f}\")\n",
    "print(f\"  - Vegetable (Class 1): {class_weights[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31296309-164e-446e-bdaa-201123800ef8",
   "metadata": {},
   "source": [
    "### Configuring the Loss Function\n",
    "\n",
    "* Define `nn.CrossEntropyLoss` as your loss function.\n",
    "    * The important step here is that you are passing your previously calculated `class_weights` tensor to the `weight` parameter.\n",
    "        * This instructs the loss function to penalize mistakes on the minority 'fruit' class more heavily than mistakes on the majority 'vegetable' class, forcing the model to learn from both classes more equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b77da59-cb1c-42eb-8c63-f550b90933f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the CrossEntropyLoss function with the calculated `class_weights`.\n",
    "loss_function = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca940fe-c0e2-4e36-8d6e-adb36057b470",
   "metadata": {},
   "source": [
    "### Training All Model Variants\n",
    "\n",
    "Now you can start training your models. The `training_loop` function handles the training process and return the trained model along with a dictionary of the final validation metrics (`val_accuracy`, `val_precision`, `val_recall`, and `val_f1`).\n",
    "\n",
    "The training loop is mostly similar to what you have seen before, with one key difference to handle the two types of models you have created:\n",
    "\n",
    "* It checks the name of the model being trained to determine how to unpack the data batches from the `DataLoader`.\n",
    "* If it's an `EmbeddingBagClassifier`, it expects a batch containing `text`, `offsets`, and `labels`, which is the output of the `collate_batch_embeddingbag` function.\n",
    "* If it's a `ManualPoolingClassifier`, it expects a batch with just the padded `text` and `labels`, which is the output of the `collate_batch_manual` function.\n",
    "\n",
    "This allows this single function to flexibly train all four of your model variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559c65be-6a6b-4bb6-be94-a168a7618888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Uncomment if you want to see the training loop function\n",
    "\n",
    "# helper_utils.display_function(helper_utils.training_loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6e9052-ea12-4c39-b9b0-20fc9c8c1fe0",
   "metadata": {
    "tags": []
   },
   "source": [
    "* First, set the number of training epochs. This value will be used for all model training runs to ensure each is trained for the same duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fa571f-410f-48d0-b383-8c1ef6909021",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6ef543-dbfd-4110-9c9a-ff18068cb1e5",
   "metadata": {},
   "source": [
    "* Run the training loop for the `EmbeddingBagClassifier`.\n",
    "    * Notice that you are passing the `train_loader_embag` and `val_loader_embag` to the function, which were created using the specific `collate_fn` required by this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95c33c4-8c3b-4652-a6f8-ac1d4f054f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform training for the EmbeddingBagClassifier.\n",
    "trained_embag, results_embag = helper_utils.training_loop(\n",
    "    model_embag, \n",
    "    train_loader_embag, \n",
    "    val_loader_embag, \n",
    "    loss_function,\n",
    "    num_epochs, \n",
    "    device\n",
    ")\n",
    "\n",
    "# Display the results \n",
    "print(\"\\nModel: EmbeddingBagClassifier\")\n",
    "helper_utils.print_final_metrics(results_embag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eef6865-46a5-4015-8858-c633b9030c93",
   "metadata": {},
   "source": [
    "* Train the `ManualPoolingClassifier` that uses the `'mean'` pooling strategy.\n",
    "    * This model, along with the other manual variants, will use the `train_loader_manual` and `val_loader_manual`, which provide padded batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a919bd3e-ec9e-44d8-90fb-440515205473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform training for the ManualPoolingClassifier, `mean` variant.\n",
    "trained_mean, results_mean = helper_utils.training_loop(\n",
    "    model_manual_mean, \n",
    "    train_loader_manual, \n",
    "    val_loader_manual, \n",
    "    loss_function, \n",
    "    num_epochs, \n",
    "    device\n",
    ")\n",
    "\n",
    "# Display the results \n",
    "print(\"\\nModel: ManualPoolingClassifier (MEAN)\")\n",
    "helper_utils.print_final_metrics(results_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7574ce69-6a47-4575-9289-e82d9eda71aa",
   "metadata": {},
   "source": [
    "* Next, train the `ManualPoolingClassifier` that uses the `'max'` pooling strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe636934-a244-4fb1-9ea6-2dc88232d305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform training for the ManualPoolingClassifier, `max` variant.\n",
    "trained_max, results_max = helper_utils.training_loop(\n",
    "    model_manual_max, \n",
    "    train_loader_manual, \n",
    "    val_loader_manual, \n",
    "    loss_function, \n",
    "    num_epochs, \n",
    "    device\n",
    ")\n",
    "\n",
    "# Display the results \n",
    "print(\"\\nModel: ManualPoolingClassifier (MAX)\")\n",
    "helper_utils.print_final_metrics(results_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ac524d-ce34-43ae-ad5c-0720b4a3d205",
   "metadata": {},
   "source": [
    "* Finally, train the `ManualPoolingClassifier` that uses the `'sum'` pooling strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ce3720-2d85-4469-a5f7-3a9b6b93693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform training for the ManualPoolingClassifier, `sum` variant.\n",
    "trained_sum, results_sum = helper_utils.training_loop(\n",
    "    model_manual_sum, \n",
    "    train_loader_manual, \n",
    "    val_loader_manual, \n",
    "    loss_function, \n",
    "    num_epochs, \n",
    "    device\n",
    ")\n",
    "\n",
    "# Display the results \n",
    "print(\"\\nModel: ManualPoolingClassifier (SUM)\")\n",
    "helper_utils.print_final_metrics(results_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec2068e",
   "metadata": {},
   "source": [
    "You can now compare the performance of all four models based on their final validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aff6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = helper_utils.get_results_df(\n",
    "    results_embag,\n",
    "    results_mean,\n",
    "    results_max,\n",
    "    results_sum\n",
    ")\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77154ec-e5c3-4598-9045-9c98a130e211",
   "metadata": {},
   "source": [
    "## Best Model Selection and Evaluation\n",
    "\n",
    "So far, you've completed the experimental phase by training four different model architectures. Each was trained under the same conditions, using class weights to handle the imbalanced dataset. This \"bake-off\" has given you performance metrics for each approach.\n",
    "\n",
    "Now, it's time to move from broad experimentation to focused finalization and evaluation.\n",
    "\n",
    "* Put all of your trained models and their results together in a single dictionary called `all_trained_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f645a62c-a758-4f21-8de0-9b0f2743a741",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trained_data = {\n",
    "    'EmbeddingBag': (trained_embag, results_embag),\n",
    "    'Manual (mean)': (trained_mean, results_mean),\n",
    "    'Manual (max)': (trained_max, results_max),\n",
    "    'Manual (sum)': (trained_sum, results_sum)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e07248-a36f-4eb5-baab-4f83ca1ac0ab",
   "metadata": {},
   "source": [
    "### Comparing Model Performance\n",
    "\n",
    "Since you have significantly more vegetable recipes than fruit recipes, accuracy alone can be a misleading metric. A model could achieve high accuracy by simply guessing the majority class ('vegetable') every time. The **F1 score**, however, is a more robust metric for this use case because it calculates a balance between precision and recall. A high F1 score indicates that the model is performing well on both the majority and the minority classes, which is exactly what you want.\n",
    "\n",
    "* Use the `plot_and_select_best_model` function to compare all four trained models based solely on their validation F1 score.\n",
    "    * This function will visualize the results in a bar chart and then return the model instance that achieved the highest F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f44293-0211-4497-a8f8-bcac14d4cca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = helper_utils.plot_and_select_best_model(all_trained_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130377b7-94de-4691-b389-56118b8b3172",
   "metadata": {},
   "source": [
    "### Testing the Best Model on New Examples\n",
    "\n",
    "You've done the heavy lifting: you trained multiple architectures and systematically selected the best model.\n",
    "\n",
    "Now for the final test. It's time to see how your best model performs on completely new, unseen data. This is the best way to get a qualitative feel for how well your model has learned to generalize.\n",
    "\n",
    "* Define a `test_products` list containing a mix of new recipe titles. This list includes straightforward examples as well as more challenging ones to see where the model excels and where it might struggle.\n",
    "    * Feel free to add your own recipe titles to this list to test the model even further!\n",
    " \n",
    "**Note**: Remember, the model's predictions are based *only* on the words in the recipe's `name`. It was never shown the ingredients list, so it has no knowledge of whether fruits or vegetables are the dominant ingredient. A recipe's name can sometimes be misleading, and the model's classification will reflect only what it has learned from the title's text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd93b64d-b4fe-469b-91f1-12b5f7ed7b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_products = [\n",
    "    \"Blueberry Muffins\",                  # Expected: Fruit\n",
    "    \"Spinach and Feta Stuffed Chicken\",   # Expected: Vegetable\n",
    "    \"Classic Carrot Cake with Frosting\",  # Expected: Vegetable\n",
    "    \"Tomato and Basil Bruschetta\",        # Expected: Vegetable\n",
    "    \"Avocado Toast\",                      # Expected: Fruit\n",
    "    \"Zucchini Bread with Walnuts\",        # Expected: Vegetable\n",
    "    \"Lemon and Herb Roasted Chicken\",     # Expected: Fruit\n",
    "    \"Strawberry Rhubarb Pie\",             # Expected: Fruit\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f0f75a-573b-4900-8b91-556ccb428bae",
   "metadata": {},
   "source": [
    "* Finally, loop through the `test_products` list to run the prediction for each recipe and see the model's final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6428539d-037e-4805-8c09-482e1a00b58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment if you want to see the predict category function\n",
    "\n",
    "# helper_utils.display_function(helper_utils.predict_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f410186d-85f6-44f8-adbf-52c58035e1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each test product\n",
    "for product in test_products:\n",
    "    # Call the prediction function with the required arguments\n",
    "    category = helper_utils.predict_category(\n",
    "        best_model,\n",
    "        product,\n",
    "        vocab,\n",
    "        preprocess_text,\n",
    "        device\n",
    "    )\n",
    "    # Print the results\n",
    "    print(f\"Product: '{product}'\\nPredicted: {category}.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd7a59b-1c13-477e-829f-136aa918afa8",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations on completing this lab! You have successfully navigated the entire pipeline for building a text classifier in PyTorch, moving from raw, unstructured text to a functional predictive model. This lab demonstrated how to translate the core concepts of text representation into a real-world application.\n",
    "\n",
    "You began by carefully preprocessing the recipe data, building a vocabulary, and preparing `DataLoaders` that can handle the variable nature of text data, a key difference from working with fixed-size images. You implemented two distinct but related model architectures: one using the highly optimized `nn.EmbeddingBag` and another that allowed for manual control over different pooling strategies (`mean`, `max`, and `sum`). This comparison gave you direct insight into how different aggregation methods can impact performance.\n",
    "\n",
    "Furthermore, you tackled the very common challenge of class imbalance by calculating and applying class weights, ensuring your model learned to classify both fruit and vegetable recipes effectively, rather than being biased toward the majority class. Finally, you systematically evaluated your models using the F1 score to identify the top performer and tested it on new, unseen data, which is the ultimate measure of a model's ability to generalize.\n",
    "\n",
    "The model you built serves as an excellent baseline. The embeddings were trained from scratch on your specific dataset. The next logical step, which builds directly on these skills, is to leverage the power of **pre-trained embeddings**. Using vectors from models like GloVe or BERT, which have been trained on billions of words, can provide your model with a much richer understanding of language from the very start, often leading to significant performance gains."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
