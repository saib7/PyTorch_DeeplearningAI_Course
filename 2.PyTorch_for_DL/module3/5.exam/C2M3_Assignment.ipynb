{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0ca2cc3-f3eb-4d33-bb5f-f9c32f2a827b",
   "metadata": {},
   "source": [
    "# Programming Assignment: AI Powered Request Dispatcher\n",
    "\n",
    "In the world of AI-powered chat and large language models, delivering a great response begins with understanding the user's prompt. Before any system can act, it must first know what is being asked. Is the user asking a question? Do they want to brainstorm ideas? Or are they requesting a summary?\n",
    "\n",
    "This is the pivotal challenge you will solve in this assignment. You're going to build an **AI Powered Request Dispatcher**, the essential first step for any advanced AI system. Your model will learn to analyze and classify the intent behind any user request, ensuring it gets routed to the correct specialized tool for a perfect response. Think of it as the smart front door for a powerful AI application.\n",
    "\n",
    "This assignment will guide you through the entire process, from handling raw data to fine-tuning a sophisticated, pre-trained model. You'll see firsthand how foundational concepts come together to create a practical, real-world tool.\n",
    "\n",
    "Specifically, you'll do the following steps:\n",
    "\n",
    "* **Preparing the Data**: You'll start with the **Databricks Dolly 15k dataset**, a high-quality collection of instructions, and preprocess it by consolidating categories and converting them into numerical labels ready for the model.\n",
    "* **Setting Up the Foundation**: You will leverage **DistilBERT**, a fast and powerful pre-trained model, and its tokenizer to serve as the core of your dispatcher.\n",
    "* **Assembling the Data Feed**: You'll construct a custom PyTorch `Dataset` for on-the-fly tokenization and build `DataLoaders` to efficiently batch and serve data to the model during training.\n",
    "* **Optimizing the Fine-Tuning Process**: You'll implement advanced strategies to make your training more effective. This includes calculating **class weights** to handle imbalanced data and using partial freezing for efficient fine-tune training.\n",
    "* **Training and Evaluation**: Finally, you'll bring everything together to train your dispatcher, analyze its performance with a confusion matrix, and test its real-world capabilities on new, unseen instructions.\n",
    "\n",
    "Let's begin building the intelligent core of your next great AI application!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39f2f56",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='submission'></a>\n",
    "\n",
    "<h4 style=\"color:green; font-weight:bold;\">TIPS FOR SUCCESSFUL GRADING OF YOUR ASSIGNMENT:</h4>\n",
    "\n",
    "* All cells are frozen except for the ones where you need to submit your solutions or when explicitly mentioned you can interact with it.\n",
    "\n",
    "* In each exercise cell, look for comments `### START CODE HERE ###` and `### END CODE HERE ###`. These show you where to write the solution code. **Do not add or change any code that is outside these comments**.\n",
    "\n",
    "* You can add new cells to experiment but these will be omitted by the grader, so don't rely on newly created cells to host your solution code, use the provided places for this.\n",
    "\n",
    "* Avoid using global variables unless you absolutely have to. The grader tests your code in an isolated environment without running all cells from the top. As a result, global variables may be unavailable when scoring your submission. Global variables that are meant to be used will be defined in UPPERCASE.\n",
    "\n",
    "* To submit your notebook for grading, first save it by clicking the ðŸ’¾ icon on the top left of the page and then click on the `Submit assignment` button on the top right of the page.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211efff6-4b37-403d-9116-192aef4f4ae2",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Imports](#0)\n",
    "- [1 - The Databricks Dolly 15k Dataset](#1)\n",
    "    - [1.1 - Preparing the Instruction Data](#1-1)\n",
    "- [2 - Setting Up the Foundation: DistilBERT](#2)\n",
    "- [3. Assembling the Data Feed for Training](#3)\n",
    "    - [3.1 - Creating a Custom PyTorch Dataset](#3-1)\n",
    "        - **[Exercise 1 - InstructionDataset](#ex-1)**\n",
    "    - [3.2 - Splitting Data for Training and Validation](#3-2)\n",
    "    - [3.3 - Defining the Data Collator](#3-2)\n",
    "        - **[Exercise 2 - create_data_collator](#ex-2)**\n",
    "    - [3.4 - Constructing the Data Loaders](#3-4)\n",
    "        - **[Exercise 3 - create_dataloaders](#ex-3)**\n",
    "- [4 - Optimizing the Fine-Tuning Process](#4)\n",
    "    - [4.1 - Addressing Class Imbalance](#4-1)\n",
    "        - **[Exercise 4 - calculate_class_weights](#ex-4)**\n",
    "    - [4.2 Configuring the Loss Function](#4-2)\n",
    "    - [4.3 - Implementing an Efficient Fine-Tuning Strategy](#4-3)\n",
    "        - - **[Exercise 5 - partially_freeze_bert_layers](#ex-5)**\n",
    "- [5 - Training the Dispatcher](#5)\n",
    "    - **[Exercise 6 - Configuring the Training Run](#5)**\n",
    "- [6 - (Optional) Evaluating the Dispatcher](#6)\n",
    "    - [6.1 - Analyzing Dispatcher Performance](#6-1)\n",
    "    - [6.2 - Testing the Dispatcher on Unseen Instructions](#6-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-f6a7-8901-2345-67890abcdef1",
   "metadata": {},
   "source": [
    "<a name='0'></a>\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6-a7b8-9012-3456-7890abcdef12",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5a8c7d-a647-4a27-9555-9db65f29318f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import helper_utils\n",
    "import unittests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef2885f-dc46-40b3-b527-51e5d374c900",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa421fa-5cfa-4026-bac7-67448130ea7e",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - The Databricks Dolly 15k Dataset\n",
    "\n",
    "Every great AI system is built on a foundation of high-quality data. For your **AI Powered Request Dispatcher**, that foundation is the [Databricks Dolly 15k](https://www.kaggle.com/datasets/snehilsanyal/databricks-dolly-15k-dataset), an open source, instruction following dataset created by thousands of Databricks employees. It contains over 15,000 high quality prompt and response pairs designed to make language models more helpful and interactive.\n",
    "\n",
    "For your task of building a dispatcher, you will focus on two key columns: `instruction`, which contains the user prompt, and `category`, which describes the type of task the instruction represents (e.g., `open_qa`, `summarization`, `creative_writing`).\n",
    "\n",
    "#### How the Dataset Was Augmented\n",
    "\n",
    "To build a truly robust dispatcher, you can't just rely on the original data. That's why this dataset has been augmented using a professional technique called **back-translation**. This process created new training examples that are grammatically different but semantically similar to the original ones.\n",
    "\n",
    "Essentially, each instruction was translated from English into 10 different intermediate languages, such as Arabic, Urdu, and German, and then immediately translated back to English. This process created new sentences with slightly different wording but the same core meaning. These unique variations were then combined with the original data to create a larger and more diverse set of examples for your training, growing the dataset from approximately 15,000 to 26,000 instructions.\n",
    "\n",
    "If you are interested in the specifics, you can take a look at the `augment_dataset_with_back_translation` function that was used for it, which can be found in the `helper_utils.py` file.\n",
    "\n",
    "With this powerful, augmented dataset in hand, your first mission is to preprocess and shape it into the perfect fuel for your model.\n",
    "\n",
    "<a name='1-1'></a>\n",
    "### 1.1 - Preparing the Instruction Data\n",
    "\n",
    "Your journey from raw data to a trained model begins with the first step of **preprocessing**. This is where you clean, refine, and structure your dataset to ensure your dispatcher learns from clear and meaningful signals.\n",
    "\n",
    "* Load the dataset from `\"./databricks-dolly-15k-dataset/databricks-dolly_augmented.csv\"` as a DataFrame and remove any incomplete rows to ensure data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e66726-5b83-498c-8ac3-aca10597e6ee",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Load the dataset and remove any rows with missing values to ensure data quality.\n",
    "data_path = \"./databricks-dolly-15k-dataset/databricks-dolly_augmented.csv\"\n",
    "df = pd.read_csv(data_path).dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949b1770-2e6a-44f4-b589-76cdb498de66",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "First, get a high-level view of the initial state of the dataset. By inspecting the **unique categories**, you can understand the different types of instructions that are present for your dispatcher to potentially learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddea530-7866-4757-bc6e-2f2a714b51ae",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Get a list of the unique category names from the 'category' column.\n",
    "unique_categories = df['category'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aefe9d5-3c13-45db-9cf5-125f22ca11ec",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Print the list of unique categories to see the initial state.\n",
    "print(\"Initial categories in the dataset:\\n\")\n",
    "pprint(unique_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40130e9a-c1a5-4ea5-b9b8-1e157d628b04",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "A quick look at the first few rows should confirm that your data has loaded correctly and gives you a feel for the structure you'll be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48141b74-1037-4a3e-a479-a8e3edfa3a43",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# EDITABLE CELL\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaaa4ed-9e4d-4711-bcbb-062569875007",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "In a real-world project, you often need to make strategic decisions about your target classes. The raw dataset has several granular categories. To create a more focused and effective dispatcher for this assignment, you'll consolidate similar categories. This common practice simplifies the classification task and can lead to a more robust model.\n",
    "\n",
    "* You'll merge all question answering types into a single `q_and_a` target and combine `information_extraction` with `summarization` to create a new `information_distillation` category. This simplifies the set of routing targets your dispatcher needs to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453f4f3c-e631-44bd-834c-d3ced971f487",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Create a mapping dictionary to define how categories should be merged.\n",
    "# Keys are the old category names, and values are the new, consolidated names.\n",
    "category_map = {\n",
    "    \"general_qa\": \"q_and_a\",\n",
    "    \"open_qa\": \"q_and_a\",\n",
    "    \"closed_qa\": \"q_and_a\",\n",
    "    \"information_extraction\": \"information_distillation\",\n",
    "    \"summarization\": \"information_distillation\"\n",
    "}\n",
    "\n",
    "# Use the replace() method on the 'category' column to apply the mapping.\n",
    "df['category'] = df['category'].replace(category_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740ea147-d5cc-4702-8f1a-f3316525df8c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "With your final categories defined, it's time to translate them into a format the model can understand. You'll convert your text categories into numerical **labels**. You'll also create a reverse mapping dictionary, a tool that will allow you to convert the model's numerical output back into human-readable predictions later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6b5653-8184-4036-b15c-0fc18521ce5e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Get an array of the unique, consolidated category names.\n",
    "unique_categories = df['category'].unique()\n",
    "\n",
    "# Create a dictionary to map each category name to a unique integer ID.\n",
    "cat2id = {category: i for i, category in enumerate(unique_categories)}\n",
    "\n",
    "# Create the new 'label' column in the DataFrame by applying the mapping.\n",
    "# The .map() function efficiently converts each category name to its corresponding integer ID.\n",
    "df['label'] = df['category'].map(cat2id)\n",
    "\n",
    "# Create the reverse mapping from integer ID back to the category name.\n",
    "id2cat = {id: category for category, id in cat2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9d5644-9c34-4a7e-a960-2e5411c07b8c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "To finalize this stage of your data pipeline, you'll extract the core components, the `instruction` texts and their corresponding numerical `labels`, into dedicated lists, ready for the next phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9881ae44-bdf0-4e6c-ba4b-962e0ec71758",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Extract the 'instruction' and 'label' columns into lists\n",
    "texts = df['instruction'].tolist()\n",
    "labels = df['label'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16e729c-fb89-4bec-aae0-47f2a609e237",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Perform a final check. A quick look at the class distribution should confirm you that the preprocessing steps were successful and show you the final number of samples for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22514342-951c-4de5-a7f3-b41487379658",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "print(f\"Total samples for classification: {len(texts)}\\n\")\n",
    "print(\"Class distribution:\")\n",
    "\n",
    "# Iterate through the category-to-ID mapping to report on each class.\n",
    "for category, label_id in cat2id.items():\n",
    "    count = labels.count(label_id)\n",
    "    print(f\"  - label {label_id}: {category:<25} {count} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de448f7b-f0e1-43fa-9e89-a3a40d827651",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Take a sneak peek at the final product of your preprocessing pipeline: clean, labeled data. This is exactly the format your `Dataset` and `DataLoaders` will use to feed the dispatcher during training.\n",
    "\n",
    "* Feel free to change the values of `num_samples` and `random_state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a90d1b-8b7e-4e68-b32a-c1059d82e470",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# EDITABLE CELL\n",
    "\n",
    "# Set the number of random samples to display, and random_state.\n",
    "num_samples = 10\n",
    "random_state = 25\n",
    "\n",
    "# Display a sample of instruction and label pairs.\n",
    "display(df[['instruction', 'label']].sample(num_samples, random_state=random_state).style.hide(axis=\"index\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f72292-5d2f-47d4-bdcd-08597931091a",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Setting Up the Foundation: DistilBERT\n",
    "\n",
    "With your data perfectly prepped, it's time to select the engine for your dispatcher. In modern NLP, you rarely build from scratch. Instead, you'll leverage a powerful, pre-trained foundation to give your project a massive head start.\n",
    "\n",
    "Your choice is [DistilBERT](https://huggingface.co/docs/transformers/en/model_doc/distilbert): a lean, efficient, and powerful version of the original BERT. For a dispatcher that needs to be fast and responsive, DistilBERT is the perfect starting point.\n",
    "\n",
    "Of course, a model is only as good as its understanding of language. Alongside the model, you'll load its corresponding **tokenizer**. This integral component is responsible for translating your raw text instructions into the numerical format that DistilBERT understands, acting as the bridge between human language and the model's brain.\n",
    "\n",
    "* Download the pre-trained model and its tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a29e54b-f761-4803-8d35-a1c11a91e835",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "model_name=\"distilbert-base-uncased\"\n",
    "model_path=\"./distilbert-local-base\"\n",
    "\n",
    "# Ensure the model is downloaded\n",
    "helper_utils.download_bert(model_name, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c543de94-4626-4009-b542-4e0745d60429",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* Load the DistilBERT model and its tokenizer into memory, officially setting up the core of your AI dispatcher.\n",
    "\n",
    "**Note**: You will see a warning that some weights were \"newly initialized.\" This is expected. It confirms that you have successfully loaded the pre-trained DistilBERT base and attached a new, untrained classification head. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d708b3e-b4f4-4093-957d-4862f93e7500",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Get the number of unique classes from the 'category' column.\n",
    "num_classes = df['category'].nunique()\n",
    "\n",
    "# Load the pre-trained DistilBERT model and its tokenizer from the local path.\n",
    "bert_model, bert_tokenizer = helper_utils.load_bert(model_path, \n",
    "                                                    num_classes=num_classes\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e72f5a5-7cf1-438e-b63d-90e396cb1f17",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3. Assembling the Data Feed for Training\n",
    "Your model's engine (DistilBERT) is ready, and your fuel (the preprocessed data) is refined. Now, you need to build the pipeline that connects them.\n",
    "\n",
    "This section is all about creating an efficient, automated data feed for your dispatcher. You'll construct the essential PyTorch components, a custom `Dataset` and powerful `DataLoaders`, that will handle tokenization, batching, and shuffling, ensuring a smooth and efficient training process.\n",
    "\n",
    "<a name='3-1'></a>\n",
    "### 3.1 - Creating a Custom PyTorch Dataset\n",
    "\n",
    "The heart of your data pipeline will be a custom `InstructionDataset`. This is not going to be just a simple data container, but an intelligent component that will perform on-the-fly tokenization.\n",
    "\n",
    "Every time the training loop will request a sample, your `Dataset` will grab the raw text, instantly tokenize it with your DistilBERT tokenizer, and pair it with its correct label. This \"just-in-time\" processing is an efficient and standard practice in modern NLP pipelines.\n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### Exercise 1 - InstructionDataset\n",
    "\n",
    "Now it's your turn to build this core component. Implement the `InstructionDataset` class, which will serve as the blueprint for preparing each individual sample for your model.\n",
    "\n",
    "**Your Task**:\n",
    "\n",
    "* **Inside the `__init__` method:**\n",
    "    * This method sets up the dataset. You need to store the lists of `texts` and `labels`, along with the `tokenizer`, so they can be used later.\n",
    "    * Assign them as they are passed in to their corresponding instance attributes.\n",
    "* **Inside the `__getitem__` method**:\n",
    "    * **Fetch the data**: Get the specific `text` and `label` for the given index (`idx`).\n",
    "    * **Tokenize the text**: Use the `self.tokenizer` to convert the raw `text` string into a processed `encoding`. You must set `truncation` to `True` and `max_length` to `512`.\n",
    "        * This encoding will be a dictionary containing `input_ids` and `attention_mask`.\n",
    "    * **Add the label**: Add the integer `label` to the encoding dictionary with the key `'labels'`. It's important to convert this label into a `torch.tensor` of type `torch.long`.\n",
    " \n",
    "<details>\n",
    "<summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
    "\n",
    "If you're stuck, here is a more detailed breakdown.\n",
    "\n",
    "**For the __init__ method:**\n",
    "* This is a standard Python constructor. You're just saving the arguments for later use.\n",
    "* For example, the first line will be `self.texts = texts`. Follow this pattern for `labels` and `tokenizer`.\n",
    "\n",
    "**For the __getitem__ method**:\n",
    "* Fetching the data: Use standard list indexing to get an item from your stored lists (e.g., `text = self.texts[idx]`).\n",
    "\n",
    "* **Tokenizing**: The tokenizer is callable. You pass the `text` string, and `truncation` and `max_length` parameters. The call will look like this:\n",
    "    * > `encoding = call the tokenizer on (the text, with the truncation option set to True, and the max_length option set to 512)`\n",
    "\n",
    "* **Adding the label**: This is a two-part step. First, access the dictionary key: `encoding['labels'] = ....` The value you assign should be the label converted to the correct tensor type. This will look like this:\n",
    "    > `encoding['labels'] = create a new torch.tensor(from the label, and set the dtype to torch's long integer type)`\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1f2-a3b4-5678-9012-def123456789",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CLASS: InstructionDataset\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for text classification.\n",
    "\n",
    "    This Dataset class stores raw texts and their corresponding labels. It is\n",
    "    designed to work efficiently with a Hugging Face tokenizer, performing\n",
    "    tokenization on the fly for each sample when it is requested.\n",
    "\n",
    "    Args:\n",
    "        texts (list[str]): A list of raw text strings.\n",
    "        labels (list[int]): A list of integer labels corresponding to the texts.\n",
    "        tokenizer: A Hugging Face tokenizer instance used for processing text.\n",
    "    \"\"\"\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        # Store the list of raw text strings\n",
    "        self.texts = texts\n",
    "        # Store the list of integer labels\n",
    "        self.labels = labels\n",
    "        # Store the tokenizer instance that will process the text\n",
    "        self.tokenizer = tokenizer\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves and processes one sample from the dataset.\n",
    "\n",
    "        For a given index, this method fetches the corresponding text and label,\n",
    "        tokenizes the text, and returns a dictionary of tensors.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the tokenized inputs ('input_ids',\n",
    "                  'attention_mask') and the 'labels' as tensors.\n",
    "        \"\"\"\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        # Get the raw text and label for the specified index\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize the text. This single call handles cleaning, tokenization,\n",
    "        # conversion to numerical IDs, and truncation.\n",
    "        # Set `truncation` to `True` and `max_length` as 512\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "        )\n",
    "\n",
    "        # Add the label to the encoding dictionary and convert it to a tensor\n",
    "        # Set `dtype` as `torch.long`\n",
    "        encoding['labels'] = torch.tensor(label, dtype=torch.long)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda66ec4-947b-4a5b-87c2-b84a64a76ed4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize the dataset to verify\n",
    "verify_class = InstructionDataset(texts, labels, bert_tokenizer)\n",
    "\n",
    "print(f\"Dataset initialized with {len(verify_class)} total samples.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# Display the first 3 instructions and their corresponding labels\n",
    "print(\"Displaying the first 3 instructions and labels from your data:\\n\")\n",
    "for i in range(3):\n",
    "    print(f\"Instruction: {verify_class.texts[i]}\")\n",
    "    print(f\"Label:       {verify_class.labels[i]}\\n\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfed797-9f35-4255-aeac-8a271eaed497",
   "metadata": {},
   "source": [
    "#### Expected Output:\n",
    "\n",
    "```\n",
    "Dataset initialized with 26238 total samples.\n",
    "--------------------------------------------------\n",
    "Displaying the first 3 instructions and labels from your data:\n",
    "\n",
    "Instruction: When did Virgin Australia start operating?\n",
    "Label:       0\n",
    "\n",
    "Instruction: Which is a species of fish? Tope or Rope\n",
    "Label:       1\n",
    "\n",
    "Instruction: Why can camels survive for long without water?\n",
    "Label:       0\n",
    "\n",
    "--------------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee29dd8-08d5-49c1-a8e8-ebb017870aaf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "unittests.exercise_1(InstructionDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e949143-9364-46e3-b4c4-b6c3a14bfe5d",
   "metadata": {},
   "source": [
    "<a name='3-2'></a>\n",
    "### 3.2 - Splitting Data for Training and Validation\n",
    "\n",
    "With your `InstructionDataset` class defined, create an instance containing your entire preprocessed dataset.\n",
    "\n",
    "However, to build a reliable model, you must evaluate its performance honestly. This means testing it on data it has never seen during training. A model that performs well on training data but fails on new data isn't truly intelligent, it has simply memorized the answers.\n",
    "\n",
    "Therefore, an important step in any machine learning pipeline is to partition the data. You will split your full dataset into a large **training set** to teach the model, and a smaller **validation set** to rigorously and impartially test its ability to generalize.\n",
    "\n",
    "* Create the `full_dataset` object by passing your prepared `texts`, `labels`, and `bert_tokenizer` to the `InstructionDataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1b02f7-e93a-4627-ab95-0874e0ccb6cb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Create the full dataset\n",
    "full_dataset = InstructionDataset(texts, labels, bert_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4799fce-0d17-487c-83c6-c74e31c763c5",
   "metadata": {},
   "source": [
    "* Split the `full_dataset` into an 80% training set and a 20% validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ceb6f9-7e94-48ce-9ad2-f0dd9ff9bf1f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Split the full dataset into an 80% training set and a 20% validation set.\n",
    "train_dataset, val_dataset = helper_utils.create_dataset_splits(\n",
    "    full_dataset, \n",
    "    train_split_percentage=0.8\n",
    ")\n",
    "\n",
    "# Print the number of samples in each set to verify the split.\n",
    "print(f\"Training samples:    {len(train_dataset)}\")\n",
    "print(f\"Validation samples:  {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7571826-d397-40a9-88bf-ce2d0bf7c5bb",
   "metadata": {},
   "source": [
    "<a name='3-3'></a>\n",
    "### 3.3 - Defining the Data Collator\n",
    "\n",
    "Your data pipeline is taking shape. You have your datasets, but there's a practical challenge to solve before you can create batches: your tokenized instructions are all different lengths, in other words each sentence has different number of words. To train your model efficiently, each batch of data must be a uniform tensor. To handle this, you need a component that can dynamically pad the shorter sequences so that every instruction in the batch matches the length of the longest one. This will ensure perfectly shaped, rectangular tensors are always fed to your dispatcher. You need a **data collator**.\n",
    "\n",
    "<a name='ex-2'></a>\n",
    "### Exercise 2 - create_data_collator\n",
    "\n",
    "Your next task is to implement the function, `create_data_collator`, that creates this essential padding component for your data pipeline.\n",
    "\n",
    "**Your Task**:\n",
    "\n",
    "* Inside the function, you need to create an instance of the `transformers.DataCollatorWithPadding` class.\n",
    "* During initialization, you must pass the `tokenizer` to the `tokenizer` parameter of the class.\n",
    "\n",
    "<details>\n",
    "<summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
    "\n",
    "This exercise involves initializing a class from the `transformers` library and assigning it to the `collator` variable.\n",
    "\n",
    "The call will look like this:\n",
    "\n",
    "> `collator = create an instance of the DataCollatorWithPadding class, and pass the tokenizer to its 'tokenizer' parameter`\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684f351f-7fef-4e93-a6a8-fcffd97a1692",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: create_data_collator\n",
    "\n",
    "def create_data_collator(tokenizer):\n",
    "    \"\"\"\n",
    "    Initializes and returns a data collator for dynamic padding.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: A Hugging Face tokenizer instance.\n",
    "    \n",
    "    Returns:\n",
    "        collator: A transformers.DataCollatorWithPadding instance.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Initialize the data collator\n",
    "    collator = transformers.DataCollatorWithPadding(\n",
    "        tokenizer=tokenizer,\n",
    "        padding='longest',  # Pad to the longest sequence in the batch\n",
    "        return_tensors='pt' # Return PyTorch tensors\n",
    "    )\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9491f4cc-d195-4d30-a6e6-300b041b941f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Create a few tokenized samples with different lengths to simulate a batch\n",
    "sample_1 = bert_tokenizer(\"This is a short sentence.\")\n",
    "sample_1['labels'] = torch.tensor(0)\n",
    "sample_2 = bert_tokenizer(\"This particular sentence is quite a bit longer.\")\n",
    "sample_2['labels'] = torch.tensor(1)\n",
    "manual_batch = [sample_1, sample_2]\n",
    "\n",
    "# Initialize the data collator\n",
    "verify_function = create_data_collator(bert_tokenizer)\n",
    "\n",
    "# Print the original, un-padded 'input_ids' for each sample\n",
    "print(\"--- Before Collation (Original input_ids) ---\")\n",
    "for i, sample in enumerate(manual_batch):\n",
    "    print(f\"Sample {i+1} 'input_ids': {sample['input_ids']}\")\n",
    "\n",
    "print(\"\\n--- After Collation (Padded and batched) ---\")\n",
    "# Pass the list of samples to the collator and inspect the result\n",
    "collated_batch = verify_function(manual_batch)\n",
    "\n",
    "# Directly print the final tensor\n",
    "print(collated_batch['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c812c722-480d-4047-ad96-52d14ce3060a",
   "metadata": {},
   "source": [
    "#### Expected Output:\n",
    "\n",
    "```\n",
    "--- Before Collation (Original input_ids) ---\n",
    "Sample 1 'input_ids': [101, 2023, 2003, 1037, 2460, 6251, 1012, 102]\n",
    "Sample 2 'input_ids': [101, 2023, 3327, 6251, 2003, 3243, 1037, 2978, 2936, 1012, 102]\n",
    "\n",
    "--- After Collation (Padded and batched) ---\n",
    "tensor([[ 101, 2023, 2003, 1037, 2460, 6251, 1012,  102,    0,    0,    0],\n",
    "        [ 101, 2023, 3327, 6251, 2003, 3243, 1037, 2978, 2936, 1012,  102]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183846c6-8fee-4933-9429-744e653cc930",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "unittests.exercise_2(create_data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8fdb49-00c5-4699-a20f-e14d16927ec0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Great work! With your function correctly defined, create the official `data_collator` instance. This component is now ready to be plugged into the final stage of your data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16d2059-97d2-416d-a4c5-63def5c5e189",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "data_collator = create_data_collator(bert_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d851d26-3b71-4c12-b13a-2a5b92fa56fb",
   "metadata": {},
   "source": [
    "<a name='3-4'></a>\n",
    "### 3.4 - Constructing the Data Loaders\n",
    "\n",
    "You've reached the final assembly stage of your data pipeline. You have the refined data (`Dataset`) and the padding mechanism (`DataCollator`). Now you need to bring them together with `DataLoaders`.\n",
    "\n",
    "These powerful iterators are the workhorses that will serve up perfectly collated and shuffled batches to your model, completing the automated bridge between your raw data and your dispatcher's training loop.\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### Exercise 3 - create_dataloaders\n",
    "\n",
    "Implement the `create_dataloaders` function to produce two `DataLoader` instances.\n",
    "\n",
    "Your final task for the data pipeline section is to implement the function, `create_dataloaders`, that constructs these `DataLoader` workhorses for both, your training and validation, sets.\n",
    "\n",
    "**Your Task**:\n",
    "\n",
    "* **Create the `train_loader`**:\n",
    "    * Instantiate a `DataLoader` for the `train_dataset`.\n",
    "    * You must pass the `train_dataset`, `batch_size`, and `collate_fn` to the correct parameters.\n",
    "    * Set `shuffle` to `True`.\n",
    "* **Create the `val_loader`**:\n",
    "    * Instantiate a `DataLoader` for the `val_dataset`.\n",
    "    * You must pass the `val_dataset`, `batch_size`, and `collate_fn` to the correct parameters.\n",
    "    * Set `shuffle` to `False`.\n",
    "\n",
    "<details>\n",
    "<summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
    "\n",
    "You will be creating two instances of the `DataLoader` class, one for training and one for validation.\n",
    "\n",
    "**For the `train_loader`**:\n",
    "* You need to call the `DataLoader` class and provide the correct arguments.\n",
    "* The first argument is the dataset itself (`train_dataset`).\n",
    "* The other arguments are passed by name: `batch_size=...`, `shuffle=...`, and `collate_fn=....` Remember to set `shuffle` to `True`.\n",
    "\n",
    "**For the `val_loader`**:\n",
    "* This is almost identical to creating the `train_loader`.\n",
    "* The key difference is that you will use the `val_dataset` and, most importantly, you must set `shuffle` to `False`.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e761586-d2f5-4a28-90e8-57ff51c4bb81",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: create_dataloaders\n",
    "\n",
    "def create_dataloaders(train_dataset, val_dataset, batch_size, collate_fn):\n",
    "    \"\"\"\n",
    "    Creates and returns DataLoader instances for training and validation sets.\n",
    "    \n",
    "    Args:\n",
    "        train_dataset (Dataset): The training dataset.\n",
    "        val_dataset (Dataset): The validation dataset.\n",
    "        batch_size (int): The number of samples per batch.\n",
    "        collate_fn (callable): The function to merge a list of samples to form a mini-batch.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing the training DataLoader and validation DataLoader.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Create the DataLoader for the training set\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,          # Shuffle the data for training\n",
    "        collate_fn=collate_fn  # Use the provided collate function\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Create the DataLoader for the validation set\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,         # Do not shuffle the data for validation\n",
    "        collate_fn=collate_fn  # Use the provided collate function\n",
    "    ) \n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b18ceb1-65fe-426b-9d86-2b1549702e5f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Create the DataLoaders\n",
    "batch_size = 16\n",
    "verify_function = create_dataloaders(train_dataset, val_dataset, batch_size, data_collator)\n",
    "\n",
    "# Inspecting the Train Loader\n",
    "first_train_batch = next(iter(verify_function[0]))\n",
    "print(\"Shape of each tensor in the train batch:\")\n",
    "for key, value in first_train_batch.items():\n",
    "    print(f\"  - {key}: {value.shape}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Inspecting the Validation Loader \n",
    "first_val_batch = next(iter(verify_function[1]))\n",
    "print(\"Shape of each tensor in the validation batch:\")\n",
    "for key, value in first_val_batch.items():\n",
    "    print(f\"  - {key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d64411f-967d-4f94-9187-f10ec7fcb345",
   "metadata": {},
   "source": [
    "#### Expected Output: \n",
    "##### (value of \"x\" below will vary for each execution):\n",
    "\n",
    "```\n",
    "Shape of each tensor in the train batch:\n",
    "  - input_ids: torch.Size([16, x])\n",
    "  - attention_mask: torch.Size([16, x])\n",
    "  - labels: torch.Size([16])\n",
    "\n",
    "Shape of each tensor in the validation batch:\n",
    "  - input_ids: torch.Size([16, x])\n",
    "  - attention_mask: torch.Size([16, x])\n",
    "  - labels: torch.Size([16])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53276f85-6d87-4724-98e4-edb0408d5a84",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "unittests.exercise_3(create_dataloaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2633a6-065c-4578-8e45-4ddd46cdbbf0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Fantastic! Your `create_dataloaders` function is working perfectly, producing properly shaped batches. With this, your data pipeline is officially complete. Create the final `train_loader` and `val_loader` that will fuel your model during its training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2a3b4-c5d6-7890-1234-f12345678901",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader, val_loader = create_dataloaders(train_dataset, val_dataset, batch_size, data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a3b4c5-d6e7-8901-2345-123456789012",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Optimizing the Fine-Tuning Process\n",
    "\n",
    "With a robust data pipeline in place, you can now turn your attention to the training process itself. Simply feeding data to a model isn't enough. Professional-grade models are trained with smart, efficient strategies.\n",
    "\n",
    "In this section, you'll implement two powerful techniques to elevate our fine-tuning process: **weighted loss** to handle class imbalance and **partial freezing** for faster, more efficient training.\n",
    "\n",
    "<a name='4-1'></a>\n",
    "### 4.1 - Addressing Class Imbalance\n",
    "\n",
    "A common challenge in real-world datasets is **class imbalance**. Some categories naturally have more data than others. If ignored, your model could become biased, favoring the most common instruction types and performing poorly on rarer ones.\n",
    "\n",
    "To build a fair and reliable dispatcher, you must address this head-on. You'll calculate **class weights**, which will strategically adjust your loss function. This tells the model to pay extra attention to the under-represented categories, ensuring it learns to classify all instruction types with equal skill.\n",
    "\n",
    "<a name='ex-4'></a>\n",
    "### Exercise 4 - calculate_class_weights\n",
    "\n",
    "Your task is to implement the function, `calculate_class_weights` that calculates these class weights.\n",
    "\n",
    "**Your Task**:\n",
    "\n",
    "* **Calculate Weights**:\n",
    "    * Use the `compute_class_weight` function from `sklearn.utils.class_weight`.\n",
    "    * You must set `class_weight` to `'balanced'` to have the function automatically calculate the weights.\n",
    "    * Pass the unique class labels to the `classes` parameter. For this, you will use <code>[np.unique()](https://numpy.org/doc/2.1/reference/generated/numpy.unique.html)</code>.\n",
    "    *  Pass the full list of training labels to the `y` parameter.\n",
    "* **Convert to Tensor:**\n",
    "    * The `compute_class_weight` function returns a NumPy array. You need to convert this array into a PyTorch tensor.\n",
    "    * Ensure the tensor's data type is set to `torch.float`.\n",
    " \n",
    "<details>\n",
    "<summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
    "\n",
    "This function involves one main function call and then a type conversion.\n",
    "\n",
    "**For calculating the weights:**\n",
    "* You will call `compute_class_weight` and assign the result to the `class_weights` variable.\n",
    "* The function call will look like this:\n",
    "    > `class_weights = call compute_class_weight with (class_weight set to 'balanced', classes set by calling np.unique() on the labels list, and y set to the full list of labels)`\n",
    "\n",
    "**For converting to a tensor**:\n",
    "* You will use `torch.tensor()` to convert the `class_weights` NumPy array.\n",
    "* The call will follow this structure:\n",
    "    > `class_weights_tensor = create a new torch.tensor(from the class_weights, and set the dtype to torch's float type)`\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a000634c-1da4-43b7-8899-18b6c1d50ac7",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: calculate_class_weights\n",
    "\n",
    "def calculate_class_weights(train_dataset, device):\n",
    "    \"\"\"\n",
    "    Calculates class weights for handling imbalanced datasets.\n",
    "    \n",
    "    Args:\n",
    "        train_dataset (torch.utils.data.Subset): \n",
    "            The training dataset, expected to be a subset object containing \n",
    "            indices to the original dataset.\n",
    "        device (torch.device): \n",
    "            The device (e.g., 'cuda' or 'cpu') to place the final tensor on.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A 1D tensor of class weights.\n",
    "    \"\"\"\n",
    "    # Extract all labels from the training set to calculate class weights\n",
    "    train_labels_list = [train_dataset.dataset.labels[i] for i in train_dataset.indices]\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Use scikit-learn's utility to automatically calculate class weights\n",
    "\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(train_labels_list),\n",
    "        y=train_labels_list\n",
    "        )\n",
    "\n",
    "    \n",
    "    # Convert the NumPy array of weights into a PyTorch tensor of type float\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return class_weights_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9712eb-a3c7-4f05-aeee-e289c1aabdf9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Calculate the class weights for the training set\n",
    "verify_function = calculate_class_weights(train_dataset, device)\n",
    "\n",
    "# Display the weights alongside their corresponding category\n",
    "print(\"Breakdown of weights per category:\")\n",
    "\n",
    "# Sort the categories by their ID to ensure the order matches the weights tensor\n",
    "sorted_categories = sorted(cat2id.items(), key=lambda item: item[1])\n",
    "\n",
    "for category, label_id in sorted_categories:\n",
    "    # .item() extracts the scalar value from the tensor\n",
    "    weight = verify_function[label_id].item()\n",
    "    print(f\"  - {category:<25}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc28148-4ee7-4c41-839f-003f62177768",
   "metadata": {},
   "source": [
    "#### Expected Output (Approximately):\n",
    "\n",
    "```\n",
    "Breakdown of weights per category:\n",
    "  - q_and_a                  : 0.3983\n",
    "  - classification           : 1.3181\n",
    "  - information_distillation : 1.1231\n",
    "  - brainstorming            : 1.6698\n",
    "  - creative_writing         : 4.1400\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c765284-995b-4380-a6b4-421bd01f5187",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "unittests.exercise_4(calculate_class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e62b725-4ca1-4131-aa35-478dcf6ccce6",
   "metadata": {},
   "source": [
    "<a name='4-2'></a>\n",
    "### 4.2 Configuring the Loss Function\n",
    "\n",
    "With your `calculate_class_weights` function ready, it is time to put it to use.\n",
    "\n",
    "You'll now configure your loss function. By passing these weights directly to `nn.CrossEntropyLoss`, you ensure that every time the model makes a mistake on a rare category, the penalty is higher. This is the mechanism that forces your dispatcher to become a more balanced and equitable classifier.\n",
    "\n",
    "* Calculate `class_weights` using `train_dataset`.\n",
    "* Define `nn.CrossEntropyLoss` as your loss function, and pass your calculated `class_weights` tensor to the weight parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b986c70f-6303-45f9-a3d3-fef84093db08",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Calculate the class weights for the training set\n",
    "class_weights = calculate_class_weights(train_dataset, device)\n",
    "\n",
    "# Initialize the CrossEntropyLoss function with the calculated `class_weights`.\n",
    "loss_function = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3e008a-6227-4c7e-8531-4bf3819de99d",
   "metadata": {},
   "source": [
    "<a name='4-3'></a>\n",
    "### 4.3 - Implementing an Efficient Fine-Tuning Strategy\n",
    "\n",
    "Now onto your second optimization strategy. Fine-tuning a language model with millions of parameters is computationally expensive and time-consuming. A more pragmatic and often equally effective approach is partial fine-tuning (also known as **parameter-efficient fine-tuning (PEFT)**).\n",
    "\n",
    "The core idea is simple but powerful: you'll **freeze** the vast majority of the model's pre-trained layers, which already have a strong understanding of language, and only train the last few layers and the final classification head. This dramatically reduces the number of trainable parameters, leading to significantly faster training cycles without a major sacrifice in performance. It's a key strategy for developing models quickly and cost-effectively.\n",
    "\n",
    "<a name='ex-5'></a>\n",
    "### Exercise 5 - partially_freeze_bert_layers\n",
    "\n",
    "Now, you will implement the function, `partially_freeze_bert_layers`, that will apply the logic for this efficient fine-tuning strategy.\n",
    "\n",
    "**Your Task**:\n",
    "\n",
    "* **Freeze All Parameters**:\n",
    "    * First, you must loop through all the parameters in the `model`.\n",
    "    * Inside the loop, \"freeze\" `requires_grad` attribute of each parameter.\n",
    "* **Unfreeze the Last N Transformer Layers**:\n",
    "    * The code to unfreeze the final classification head is already provided for you. Your task is to unfreeze the transformer layers just before it.\n",
    "    * Loop from `i` in the range of `layers_to_train`.\n",
    "    * Inside the loop, select the correct layer from `transformer_layers` using **negative indexing**. Since you want the *last* N layers, an index like `[-(i + 1)]` will work (e.g., for `i=0`, it gets the last layer; for `i=1`, the second to last, and so on).\n",
    "    * Once you have the `layer_to_unfreeze`, create another loop to iterate through its specific parameters and \"unfreeze\" their `requires_grad` attribute.\n",
    " \n",
    "<details>\n",
    "<summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
    "\n",
    "This function requires you to implement two separate loops.\n",
    "\n",
    "**For freezing all parameters**:\n",
    "* This is a simple loop. The first line is:\n",
    "    > `for param in model.parameters():`\n",
    "* Inside this loop, you just need to set the `requires_grad` attribute of the `param` to `False`.\n",
    "\n",
    "**For unfreezing the last N layers**:\n",
    "* This involves a nested loop structure.\n",
    "* **Outer loop**: This should iterate up to the number of layers you want to train: `for i in range(layers_to_train):`\n",
    "* **Layer Selection**: You need to select a layer from the `transformer_layers` list using a negative index to count from the end. The line will look like this:\n",
    "    > `layer_to_unfreeze = select from transformer_layers using the index -(i + 1)`\n",
    "* **Inner loop**: Once you have the layer, loop through its parameters just like you did when freezing them:\n",
    "    > `for param in layer_to_unfreeze.parameters():`\n",
    "* Inside the inner loop, set the `requires_grad` attribute of the param to `True`.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af696ef3-96bb-46b5-9105-23404548d2a8",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: partially_freeze_bert_layers\n",
    "\n",
    "def partially_freeze_bert_layers(model, layers_to_train=3):\n",
    "    \"\"\"\n",
    "    Freezes all but the last N transformer layers and the classification head\n",
    "    of a DistilBERT model. The model is modified in-place.\n",
    "\n",
    "    Args:\n",
    "        model (transformers.DistilBertForSequenceClassification):\n",
    "            The DistilBERT model to modify.\n",
    "        \n",
    "        layers_to_train (int):\n",
    "            The number of final transformer layers to unfreeze for training.\n",
    "    \n",
    "    Returns:\n",
    "        transformers.DistilBertForSequenceClassification:\n",
    "            The same model instance, now modified with the specified layers\n",
    "            unfrozen.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # First, freeze ALL model parameters\n",
    "    for param in model.parameters():\n",
    "        # Freeze the parameter.\n",
    "        param.requires_grad = False\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Get reference to the list of transformer layers in the model.\n",
    "    transformer_layers = model.distilbert.transformer.layer\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Loop `layers_to_train` times to unfreeze the specified number of final layers.\n",
    "    for i in range(layers_to_train):\n",
    "        # Use negative indexing to select layers from the end of the list.\n",
    "        layer_to_unfreeze = transformer_layers[-(i + 1)]\n",
    "\n",
    "        # Iterate through all parameters *within the selected layer*.\n",
    "        for param in layer_to_unfreeze.parameters():\n",
    "            # Unfreeze the parameter.\n",
    "            param.requires_grad = True\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Unfreeze the final classification head\n",
    "    for param in model.pre_classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "      \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d232d33-dd9a-47cb-9452-36fb563e7a6c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Create a partially frozen model instance\n",
    "verify_function = partially_freeze_bert_layers(bert_model)\n",
    "\n",
    "# Iterate through the model's parameters to check their status\n",
    "for name, param in verify_function.named_parameters():\n",
    "    # Check a parameter from an early, frozen layer\n",
    "    if 'layer.0.attention.q_lin.weight' in name:\n",
    "        print(f\"Parameter from an EARLY layer: '{name}'\")\n",
    "        print(f\"  - Trainable (requires_grad): {param.requires_grad}\")\n",
    "\n",
    "    # Check a parameter from a later, unfrozen layer\n",
    "    if 'layer.4.attention.q_lin.weight' in name:\n",
    "        print(f\"\\nParameter from a LATE layer: '{name}'\")\n",
    "        print(f\"  - Trainable (requires_grad): {param.requires_grad}\")\n",
    "\n",
    "    # Check a parameter from the final classification head\n",
    "    if 'classifier.weight' in name:\n",
    "        print(f\"\\nParameter from the CLASSIFIER head: '{name}'\")\n",
    "        print(f\"  - Trainable (requires_grad): {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0561fe7f-5c6a-4385-9d45-8f211c1fb6e4",
   "metadata": {},
   "source": [
    "#### Expected Output:\n",
    "\n",
    "```\n",
    "Parameter from an EARLY layer: 'distilbert.transformer.layer.0.attention.q_lin.weight'\n",
    "  - Trainable (requires_grad): False\n",
    "\n",
    "Parameter from a LATE layer: 'distilbert.transformer.layer.4.attention.q_lin.weight'\n",
    "  - Trainable (requires_grad): True\n",
    "\n",
    "Parameter from the CLASSIFIER head: 'pre_classifier.weight'\n",
    "  - Trainable (requires_grad): True\n",
    "\n",
    "Parameter from the CLASSIFIER head: 'classifier.weight'\n",
    "  - Trainable (requires_grad): True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200ca3e2-a784-4c52-b6b4-3ceceeb0290e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "unittests.exercise_5(partially_freeze_bert_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04855a3a-984e-4d01-8b90-29212f60a988",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Training the Dispatcher\n",
    "\n",
    "Welcome to the training stage! This is where all your hard work comes together. You've built a robust data pipeline and prepared the model by implementing various optimizations. Now, you'll orchestrate the training process, taking all the components you've meticulously built and using them to teach your AI Powered Request Dispatcher how to classify user intent.\n",
    "\n",
    "<a name='ex-6'></a>\n",
    "### Exercise 6 - Configuring the Training Run\n",
    "\n",
    "Before starting the training loop, you'll set key hyperparameters that guide the process, specifically the number of layers to train (`layers_to_train`), the optimizer's learning rate (`learning_rate`), and the total number of training epochs (`num_epochs`). The choices you make here will directly impact the model's performance, training time, and computational cost. **Since the dataset is imbalanced, your target is to reach a validation loss below 0.7 and a validation F1 score of 0.8 or more.**\n",
    "\n",
    "**Your Task**:\n",
    "\n",
    "You need to set the following hyperparameters to meet your target training metrics.\n",
    "\n",
    "* `layers_to_train`: This controls how many of the model's final transformer layers are unfrozen, balancing deeper adaptation against longer training times.\n",
    "    * *DistilBERT has a total of **6 transformer layers***.\n",
    "* `learning_rate`: This sets the optimizer's step size, trading off faster training speed against the risk of overshooting the best solution.\n",
    "* `num_epochs`: This determines the number of full passes over the training data, balancing more thorough learning against the risk of overfitting.\n",
    "\n",
    "As a reminder, the target is to achieve a **validation loss below 0.7** and a **validation F1 score of 0.8 or more**.\n",
    "\n",
    "<details>\n",
    "<summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
    "\n",
    "Finding the right combination of hyperparameters is a key skill that involves experimentation. We strongly encourage you to try different values on your own first to see how they affect the outcome.\n",
    "\n",
    "If you are stuck or would like to see a combination that is known to work well, you can **scroll to the very bottom of this notebook**. There you will find a code cell that reveals a suggested set of values.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d0f298-76ab-4d85-8069-9abf53a374b2",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# GRADED CELL: Configuring the Training Run\n",
    "\n",
    "### START CODE HERE ###\n",
    "\n",
    "# Set the number of final transformer layers to unfreeze for training. \n",
    "layers_to_train = 4\n",
    "\n",
    "# Set the learning rate for the optimizer\n",
    "learning_rate = 5e-5\n",
    "\n",
    "# Set the total number of training epochs\n",
    "num_epochs = 3\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90916ae9-bd55-4e47-a2a9-ec8c39d6c176",
   "metadata": {},
   "source": [
    "* The `if/elif` block below acts as a safety check on the `layers_to_train` variable. It ensures the value is always valid (between 0 and 6) by automatically correcting any negative numbers to 0 and any numbers greater than 6 down to the maximum of 6 (the total number of DistilBERT's transformer layers). This makes the training setup more robust and prevents errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f81c51-09b0-47ec-9b8b-ae015794d4c5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Check if the value is negative\n",
    "if layers_to_train < 0:\n",
    "    print(f\"'layers_to_train' was set to {layers_to_train}, which is not valid.\\n\")\n",
    "    print(\"The number of transformer layers to train cannot be negative. Setting layers_to_train=0\")\n",
    "    layers_to_train = 0\n",
    "\n",
    "# Check if the value exceeds the total number of layers in the model\n",
    "elif layers_to_train > 6:\n",
    "    print(f\"'layers_to_train' was set to {layers_to_train}, but DistilBERT only has 6 transformer layers.\\n\")\n",
    "    print(f\"Capping at the maximum value. Setting layers_to_train = 6\")\n",
    "    layers_to_train = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b3cef6-698c-4fb2-b764-93b6848a9403",
   "metadata": {},
   "source": [
    "* With the configuration validated, call your `partially_freeze_bert_layers` function. It creates the `partial_finetune_model`, a model instance with the specific layers unfrozen, which is now fully prepared for the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914e82c5-485f-4b8a-a7ed-b3535f2a1efe",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "partial_finetune_model = partially_freeze_bert_layers(bert_model, layers_to_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e3ee34-9972-4ee6-8de7-ee09913215cc",
   "metadata": {},
   "source": [
    "* Execute the `training_loop` to fine-tune the dispatcher.\n",
    "\n",
    "**Note**: Remember, your goal is to achieve a **validation loss below 0.7** and a **validation F1 score of 0.8 or more**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259c8be4-15b0-41a8-b524-01ea310cdfa0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Call the training loop to start the partial fine-tuning process.\n",
    "trained_finetuned_bert, partial_results = helper_utils.training_loop(\n",
    "    model=partial_finetune_model, \n",
    "    train_loader=train_loader, \n",
    "    val_loader=val_loader, \n",
    "    loss_function=loss_function, \n",
    "    learning_rate=learning_rate, \n",
    "    num_epochs=num_epochs,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Display the results \n",
    "print(\"Final Validation Metrics\")\n",
    "print(f\"\\nLoss:       {partial_results['val_loss']:.4f}\")\n",
    "print(f\"Accuracy:   {partial_results['val_accuracy']:.4f}\")\n",
    "print(f\"F1:         {partial_results['val_f1']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab458dc-4971-4757-9947-bdd976d48fc4",
   "metadata": {},
   "source": [
    "#### Expected Output:\n",
    "\n",
    "```\n",
    "--- Training complete ---\n",
    "Final Validation Metrics\n",
    "\n",
    "Loss:       < 0.7\n",
    "Accuracy:   x\n",
    "F1:         > 0.8\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3202c72e-fdfa-4c24-8f1a-13980a00c94a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "unittests.exercise_6(partial_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edac3a79-7341-43d2-a304-e16743e25adc",
   "metadata": {},
   "source": [
    "#### Save your Training Logs\n",
    "\n",
    "<p style=\"background-color:#ffe6f0; color:#282828; padding:15px; border-width:3px; border-color:#d8b2c2; border-style:solid; border-radius:6px\">\n",
    "  ðŸš¨&nbsp;<b>IMPORTANT:</b> Please ensure the test in the cell above has passed successfully before proceeding. The following cell will save your training logs, which is essential for grading. The saved training logs, along with your assignment notebook, will be used in the overall grading of your assignment when you submit it. Failure to run the next cell will prevent your training logs from being saved, resulting in an error during assignment submission.<br><br>\n",
    "  ðŸ”„&nbsp;<b>Note:</b> You can run this cell multiple times to save your training logs. Each run will overwrite the previously saved version.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e738f56-3172-4b61-85c0-179f5a37bcbf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "helper_utils.save_training_logs(partial_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dc2fef-1491-42a6-b742-4ef37e9ef71b",
   "metadata": {},
   "source": [
    "---\n",
    "# Submission Note\n",
    "\n",
    "Congratulations! You've completed the final graded exercise of this assignment.\n",
    "\n",
    "If you've successfully passed all the unit tests above, you've completed the core requirements of this assignment. Feel free to [submit](#submission) your work now. The grading process runs in the background, so it will not disrupt your progress and you can continue on with the rest of the material.\n",
    "\n",
    "**ðŸš¨ IMPORTANT NOTE** If you have passed all tests within the notebook, but the autograder shows a system error after you submit your work:\n",
    "\n",
    "<div style=\"background-color: #1C1C1E; border: 1px solid #444444; color: #FFFFFF; padding: 15px; border-radius: 5px;\">\n",
    "    <p><strong>Grader Error: Grader feedback not found</strong></p>\n",
    "    <p>Autograder failed to produce the feedback...</p>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "This is typically a temporary system glitch. The most common solution is to resubmit your assignment, as this often resolves the problem. Occasionally, it may be necessary to resubmit more than once. \n",
    ">\n",
    "If the error persists, please reach out for support in the [DeepLearning.AI Community Forum](https://community.deeplearning.ai/c/course-q-a/pytorch-for-developers/pytorch-techniques-and-ecosystem-tools/561).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aada1cd-91c9-4176-a360-be65609fe5ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='6'></a>\n",
    "## 6 - (Optional) Evaluating the Dispatcher\n",
    "\n",
    "Congratulations, your model is trained! But is it any good? This optional section is all about answering that question. **Evaluation** is where you pressure-test your model to discover its true capabilities. You'll first perform a quantitative analysis to see the hard numbers on its performance and then move to a qualitative test, challenging it with new instructions to see how it behaves in real-world scenarios. Let's see what your dispatcher can do!\n",
    "\n",
    "<a name='6-1'></a>\n",
    "### 6.1 - Analyzing Dispatcher Performance\n",
    "\n",
    "The first step in your evaluation is to look beyond single-number metrics. To truly understand your dispatcher, you need to see not just if it was right or wrong, but how it was wrong. This is where a confusion matrix comes in. This powerful visualization breaks down the model's predictions for every category, revealing exactly where it's confident and where it's getting confused. Is it consistently mistaking `brainstorming` for `creative_writing`? The confusion matrix will tell all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceccc50e-2dfe-4e40-b239-5c308c14d137",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "helper_utils.analyze_and_plot_results(partial_results, id2cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0fbe0f-e2d5-4dc6-8557-7d242efc5b3f",
   "metadata": {},
   "source": [
    "<a name='6-2'></a>\n",
    "### 6.2 - Testing the Dispatcher on Unseen Instructions\n",
    "\n",
    "Metrics and matrices are essential, but the ultimate measure of your AI dispatcher is how it performs in the wild. This is the final and most insightful test: confronting your model with completely new instructions it has never seen.\n",
    "\n",
    "You will feed it a variety of prompts and observe its classifications in real-time. Does it correctly identify a question? Can it spot a request for a summary? This is where you see if you model has truly learned to generalize. \n",
    "\n",
    "Feel free to add your own custom instructions to the test list to push the dispatcher even further!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975aa376-091d-429d-916e-54bfe71c5f86",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# EDITABLE CELL\n",
    "\n",
    "test_examples = {\n",
    "    \"q_and_a\": [\n",
    "        {\"instruction\": \"Explain the main stages of the water cycle.\", \"expected\": \"q_and_a\"},\n",
    "        {\"instruction\": \"What is the historical significance of the Magna Carta?\", \"expected\": \"q_and_a\"}\n",
    "    ],\n",
    "    \"classification\": [\n",
    "        {\"instruction\": \"Is this movie review positive or negative? 'The plot was amazing and the acting was superb!'\", \"expected\": \"classification\"},\n",
    "        {\"instruction\": \"Is a tomato a fruit or a vegetable?\", \"expected\": \"classification\"}\n",
    "    ],\n",
    "    \"information_distillation\": [\n",
    "        {\"instruction\": \"Summarize the main arguments of the article on climate change from this text.\", \"expected\": \"information_distillation\"},\n",
    "        {\"instruction\": \"Pull out all the dates mentioned in the following project timeline.\", \"expected\": \"information_distillation\"}\n",
    "    ],\n",
    "    \"brainstorming\": [\n",
    "        {\"instruction\": \"Come up with some catchy slogans for a new brand of eco-friendly sneakers.\", \"expected\": \"brainstorming\"},\n",
    "        {\"instruction\": \"Suggest some team-building activities for a company retreat.\", \"expected\": \"brainstorming\"}\n",
    "    ],\n",
    "    \"creative_writing\": [\n",
    "        {\"instruction\": \"Write a short story about a robot who discovers music.\", \"expected\": \"creative_writing\"},\n",
    "        {\"instruction\": \"Compose a haiku about a rainy day.\", \"expected\": \"creative_writing\"}\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2a9516-4dc2-42b5-94fc-22f6e2ae9509",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode before starting the loop.\n",
    "trained_finetuned_bert.eval()\n",
    "\n",
    "print(\"--- Testing Dispatcher on New Instructions ---\\n\")\n",
    "\n",
    "# Loop through each category and its list of examples in the dictionary.\n",
    "for category, examples in test_examples.items():\n",
    "    print(f\"--- Category: {category} ---\")\n",
    "    for example in examples:\n",
    "        instruction = example[\"instruction\"]\n",
    "        expected = example[\"expected\"]\n",
    "        \n",
    "        # Get the model's prediction for the instruction.\n",
    "        predicted = helper_utils.predict_category(\n",
    "            model=trained_finetuned_bert,\n",
    "            tokenizer=bert_tokenizer,\n",
    "            text=instruction,\n",
    "            device=device,\n",
    "            id2cat=id2cat # Pass the mapping dictionary\n",
    "        )\n",
    "        \n",
    "        # Check if the prediction was correct and set the result string/emoji.\n",
    "        if predicted == expected:\n",
    "            result = \"âœ… Correct\"\n",
    "        else:\n",
    "            result = \"âŒ Incorrect\"\n",
    "            \n",
    "        # Print the results in the desired format.\n",
    "        print(f\"Instruction: '{instruction}'\")\n",
    "        print(f\"Expected:    {expected}\")\n",
    "        print(f\"Predicted:   {predicted}\")\n",
    "        print(f\"Result:      {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13bb8c2-36aa-454e-a032-ccb923229e12",
   "metadata": {},
   "source": [
    "## Conclusion: Your Dispatcher is Ready for Duty!\n",
    "\n",
    "Congratulations! You've successfully built, fine-tuned, and tested a complete **AI Powered Request Dispatcher**. You started with a raw dataset and transformed it into a smart, efficient classification model capable of understanding and directing user intent, a vital component in any modern AI system.\n",
    "\n",
    "Throughout this journey, you've put theory into practice. You saw how a pipeline of **tokenization**, where text is broken into smaller units, and **tensorization**, where tokens become numerical tensors for the model, forms the bedrock of any NLP task. You harnessed the power of a pre-trained model, **DistilBERT**, saving immense time and leveraging its deep, pre-existing knowledge of language.\n",
    "\n",
    "Most importantly, you went beyond basic training. By implementing **weighted loss** to tackle class imbalance and applying a **partial-freezing** strategy, you engaged in the kind of parameter-efficient fine-tuning (PEFT) that makes developing high-performance models both practical and cost-effective. You didn't just build a model; you engineered a solution.\n",
    "\n",
    "The dispatcher you've created is more than just an assignment. It represents a fundamental building block for countless advanced applications, from sophisticated chatbots and intelligent search engines to automated content creation tools. You now have the skills and the hands-on experience to build the smart systems of the future. Well done!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842e014f-1623-4aaf-a337-2a7c3275c625",
   "metadata": {},
   "source": [
    "## Need a Hint on Hyperparameters?\n",
    "\n",
    "While we strongly encourage you to experiment first, if you're stuck, run the following cell to see a set of hyperparameters that will help you achieve the target training metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2204e010-943d-4d2a-ae03-c89776740ef7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "encoded_answer = \"VHJ5IHRoZSBmb2xsb3dpbmcgaHlwZXJwYXJhbWV0ZXJzOgoKLSBsYXllcnNfdG9fdHJhaW4gPSA0Ci0gbGVhcm5pbmdfcmF0ZSA9IDVlLTUKLSBudW1fZXBvY2hzID0gNQ==\"\n",
    "encoded_answer = encoded_answer.encode('ascii')\n",
    "answer = base64.b64decode(encoded_answer)\n",
    "answer = answer.decode('ascii')\n",
    "\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "grader_version": "1",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
